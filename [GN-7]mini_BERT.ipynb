{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[GN]mini_BERT\n",
    "\n",
    "이번 프로젝트는 vocab size 8000의 미니 BERT를 구현하는 프로젝트 이다.\n",
    "- vocab_size = 8000\n",
    "- 전체 파라미터 사이즈 1M\n",
    "- 10 epoch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Library check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.1\n",
      "1.23.5\n",
      "2.0.1\n",
      "3.7.1\n",
      "2.0.9\n",
      "2.2.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "import numpy\n",
    "import pandas\n",
    "import matplotlib\n",
    "import json\n",
    "import re\n",
    "\n",
    "print(tensorflow.__version__)\n",
    "print(numpy.__version__)\n",
    "print(pandas.__version__)\n",
    "print(matplotlib.__version__)\n",
    "print(json.__version__)\n",
    "print(re.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Tokenizer 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.1\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import collections\n",
    "import json\n",
    "import shutil\n",
    "import zipfile\n",
    "import copy\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sentencepiece as spm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "random_seed = 1234\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "# tf version 및 gpu 확인\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "완료=3\n"
     ]
    }
   ],
   "source": [
    "# 실행하길 원한다면 \"\"\" 를 지워주세요.\n",
    "\"\"\"\n",
    "import sentencepiece as spm\n",
    "import os\n",
    "corpus_file = './bert_pretrain/data/kowiki.txt'\n",
    "prefix = 'ko_8000'\n",
    "vocab_size = 8000\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    "    f\"--input={corpus_file} --model_prefix={prefix} --vocab_size={vocab_size + 7}\" + \n",
    "    \" --model_type=bpe\" +\n",
    "    \" --max_sentence_length=999999\" + # 문장 최대 길이\n",
    "    \" --pad_id=0 --pad_piece=[PAD]\" + # pad (0)\n",
    "    \" --unk_id=1 --unk_piece=[UNK]\" + # unknown (1)\n",
    "    \" --bos_id=2 --bos_piece=[BOS]\" + # begin of sequence (2)\n",
    "    \" --eos_id=3 --eos_piece=[EOS]\" + # end of sequence (3)\n",
    "    \" --user_defined_symbols=[SEP],[CLS],[MASK]\") # 사용자 정의 토큰\n",
    "\"\"\"\n",
    "print(\"완료=3\")   # 완료메시지가 출력될 때까지 아무 출력내용이 없더라도 기다려 주세요. 2분 38초"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = './bert_pretrain/data'\n",
    "model_dir = './bert_pretrain/models'\n",
    "\n",
    "# vocab loading\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(f\"{model_dir}/ko_8000.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특수 token 7개를 제외한 나머지 tokens 들\n",
    "vocab_list = []\n",
    "for id in range(7, len(vocab)):\n",
    "    if not vocab.is_unknown(id):\n",
    "        vocab_list.append(vocab.id_to_piece(id))\n",
    "print(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# [CLS], tokens a, [SEP], tokens b, [SEP] 형태의 token 생성\n",
    "string_a = \"추적추적 비가 내리는 날이었어 그날은 왠지 손님이 많아 첫 번에 삼십 전 둘째번 오십 전 오랜만에 받아보는 십 전짜리 백통화 서푼에\"\n",
    "string_b = \"손바닥 위엔 기쁨의 눈물이 흘러 컬컬한 목에 모주 한잔을 적셔 몇 달 포 전부터 콜록거리는 아내 생각에 그토록 먹고 싶다던\"\n",
    "tokens_org = [\"[CLS]\"] + vocab.encode_as_pieces(string_a) + [\"[SEP]\"] + vocab.encode_as_pieces(string_b) + [\"[SEP]\"]\n",
    "print(tokens_org)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.데이터 전처리 (1) MASK 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokens_org)\n",
    "\n",
    "# 전체 token의 15% mask\n",
    "mask_cnt = int((len(tokens_org) - 3) * 0.15)\n",
    "mask_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4] ['▁추', '적', '추', '적']\n",
      "[5, 6] ['▁비', '가']\n",
      "[7, 8] ['▁내', '리는']\n",
      "[9, 10, 11] ['▁날', '이었', '어']\n",
      "[12, 13, 14] ['▁그', '날', '은']\n",
      "[15, 16, 17] ['▁', '왠', '지']\n",
      "[18, 19, 20] ['▁손', '님', '이']\n",
      "[21, 22] ['▁많', '아']\n",
      "[23] ['▁첫']\n",
      "[24, 25] ['▁번', '에']\n",
      "[26, 27] ['▁삼', '십']\n",
      "[28] ['▁전']\n",
      "[29, 30, 31] ['▁둘', '째', '번']\n",
      "[32, 33] ['▁오', '십']\n",
      "[34] ['▁전']\n",
      "[35, 36, 37] ['▁오', '랜', '만에']\n",
      "[38, 39, 40] ['▁받아', '보', '는']\n",
      "[41] ['▁십']\n",
      "[42, 43, 44] ['▁전', '짜', '리']\n",
      "[45, 46, 47] ['▁백', '통', '화']\n",
      "[48, 49, 50] ['▁서', '푼', '에']\n",
      "[52, 53, 54] ['▁손', '바', '닥']\n",
      "[55, 56] ['▁위', '엔']\n",
      "[57, 58, 59] ['▁기', '쁨', '의']\n",
      "[60, 61] ['▁눈', '물이']\n",
      "[62, 63] ['▁흘', '러']\n",
      "[64, 65, 66] ['▁컬', '컬', '한']\n",
      "[67, 68] ['▁목', '에']\n",
      "[69, 70] ['▁모', '주']\n",
      "[71, 72, 73] ['▁한', '잔', '을']\n",
      "[74, 75] ['▁적', '셔']\n",
      "[76] ['▁몇']\n",
      "[77] ['▁달']\n",
      "[78] ['▁포']\n",
      "[79, 80] ['▁전', '부터']\n",
      "[81, 82, 83, 84] ['▁콜', '록', '거', '리는']\n",
      "[85] ['▁아내']\n",
      "[86, 87] ['▁생각', '에']\n",
      "[88, 89, 90] ['▁그', '토', '록']\n",
      "[91, 92] ['▁먹', '고']\n",
      "[93, 94, 95] ['▁싶', '다', '던']\n"
     ]
    }
   ],
   "source": [
    "# 띄어쓰기 단위로 mask하기 위해서 index 분할\n",
    "cand_idx = []  # word 단위의 index array\n",
    "for (i, token) in enumerate(tokens_org):\n",
    "    if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "        continue\n",
    "    if 0 < len(cand_idx) and not token.startswith(u\"\\u2581\"):  # u\"\\u2581\"는 단어의 시작을 의미하는 값\n",
    "        cand_idx[-1].append(i)\n",
    "    else:\n",
    "        cand_idx.append([i])\n",
    "\n",
    "# 결과확인\n",
    "for cand in cand_idx:\n",
    "    print(cand, [tokens_org[i] for i in cand])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[24, 25],\n",
       " [57, 58, 59],\n",
       " [32, 33],\n",
       " [64, 65, 66],\n",
       " [41],\n",
       " [79, 80],\n",
       " [52, 53, 54],\n",
       " [67, 68],\n",
       " [29, 30, 31],\n",
       " [91, 92],\n",
       " [23],\n",
       " [26, 27],\n",
       " [76],\n",
       " [42, 43, 44],\n",
       " [78],\n",
       " [60, 61],\n",
       " [38, 39, 40],\n",
       " [93, 94, 95],\n",
       " [9, 10, 11],\n",
       " [81, 82, 83, 84],\n",
       " [85],\n",
       " [12, 13, 14],\n",
       " [34],\n",
       " [71, 72, 73],\n",
       " [77],\n",
       " [45, 46, 47],\n",
       " [48, 49, 50],\n",
       " [28],\n",
       " [74, 75],\n",
       " [62, 63],\n",
       " [88, 89, 90],\n",
       " [5, 6],\n",
       " [35, 36, 37],\n",
       " [55, 56],\n",
       " [18, 19, 20],\n",
       " [86, 87],\n",
       " [7, 8],\n",
       " [15, 16, 17],\n",
       " [1, 2, 3, 4],\n",
       " [21, 22],\n",
       " [69, 70]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random mask를 위해서 index 순서를 섞음\n",
    "random.shuffle(cand_idx)\n",
    "cand_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens_org\n",
      "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]'] \n",
      "\n",
      "tokens\n",
      "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '[MASK]', '[MASK]', '[MASK]', '▁삼', '십', '▁전', '▁둘', '째', '번', '[MASK]', '[MASK]', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '프', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '[MASK]', '[MASK]', '[MASK]', '▁눈', '물이', '▁흘', '러', '[MASK]', '[MASK]', '[MASK]', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# tokens가 mask되므로 재 실행을 위해서 넣어줌 (테스트용)\n",
    "tokens = copy.deepcopy(tokens_org)\n",
    "\n",
    "mask_lms = []  # mask 된 값\n",
    "for index_set in cand_idx:\n",
    "    if len(mask_lms) >= mask_cnt:  # 핸재 mask된 개수가 15%를 넘으면 중지\n",
    "          break\n",
    "    if len(mask_lms) + len(index_set) > mask_cnt:  # 이번에 mask할 개수를 포함해 15%를 넘으면 skip\n",
    "          continue\n",
    "    dice = random.random()  # 0과 1 사이의 확률 값\n",
    "\n",
    "    for index in index_set:\n",
    "        masked_token = None\n",
    "        if dice < 0.8:  # 80% replace with [MASK]\n",
    "            masked_token = \"[MASK]\"\n",
    "        elif dice < 0.9: # 10% keep original\n",
    "            masked_token = tokens[index]\n",
    "        else:  # 10% random word\n",
    "            masked_token = random.choice(vocab_list)\n",
    "        mask_lms.append({\"index\": index, \"label\": tokens[index]})\n",
    "        tokens[index] = masked_token\n",
    "\n",
    "print(\"tokens_org\")\n",
    "print(tokens_org, \"\\n\")\n",
    "print(\"tokens\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask_idx   : [23, 24, 25, 32, 33, 41, 57, 58, 59, 64, 65, 66, 79, 80]\n",
      "mask_label : ['▁첫', '▁번', '에', '▁오', '십', '▁십', '▁기', '쁨', '의', '▁컬', '컬', '한', '▁전', '부터']\n"
     ]
    }
   ],
   "source": [
    "# 순서 정렬 및 mask_idx, mask_label 생성\n",
    "mask_lms = sorted(mask_lms, key=lambda x: x[\"index\"])\n",
    "mask_idx = [p[\"index\"] for p in mask_lms]\n",
    "mask_label = [p[\"label\"] for p in mask_lms]\n",
    "\n",
    "print(\"mask_idx   :\", mask_idx)\n",
    "print(\"mask_label :\", mask_label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🔶create_pretrain_mask() : Masked LM을 위한 코퍼스 생성 메소드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretrain_mask(tokens, mask_cnt, vocab_list):\n",
    "    \"\"\"\n",
    "    마스크 생성\n",
    "    :param tokens: tokens\n",
    "    :param mask_cnt: mask 개수 (전체 tokens의 15%)\n",
    "    :param vocab_list: vocab list (random token 용)\n",
    "    :return tokens: mask된 tokens\n",
    "    :return mask_idx: mask된 token의 index\n",
    "    :return mask_label: mask된 token의 원래 값\n",
    "    \"\"\"\n",
    "    # 단어 단위로 mask 하기 위해서 index 분할\n",
    "    cand_idx = []  # word 단위의 index array\n",
    "    for (i, token) in enumerate(tokens):\n",
    "        if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "            continue\n",
    "        if 0 < len(cand_idx) and not token.startswith(u\"\\u2581\"):\n",
    "            cand_idx[-1].append(i)\n",
    "        else:\n",
    "            cand_idx.append([i])\n",
    "    # random mask를 위해서 순서를 섞음\n",
    "    random.shuffle(cand_idx)\n",
    "\n",
    "    mask_lms = []  # mask 된 값\n",
    "    for index_set in cand_idx:\n",
    "        if len(mask_lms) >= mask_cnt:  # 핸재 mask된 개수가 15%를 넘으면 중지\n",
    "            break\n",
    "        if len(mask_lms) + len(index_set) > mask_cnt:  # 이번에 mask할 개수를 포함해 15%를 넘으면 skip\n",
    "            continue\n",
    "        dice = random.random()  # 0..1 사이의 확률 값\n",
    "        for index in index_set:\n",
    "            masked_token = None\n",
    "            if dice < 0.8:  # 80% replace with [MASK]\n",
    "                masked_token = \"[MASK]\"\n",
    "            elif dice < 0.9: # 10% keep original\n",
    "                masked_token = tokens[index]\n",
    "            else:  # 10% random word\n",
    "                masked_token = random.choice(vocab_list)\n",
    "            mask_lms.append({\"index\": index, \"label\": tokens[index]})\n",
    "            tokens[index] = masked_token\n",
    "    # mask_lms 정렬 후 mask_idx, mask_label 추출\n",
    "    mask_lms = sorted(mask_lms, key=lambda x: x[\"index\"])\n",
    "    mask_idx = [p[\"index\"] for p in mask_lms]  # mask된 token의 index\n",
    "    mask_label = [p[\"label\"] for p in mask_lms]  # mask된 token의 원래 값\n",
    "\n",
    "    return tokens, mask_idx, mask_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens_org\n",
      "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]'] \n",
      "\n",
      "tokens\n",
      "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '[MASK]', '[MASK]', '[MASK]', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '팹', '著', '內', '[SEP]', '▁손', '바', '닥', '[MASK]', '[MASK]', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '▁컬', '컬', '한', '▁목', '에', '[MASK]', '[MASK]', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '[MASK]', '[MASK]', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]'] \n",
      "\n",
      "mask_idx   : [21, 22, 23, 48, 49, 50, 55, 56, 62, 63, 69, 70, 86, 87]\n",
      "mask_label : ['▁많', '아', '▁첫', '▁서', '푼', '에', '▁위', '엔', '▁흘', '러', '▁모', '주', '▁생각', '에']\n"
     ]
    }
   ],
   "source": [
    "# tokens가 mask되므로 재 실행을 위해서 넣어줌 (테스트용)\n",
    "tokens = copy.deepcopy(tokens_org)\n",
    "\n",
    "tokens, mask_idx, mask_label = create_pretrain_mask(tokens, mask_cnt, vocab_list)\n",
    "\n",
    "print(\"tokens_org\")\n",
    "print(tokens_org, \"\\n\")\n",
    "print(\"tokens\")\n",
    "print(tokens, \"\\n\")\n",
    "\n",
    "print(\"mask_idx   :\", mask_idx)\n",
    "print(\"mask_label :\", mask_label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.데이터 전처리 (2) NSP pair 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"\"\"추적추적 비가 내리는 날이었어\n",
    "그날은 왠지 손님이 많아\n",
    "첫 번에 삼십 전 둘째 번 오십 전\n",
    "오랜만에 받아보는 십 전짜리 백통화 서푼에\n",
    "손바닥 위엔 기쁨의 눈물이 흘러\n",
    "컬컬한 목에 모주 한잔을 적셔\n",
    "몇 달 포 전부터 콜록거리는 아내\n",
    "생각에 그토록 먹고 싶다던\n",
    "설렁탕 한 그릇을 이제는 살 수 있어\n",
    "집으로 돌아가는 길 난 문득 떠올라\n",
    "아내의 목소리가 거칠어만 가는 희박한 숨소리가\n",
    "오늘은 왠지 나가지 말라던 내 옆에 있어 달라던\n",
    "그리도 나가고 싶으면 일찍이라도 들어와 달라던\n",
    "아내의 간절한 목소리가 들려와\n",
    "나를 원망하듯 비는 점점 거세져\n",
    "싸늘히 식어가는 아내가 떠올라 걱정은 더해져\n",
    "난 몰라 오늘은 운수 좋은 날\n",
    "난 맨날 이렇게 살 수 있으면 얼마나 좋을까\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어'],\n",
       " ['▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아'],\n",
       " ['▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십', '▁전']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 줄 단위로 tokenize\n",
    "doc = [vocab.encode_as_pieces(line) for line in string.split(\"\\n\")]\n",
    "doc[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최대 길이\n",
    "n_test_seq = 64\n",
    "# 최소 길이\n",
    "min_seq = 8\n",
    "# [CLS], tokens_a, [SEB], tokens_b, [SEP]\n",
    "max_seq = n_test_seq - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_chunk: 5 62 [['▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어'], ['▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아'], ['▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십', '▁전'], ['▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에'], ['▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러']]\n",
      "tokens_a: 50 ['▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에']\n",
      "tokens_b: 12 ['▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러']\n",
      "\n",
      "current_chunk: 6 71 [['▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔'], ['▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내'], ['▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던'], ['▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '▁살', '▁수', '▁있어'], ['▁집', '으로', '▁돌아', '가는', '▁길', '▁난', '▁문', '득', '▁떠', '올', '라'], ['▁아내', '의', '▁목', '소', '리가', '▁거', '칠', '어', '만', '▁가는', '▁희', '박', '한', '▁숨', '소', '리가']]\n",
      "tokens_a: 55 ['▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '▁살', '▁수', '▁있어', '▁집', '으로', '▁돌아', '가는', '▁길', '▁난', '▁문', '득', '▁떠', '올', '라']\n",
      "tokens_b: 16 ['▁아내', '의', '▁목', '소', '리가', '▁거', '칠', '어', '만', '▁가는', '▁희', '박', '한', '▁숨', '소', '리가']\n",
      "\n",
      "current_chunk: 5 73 [['▁오늘', '은', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '▁달', '라', '던'], ['▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일', '찍', '이라', '도', '▁들어', '와', '▁달', '라', '던'], ['▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와'], ['▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져'], ['▁싸', '늘', '히', '▁식', '어', '가는', '▁아내', '가', '▁떠', '올', '라', '▁', '걱', '정은', '▁더', '해', '져']]\n",
      "tokens_a: 56 ['▁오늘', '은', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '▁달', '라', '던', '▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일', '찍', '이라', '도', '▁들어', '와', '▁달', '라', '던', '▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와', '▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져']\n",
      "tokens_b: 17 ['▁싸', '늘', '히', '▁식', '어', '가는', '▁아내', '가', '▁떠', '올', '라', '▁', '걱', '정은', '▁더', '해', '져']\n",
      "\n",
      "current_chunk: 2 22 [['▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날'], ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까']]\n",
      "tokens_a: 9 ['▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날']\n",
      "tokens_b: 13 ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "current_chunk = []  # line 단위 tokens\n",
    "current_length = 0\n",
    "for i in range(len(doc)):  # doc 전체를 loop\n",
    "    current_chunk.append(doc[i])  # line 단위로 추가\n",
    "    current_length += len(doc[i])  # current_chunk의 token 수\n",
    "    if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):  # 마지막 줄 이거나 길이가 max_seq 이상 인 경우, 학습 데이터를 만듭니다. \n",
    "        print(\"current_chunk:\", len(current_chunk), current_length, current_chunk)\n",
    "\n",
    "        #######################################\n",
    "        # token a\n",
    "        a_end = 1\n",
    "        if 1 < len(current_chunk):\n",
    "            a_end = random.randrange(1, len(current_chunk))\n",
    "        tokens_a = []\n",
    "        for j in range(a_end):\n",
    "            tokens_a.extend(current_chunk[j])\n",
    "        # token b\n",
    "        tokens_b = []\n",
    "        for j in range(a_end, len(current_chunk)):\n",
    "            tokens_b.extend(current_chunk[j])\n",
    "          \n",
    "        print(\"tokens_a:\", len(tokens_a), tokens_a)\n",
    "        print(\"tokens_b:\", len(tokens_b), tokens_b)\n",
    "        #######################################\n",
    "        print()\n",
    "\n",
    "        current_chunk = []\n",
    "        current_length = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_tokens(tokens_a, tokens_b, max_seq):\n",
    "    \"\"\"\n",
    "    tokens_a, tokens_b의 길이를 줄임 최대 길이: max_seq\n",
    "    :param tokens_a: tokens A\n",
    "    :param tokens_b: tokens B\n",
    "    :param max_seq: 두 tokens 길이의 최대 값\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_seq:\n",
    "            break\n",
    "\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            del tokens_a[0]\n",
    "        else:\n",
    "            tokens_b.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_chunk: 5 62 [['▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어'], ['▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아'], ['▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십', '▁전'], ['▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에'], ['▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러']]\n",
      "is_next: 0\n",
      "tokens_a: 28 ['▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러']\n",
      "tokens_b: 33 ['▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십']\n",
      "\n",
      "current_chunk: 6 71 [['▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔'], ['▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내'], ['▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던'], ['▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '▁살', '▁수', '▁있어'], ['▁집', '으로', '▁돌아', '가는', '▁길', '▁난', '▁문', '득', '▁떠', '올', '라'], ['▁아내', '의', '▁목', '소', '리가', '▁거', '칠', '어', '만', '▁가는', '▁희', '박', '한', '▁숨', '소', '리가']]\n",
      "is_next: 0\n",
      "tokens_a: 39 ['▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '▁살', '▁수', '▁있어', '▁집', '으로', '▁돌아', '가는', '▁길', '▁난', '▁문', '득', '▁떠', '올', '라', '▁아내', '의', '▁목', '소', '리가', '▁거', '칠', '어', '만', '▁가는', '▁희', '박', '한', '▁숨', '소', '리가']\n",
      "tokens_b: 22 ['▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내']\n",
      "\n",
      "current_chunk: 5 73 [['▁오늘', '은', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '▁달', '라', '던'], ['▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일', '찍', '이라', '도', '▁들어', '와', '▁달', '라', '던'], ['▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와'], ['▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져'], ['▁싸', '늘', '히', '▁식', '어', '가는', '▁아내', '가', '▁떠', '올', '라', '▁', '걱', '정은', '▁더', '해', '져']]\n",
      "is_next: 1\n",
      "tokens_a: 17 ['▁오늘', '은', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '▁달', '라', '던']\n",
      "tokens_b: 44 ['▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일', '찍', '이라', '도', '▁들어', '와', '▁달', '라', '던', '▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와', '▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져', '▁싸', '늘', '히', '▁식', '어']\n",
      "\n",
      "current_chunk: 2 22 [['▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날'], ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까']]\n",
      "is_next: 1\n",
      "tokens_a: 9 ['▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날']\n",
      "tokens_b: 13 ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "current_chunk = []  # line 단위 tokens\n",
    "current_length = 0\n",
    "for i in range(len(doc)):  # doc 전체를 loop\n",
    "    current_chunk.append(doc[i])  # line 단위로 추가\n",
    "    current_length += len(doc[i])  # current_chunk의 token 수\n",
    "    if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):  # 마지막 줄 이거나 길이가 max_seq 이상 인 경우\n",
    "        print(\"current_chunk:\", len(current_chunk), current_length, current_chunk)\n",
    "\n",
    "        # token a\n",
    "        a_end = 1\n",
    "        if 1 < len(current_chunk):\n",
    "            a_end = random.randrange(1, len(current_chunk))\n",
    "        tokens_a = []\n",
    "        for j in range(a_end):\n",
    "            tokens_a.extend(current_chunk[j])\n",
    "        # token b\n",
    "        tokens_b = []\n",
    "        for j in range(a_end, len(current_chunk)):\n",
    "            tokens_b.extend(current_chunk[j])\n",
    "\n",
    "        #######################################\n",
    "        if random.random() < 0.5:  # 50% 확률로 swap\n",
    "            is_next = 0     #False\n",
    "            tokens_t = tokens_a\n",
    "            tokens_a = tokens_b\n",
    "            tokens_b = tokens_t\n",
    "        else:\n",
    "            is_next = 1    #True\n",
    "        # max_seq 보다 큰 경우 길이 조절\n",
    "        trim_tokens(tokens_a, tokens_b, max_seq)\n",
    "        assert 0 < len(tokens_a)\n",
    "        assert 0 < len(tokens_b)\n",
    "\n",
    "        print(\"is_next:\", is_next)\n",
    "        print(\"tokens_a:\", len(tokens_a), tokens_a)\n",
    "        print(\"tokens_b:\", len(tokens_b), tokens_b)\n",
    "        #######################################\n",
    "        print()\n",
    "\n",
    "        current_chunk = []\n",
    "        current_length = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_chunk: 5 62 [['▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어'], ['▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아'], ['▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십', '▁전'], ['▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에'], ['▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러']]\n",
      "is_next: 0\n",
      "tokens_a: 12 ['▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러']\n",
      "tokens_b: 49 ['▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼']\n",
      "tokens: 64 ['[CLS]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '[SEP]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '[SEP]']\n",
      "segment: 64 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "masked tokens: 64 ['[CLS]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '[SEP]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '[MASK]', '[MASK]', '[MASK]', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '[MASK]', '▁번', '에', '▁삼', '십', '가로', '[MASK]', '[MASK]', '▁번', '[MASK]', '[MASK]', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '[SEP]']\n",
      "masked index: 9 [25, 26, 27, 36, 41, 42, 43, 45, 46]\n",
      "masked label: 9 ['▁그', '날', '은', '▁첫', '▁전', '▁둘', '째', '▁오', '십']\n",
      "\n",
      "current_chunk: 6 71 [['▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔'], ['▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내'], ['▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던'], ['▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '▁살', '▁수', '▁있어'], ['▁집', '으로', '▁돌아', '가는', '▁길', '▁난', '▁문', '득', '▁떠', '올', '라'], ['▁아내', '의', '▁목', '소', '리가', '▁거', '칠', '어', '만', '▁가는', '▁희', '박', '한', '▁숨', '소', '리가']]\n",
      "is_next: 1\n",
      "tokens_a: 12 ['▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔']\n",
      "tokens_b: 49 ['▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '▁살', '▁수', '▁있어', '▁집', '으로', '▁돌아', '가는', '▁길', '▁난', '▁문', '득', '▁떠', '올', '라', '▁아내', '의', '▁목', '소', '리가', '▁거']\n",
      "tokens: 64 ['[CLS]', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '[SEP]', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '▁살', '▁수', '▁있어', '▁집', '으로', '▁돌아', '가는', '▁길', '▁난', '▁문', '득', '▁떠', '올', '라', '▁아내', '의', '▁목', '소', '리가', '▁거', '[SEP]']\n",
      "segment: 64 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "masked tokens: 64 ['[CLS]', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '暴', '°', '[SEP]', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '[MASK]', '▁수', '▁있어', '▁집', '으로', '▁돌아', '가는', '[MASK]', '▁난', '▁문', '득', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁목', '소', '리가', '▁거', '[SEP]']\n",
      "masked index: 9 [11, 12, 43, 50, 54, 55, 56, 57, 58]\n",
      "masked label: 9 ['▁적', '셔', '▁살', '▁길', '▁떠', '올', '라', '▁아내', '의']\n",
      "\n",
      "current_chunk: 5 73 [['▁오늘', '은', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '▁달', '라', '던'], ['▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일', '찍', '이라', '도', '▁들어', '와', '▁달', '라', '던'], ['▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와'], ['▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져'], ['▁싸', '늘', '히', '▁식', '어', '가는', '▁아내', '가', '▁떠', '올', '라', '▁', '걱', '정은', '▁더', '해', '져']]\n",
      "is_next: 1\n",
      "tokens_a: 17 ['▁오늘', '은', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '▁달', '라', '던']\n",
      "tokens_b: 44 ['▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일', '찍', '이라', '도', '▁들어', '와', '▁달', '라', '던', '▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와', '▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져', '▁싸', '늘', '히', '▁식', '어']\n",
      "tokens: 64 ['[CLS]', '▁오늘', '은', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '▁달', '라', '던', '[SEP]', '▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일', '찍', '이라', '도', '▁들어', '와', '▁달', '라', '던', '▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와', '▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져', '▁싸', '늘', '히', '▁식', '어', '[SEP]']\n",
      "segment: 64 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "masked tokens: 64 ['[CLS]', '[MASK]', '[MASK]', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '[MASK]', '[MASK]', '[MASK]', '[SEP]', '▁그리', '도', '▁나가', '고', '▁싶', '으면', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁들어', '와', '▁달', '라', '던', '▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와', '▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져', '▁싸', '늘', '히', '▁식', '어', '[SEP]']\n",
      "masked index: 9 [1, 2, 15, 16, 17, 25, 26, 27, 28]\n",
      "masked label: 9 ['▁오늘', '은', '▁달', '라', '던', '▁일', '찍', '이라', '도']\n",
      "\n",
      "current_chunk: 2 22 [['▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날'], ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까']]\n",
      "is_next: 0\n",
      "tokens_a: 13 ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까']\n",
      "tokens_b: 9 ['▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날']\n",
      "tokens: 25 ['[CLS]', '▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까', '[SEP]', '▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날', '[SEP]']\n",
      "segment: 25 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "masked tokens: 25 ['[CLS]', '▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까', '[SEP]', '▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날', '[SEP]']\n",
      "masked index: 3 [7, 8, 15]\n",
      "masked label: 3 ['▁있', '으면', '▁난']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "instances = []\n",
    "current_chunk = []  # line 단위 tokens\n",
    "current_length = 0\n",
    "for i in range(len(doc)):  # doc 전체를 loop\n",
    "    current_chunk.append(doc[i])  # line 단위로 추가\n",
    "    current_length += len(doc[i])  # current_chunk의 token 수\n",
    "    if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):  # 마지막 줄 이거나 길이가 max_seq 이상 인 경우\n",
    "        print(\"current_chunk:\", len(current_chunk), current_length, current_chunk)\n",
    "\n",
    "        # token a\n",
    "        a_end = 1\n",
    "        if 1 < len(current_chunk):\n",
    "            a_end = random.randrange(1, len(current_chunk))\n",
    "        tokens_a = []\n",
    "        for j in range(a_end):\n",
    "            tokens_a.extend(current_chunk[j])\n",
    "        # token b\n",
    "        tokens_b = []\n",
    "        for j in range(a_end, len(current_chunk)):\n",
    "            tokens_b.extend(current_chunk[j])\n",
    "\n",
    "        if random.random() < 0.5:  # 50% 확률로 swap\n",
    "            is_next = 0    # False\n",
    "            tokens_t = tokens_a\n",
    "            tokens_a = tokens_b\n",
    "            tokens_b = tokens_t\n",
    "        else:\n",
    "            is_next = 1   # True\n",
    "        # max_seq 보다 큰 경우 길이 조절\n",
    "        trim_tokens(tokens_a, tokens_b, max_seq)\n",
    "        assert 0 < len(tokens_a)\n",
    "        assert 0 < len(tokens_b)\n",
    "\n",
    "        print(\"is_next:\", is_next)\n",
    "        print(\"tokens_a:\", len(tokens_a), tokens_a)\n",
    "        print(\"tokens_b:\", len(tokens_b), tokens_b)\n",
    "        #######################################\n",
    "\n",
    "        # tokens & segment 생성\n",
    "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
    "        segment = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
    "        print(\"tokens:\", len(tokens), tokens)\n",
    "        print(\"segment:\", len(segment), segment)\n",
    "        \n",
    "        # mask\n",
    "        tokens, mask_idx, mask_label = create_pretrain_mask(tokens, int((len(tokens) - 3) * 0.15), vocab_list)\n",
    "        print(\"masked tokens:\", len(tokens), tokens)\n",
    "        print(\"masked index:\", len(mask_idx), mask_idx)\n",
    "        print(\"masked label:\", len(mask_label), mask_label)\n",
    "\n",
    "        instance = {\n",
    "            \"tokens\": tokens,\n",
    "            \"segment\": segment,\n",
    "            \"is_next\": is_next,\n",
    "            \"mask_idx\": mask_idx,\n",
    "            \"mask_label\": mask_label\n",
    "        }\n",
    "        instances.append(instance)\n",
    "        #######################################\n",
    "        print()\n",
    "\n",
    "        current_chunk = []\n",
    "        current_length = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['[CLS]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '[SEP]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '[MASK]', '[MASK]', '[MASK]', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '[MASK]', '▁번', '에', '▁삼', '십', '가로', '[MASK]', '[MASK]', '▁번', '[MASK]', '[MASK]', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [25, 26, 27, 36, 41, 42, 43, 45, 46], 'mask_label': ['▁그', '날', '은', '▁첫', '▁전', '▁둘', '째', '▁오', '십']}\n",
      "{'tokens': ['[CLS]', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '暴', '°', '[SEP]', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '[MASK]', '▁수', '▁있어', '▁집', '으로', '▁돌아', '가는', '[MASK]', '▁난', '▁문', '득', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁목', '소', '리가', '▁거', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [11, 12, 43, 50, 54, 55, 56, 57, 58], 'mask_label': ['▁적', '셔', '▁살', '▁길', '▁떠', '올', '라', '▁아내', '의']}\n",
      "{'tokens': ['[CLS]', '[MASK]', '[MASK]', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '[MASK]', '[MASK]', '[MASK]', '[SEP]', '▁그리', '도', '▁나가', '고', '▁싶', '으면', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁들어', '와', '▁달', '라', '던', '▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와', '▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져', '▁싸', '늘', '히', '▁식', '어', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [1, 2, 15, 16, 17, 25, 26, 27, 28], 'mask_label': ['▁오늘', '은', '▁달', '라', '던', '▁일', '찍', '이라', '도']}\n",
      "{'tokens': ['[CLS]', '▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까', '[SEP]', '▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [7, 8, 15], 'mask_label': ['▁있', '으면', '▁난']}\n"
     ]
    }
   ],
   "source": [
    "# 최종 데이터셋 결과 확인\n",
    "for instance in instances:\n",
    "    print(instance)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔶 create_pretrain_instances() : Next Sentence Prediction을 위한 코퍼스 생성 메소드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list):\n",
    "    \"\"\"\n",
    "    doc별 pretrain 데이터 생성\n",
    "    \"\"\"\n",
    "    # for CLS], [SEP], [SEP]\n",
    "    max_seq = n_seq - 3\n",
    "\n",
    "    instances = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    for i in range(len(doc)):\n",
    "        current_chunk.append(doc[i])  # line\n",
    "        current_length += len(doc[i])\n",
    "        if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):\n",
    "            # token a\n",
    "            a_end = 1\n",
    "            if 1 < len(current_chunk):\n",
    "                a_end = random.randrange(1, len(current_chunk))\n",
    "            tokens_a = []\n",
    "            for j in range(a_end):\n",
    "                tokens_a.extend(current_chunk[j])\n",
    "            # token b\n",
    "            tokens_b = []\n",
    "            for j in range(a_end, len(current_chunk)):\n",
    "                tokens_b.extend(current_chunk[j])\n",
    "\n",
    "            if random.random() < 0.5:  # 50% 확률로 swap\n",
    "                is_next = 0\n",
    "                tokens_t = tokens_a\n",
    "                tokens_a = tokens_b\n",
    "                tokens_b = tokens_t\n",
    "            else:\n",
    "                is_next = 1\n",
    "            # max_seq 보다 큰 경우 길이 조절\n",
    "            trim_tokens(tokens_a, tokens_b, max_seq)\n",
    "            assert 0 < len(tokens_a)\n",
    "            assert 0 < len(tokens_b)\n",
    "            # tokens & aegment 생성\n",
    "            tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
    "            segment = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
    "            # mask\n",
    "            tokens, mask_idx, mask_label = create_pretrain_mask(tokens, int((len(tokens) - 3) * mask_prob), vocab_list)\n",
    "\n",
    "            instance = {\n",
    "                \"tokens\": tokens,\n",
    "                \"segment\": segment,\n",
    "                \"is_next\": is_next,\n",
    "                \"mask_idx\": mask_idx,\n",
    "                \"mask_label\": mask_label\n",
    "            }\n",
    "            instances.append(instance)\n",
    "\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['[CLS]', '▁번', '에', '▁삼', '십', '[MASK]', '▁둘', '째', 'え', '▁오', '십', '▁전', '▁오', '랜', '만에', '[MASK]', '[MASK]', '[MASK]', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[SEP]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [5, 8, 15, 16, 17, 36, 37, 38, 39], 'mask_label': ['▁전', '▁번', '▁받아', '보', '는', '▁눈', '물이', '▁흘', '러']}\n",
      "{'tokens': ['[CLS]', '[MASK]', '[MASK]', '▁몇', '▁달', '[MASK]', '▁전', '부터', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '[MASK]', '▁수', '▁있어', '▁집', '으로', '▁돌아', '가는', '[MASK]', '▁난', '▁문', '득', '▁떠', '올', '라', '[SEP]', '▁아내', '의', '▁목', '소', '리가', '▁거', '칠', '어', '만', '▁가는', '▁희', '박', '한', '▁숨', '소', '리가', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [1, 2, 5, 8, 9, 10, 11, 32, 39], 'mask_label': ['▁적', '셔', '▁포', '▁콜', '록', '거', '리는', '▁살', '▁길']}\n",
      "{'tokens': ['[CLS]', '에', '▁있어', '▁달', '라', '던', '▁그리', '도', '[MASK]', '[MASK]', '▁싶', '으면', '▁일', '찍', '이라', '도', '▁들어', '와', '▁달', '라', '던', '▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와', '[SEP]', '▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '썹', '숙', '촐', '▁싸', '늘', '히', '▁식', '어', '가는', '▁아내', '가', '▁떠', '올', '라', '[MASK]', '[MASK]', '[MASK]', '▁더', '해', '져', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [2, 8, 9, 43, 44, 45, 57, 58, 59], 'mask_label': ['▁있어', '▁나가', '고', '▁거', '세', '져', '▁', '걱', '정은']}\n",
      "{'tokens': ['[CLS]', '▁난', '[MASK]', '[MASK]', '▁이렇게', '▁살', '[MASK]', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까', '[SEP]', '▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [2, 3, 6], 'mask_label': ['▁맨', '날', '▁수']}\n"
     ]
    }
   ],
   "source": [
    "instances = create_pretrain_instances(vocab, doc, n_test_seq, 0.15, vocab_list)\n",
    "\n",
    "# 최종 데이터셋 결과 확인\n",
    "for instance in instances:\n",
    "    print(instance)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🧐 Mask가 잘 설정된 것을 볼 수 있다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.데이터 전처리 (3) 데이터셋 완성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3957761"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_file = './bert_pretrain/data/kowiki.txt'\n",
    "\n",
    "# line count 확인\n",
    "total = 0\n",
    "with open(corpus_file, 'r') as in_f:\n",
    "    for line in in_f:\n",
    "        total += 1\n",
    "\n",
    "total   # 3957761"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "422f231137c64ed88fcbb1b25e08d127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3957761 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 lines : ['▁지', '미', '▁카', '터']\n",
      "['▁제임스', '▁얼', '▁\"', '지', '미', '\"', '▁카', '터', '▁주', '니어', '(,', '▁192', '4', '년', '▁10', '월', '▁1', '일', '▁~', '▁)', '는', '▁민주', '당', '▁출신', '▁미국', '▁3', '9', '번째', '▁대통령', '▁(19', '7', '7', '년', '▁~', '▁1981', '년', ')', '이다', '.']\n",
      "['▁그는', '▁2002', '년', '▁말', '▁인', '권', '과', '▁중', '재', '▁역할', '에', '▁대한', '▁공', '로를', '▁인정', '받아', '▁노', '벨', '▁평화', '상을', '▁받', '게', '▁되었다', '.']\n",
      "\n",
      "14 lines : ['▁수학']\n",
      "['▁수학', '(', '數', '學', ',', '▁)', '은', '▁양', ',', '▁구조', ',', '▁공간', ',', '▁변화', ',', '▁미', '적', '분', '▁등의', '▁개념', '을', '▁다루', '는', '▁학', '문', '이다', '.', '▁현대', '▁수학', '은', '▁형식', '▁논', '리를', '▁이용', '해서', '▁공', '리로', '▁구성된', '▁추', '상', '적', '▁구조를', '▁연구', '하는', '▁학', '문', '으로', '▁여겨', '지', '기도', '▁한다', '.', '▁수학', '은', '▁그', '▁구조', '와', '▁발전', '▁과정', '에서는', '▁자연', '과학', '에', '▁속하는', '▁물리', '학을', '▁비롯한', '▁다른', '▁학', '문', '들과', '▁깊', '은', '▁연', '관을', '▁맺', '고', '▁있다', '.', '▁하지만', ',', '▁어느', '▁과학', '의', '▁분야', '들과', '는', '▁달리', ',', '▁자연', '계에서', '▁관측', '되지', '▁않는', '▁개념', '들에', '▁대해서', '까지', '▁이론', '을', '▁일반', '화', '▁및', '▁추', '상', '화', '시', '킬', '▁수', '▁있다는', '▁차', '이가', '▁있다고', '▁한다', '.', '▁수', '학자', '들은', '▁그러', '한', '▁개념', '들에', '▁대해서', '▁추', '측', '을', '▁하고', ',', '▁적', '절', '하게', '▁선택', '된', '▁정의', '와', '▁공', '리', '로부터', '의', '▁엄', '밀', '한', '▁연', '역을', '▁통해', '서', '▁추', '측', '들의', '▁진', '위를', '▁파', '악', '한다', '.']\n",
      "['▁수', '학의', '▁기초', '를', '▁확', '실', '히', '▁세', '우', '기', '▁위해', ',', '▁수', '리', '논', '리', '학과', '▁집합', '론', '이', '▁발전', '하였고', ',', '▁이와', '▁더불어', '▁범', '주', '론', '이', '▁최근', '에도', '▁발전', '되고', '▁있다', '.', '▁“', '근', '본', '▁위', '기', '”', '라는', '▁말', '은', '▁대', '략', '▁19', '00', '년', '에서', '▁1930', '년', '▁사이에', '▁일어난', ',', '▁수', '학의', '▁엄', '밀', '한', '▁기초', '에', '▁대한', '▁탐', '구를', '▁상징', '적으로', '▁보여', '주는', '▁말이다', '.', '▁수', '학의', '▁엄', '밀', '한', '▁기초', '에', '▁대한', '▁몇', '▁가지', '▁의견', '▁불', '일', '치는', '▁오늘날', '에도', '▁계속', '되고', '▁있다', '.', '▁수', '학의', '▁기초', '에', '▁대한', '▁위', '기는', '▁그', '▁당시', '▁수많은', '▁논', '쟁', '에', '▁의해', '▁촉', '발', '되었으며', ',', '▁그', '▁논', '쟁', '에는', '▁칸', '토', '어의', '▁집합', '론', '과', '▁브라', '우', '어', '-', '힐', '베', '르트', '▁논', '쟁', '이', '▁포함', '되었다', '.']\n",
      "\n",
      "4 lines : ['▁수학', '▁상', '수']\n",
      "['▁수학', '에서', '▁상', '수', '란', '▁그', '▁값', '이', '▁변', '하지', '▁않는', '▁불', '변', '량', '으로', ',', '▁변', '수의', '▁반대', '말', '이다', '.', '▁물리', '▁상', '수', '와는', '▁달리', ',', '▁수학', '▁상', '수는', '▁물리', '적', '▁측정', '과는', '▁상', '관', '없이', '▁정의', '된다', '.']\n",
      "['▁특정', '▁수학', '▁상', '수', ',', '▁예를', '▁들', '면', '▁골', '롬', '-', '딕', '맨', '▁상', '수', ',', '▁프랑', '세', '즈', '-', '로', '빈', '슨', '▁상', '수', ',', '▁formula', '_1', ',', '▁레', '비', '▁상', '수', '같은', '▁상', '수는', '▁다른', '▁수학', '상', '수', '▁또는', '▁함수', '와', '▁약', '한', '▁상', '관', '관', '계', '▁또는', '▁강한', '▁상', '관', '관', '계를', '▁갖', '는다', '.']\n",
      "\n",
      "10 lines : ['▁문학']\n",
      "['▁문학', '(', '文', '學', ')', '은', '▁언', '어를', '▁예술', '적', '▁표현', '의', '▁제', '재', '로', '▁삼', '아', '▁새로운', '▁의미', '를', '▁창', '출', '하여', ',', '▁인간', '과', '▁사회', '를', '▁진', '실', '되', '게', '▁묘사', '하는', '▁예술', '의', '▁하', '위', '분', '야', '이다', '.', '▁간', '단', '하게', '▁설명', '하면', ',', '▁언', '어를', '▁통해', '▁인간의', '▁삶', '을', '▁미', '적', '(', '美', '的', ')', '으로', '▁형', '상', '화', '한', '▁것이라고', '▁볼', '▁수', '▁있다', '.', '▁문학', '은', '▁원래', '▁문', '예', '(', '文', '藝', ')', '라고', '▁부', '르는', '▁것이', '▁', '옳', '으며', ',', '▁문', '학을', '▁학', '문', '의', '▁대상', '으로서', '▁탐', '구', '하는', '▁학', '문', '의', '▁명칭', '▁역시', '▁문', '예', '학', '이다', '.', '▁문', '예', '학', '은', '▁음악', '사', '학', ',', '▁미술', '사', '학', '▁등과', '▁함께', '▁예술', '학의', '▁핵', '심', '분', '야', '로서', '▁인', '문', '학의', '▁하', '위', '범', '주에', '▁포함', '된다', '.']\n",
      "['▁반', '영', '론', '적', '▁관', '점에', '▁의한', '▁감', '상은', '▁작품', '을', '▁창', '작', '된', '▁당시', '▁시대', '▁정', '황', '과', '▁연결', '시켜', '▁감', '상', '하는', '▁입', '장', '이고', ',', '▁내', '재', '적', '▁관', '점', '의', '▁감', '상은', '▁작품', '의', '▁형식', ',', '▁내용', '에', '▁국', '한', '하여', '▁감', '상', '하는', '▁것이다', '.', '▁표현', '론', '적', '▁관', '점', '의', '▁감', '상은', '▁작가', '의', '▁전기', '적', '▁사실', '과', '▁작품', '을', '▁연결', '시켜', '▁감', '상', '하는', '▁것이', '고', ',', '▁수용', '론', '적', '▁관', '점', '의', '▁감', '상은', '▁독', '자와', '▁작품', '을', '▁연결', '시켜', '▁감', '상', '하는', '▁것을', '▁말한다', '.']\n",
      "\n",
      "10 lines : ['▁나라', '▁목록']\n",
      "['▁이', '▁문', '서는', '▁나라', '▁목록', '이며', ',', '▁전', '▁세계', '▁20', '6', '개', '▁나라', '의', '▁각', '▁현', '황', '과', '▁주', '권', '▁승', '인', '▁정보를', '▁개', '요', '▁형태로', '▁나', '열', '하고', '▁있다', '.']\n",
      "['▁위', '▁목록', '에', '▁포함', '되지', '▁않은', '▁다음', '▁국가', '는', '▁몬', '테', '비', '데', '오', '▁협', '약', '의', '▁모든', '▁조건', '을', '▁만족', '하지', '▁못', '하거나', ',', '▁자주', '적이고', '▁독립', '적', '임을', '▁주장', '하지', '▁않는', '▁국가', '이다', '.']\n",
      "\n",
      "['▁화학']\n",
      "['▁화학', '(', '化', '學', ',', '▁)', '은', '▁물질', '의', '▁성', '질', ',', '▁조성', ',', '▁구조', ',', '▁변화', '▁및', '▁그', '에', '▁수', '반', '하는', '▁에너', '지의', '▁변', '화를', '▁연구', '하는', '▁자연', '과', '학의', '▁한', '▁분야', '이다', '.', '▁물리', '학', '도', '▁역시', '▁물질', '을', '▁다루', '는', '▁학', '문', '이지만', ',', '▁물리', '학', '이', '▁원', '소', '와', '▁화', '합', '물을', '▁모두', '▁포함한', '▁물', '체의', '▁운동', '과', '▁에너', '지', ',', '▁열', '적', '·', '전', '기', '적', '·', '광', '학적', '·', '기', '계', '적', '▁속', '성을', '▁다루', '고', '▁이러한', '▁현', '상', '으로부터', '▁통일', '된', '▁이론', '을', '▁구축', '하려는', '▁것', '과는', '▁달리', '▁화학', '에서는', '▁물질', '▁자', '체를', '▁연구', '▁대상으로', '▁한다', '.', '▁화학', '은', '▁이미', '▁존재', '하는', '▁물질', '을', '▁이용하여', '▁특', '정한', '▁목', '적', '에', '▁맞', '는', '▁새로운', '▁물질', '을', '▁합', '성', '하는', '▁길', '을', '▁제공', '하며', ',', '▁이는', '▁농', '작', '물의', '▁증', '산', ',', '▁질', '병', '의', '▁치료', '▁및', '▁예', '방', ',', '▁에너', '지', '▁효', '율', '▁증', '대', ',', '▁환경', '오', '염', '▁감소', '▁등', '▁여러', '▁가지', '▁이', '점을', '▁제공', '한다', '.']\n",
      "['▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '▁화', '합', '물을', '▁연구', '하는', '▁분', '과', '이다', '.', '▁원래', '▁유', '기', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '해', '낸', '▁화', '합', '물을', '▁뜻', '하였으나', '▁지금', '은', '▁유', '기', '▁화', '합', '물의', '▁범', '위가', '▁크게', '▁넓', '어져', '▁탄', '소', '▁사', '슬', '▁또는', '▁탄', '소', '▁고', '리를', '▁가진', '▁모든', '▁화', '합', '물을', '▁뜻', '한다', '.', '▁유', '기', '화', '학의', '▁오', '랜', '▁관', '심', '사는', '▁유', '기', '▁화', '합', '물의', '▁합', '성', '▁메', '커', '니', '즘', '이다', '.', '▁현', '대에', '▁들어', '서', '▁핵', '자', '기', '▁공', '명', '법', '과', '▁X', '선', '▁결정', '학', '▁등이', '▁개발', '되어', '▁유', '기', '▁화', '합', '물', '▁분석', '에', '▁있어서', '▁매우', '▁중요한', '▁방법', '으로', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.']\n"
     ]
    }
   ],
   "source": [
    "# 위키가 주제별로 잘 나눠지는지 여부 확인\n",
    "count = 5\n",
    "\n",
    "with open(corpus_file, 'r') as in_f:\n",
    "    doc = []  # 단락 단위로 문서 저장\n",
    "    for line in tqdm(in_f, total=total):\n",
    "        line = line.strip()\n",
    "        if line == \"\":  # line이 빈줄 일 경우 (새로운 단락)  \n",
    "            if 0 < len(doc):\n",
    "                if 0 < count:\n",
    "                    count -= 1\n",
    "                    print(len(doc), \"lines :\", doc[0])\n",
    "                    print(doc[1])\n",
    "                    print(doc[-1])\n",
    "                    print()\n",
    "                else:\n",
    "                    break\n",
    "                doc = []\n",
    "        else:  # 빈 줄이 아니면 doc에 저장\n",
    "            pieces = vocab.encode_as_pieces(line)    \n",
    "            if 0 < len(pieces):\n",
    "                doc.append(pieces)\n",
    "    if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
    "        print(doc[0])\n",
    "        print(doc[1])\n",
    "        print(doc[-1])\n",
    "        doc = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🧐 주제별로 내용이 잘 분리되어있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d62f22c239c342d88db4fb445bf1d67b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3957761 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc: 21 instances: 10\n",
      "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '[MASK]', '[MASK]', '[MASK]', '▁연구', '하는', '▁분', '과', '이다', '.', '▁원래', '▁유', '기', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [10, 11, 12, 13, 14, 15, 41, 42, 43], 'mask_label': ['▁합', '성', '섬', '유', '등', '의', '▁화', '합', '물을']}\n",
      "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '▁화', '합', '물을', '▁연구', '하는', '▁분', '과', '이다', '.', '▁원래', '▁유', '기', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [23, 24, 25, 26, 27, 53, 54, 55, 56], 'mask_label': ['▁유', '기', '화', '학', '에서', '▁화', '합', '물', '은']}\n",
      "\n",
      "doc: 14 instances: 7\n",
      "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '[MASK]', '[MASK]', '[MASK]', '▁화', '합', '물을', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁원래', '▁유', '기', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [38, 39, 40, 44, 45, 46, 47, 48, 49], 'mask_label': ['▁탄', '소로', '▁이루어진', '▁연구', '하는', '▁분', '과', '이다', '.']}\n",
      "{'tokens': ['[CLS]', '[MASK]', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '[MASK]', '▁화', '합', '물을', '[MASK]', '[MASK]', '▁분', '과', '이다', '.', '[MASK]', '[MASK]', '[MASK]', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '[MASK]', '[MASK]', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [1, 40, 44, 45, 50, 51, 52, 61, 62], 'mask_label': ['으로', '▁이루어진', '▁연구', '하는', '▁원래', '▁유', '기', '▁추', '출']}\n",
      "\n",
      "doc: 4 instances: 2\n",
      "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '▁화', '합', '물을', '[MASK]', '[MASK]', '▁분', '과', '이다', '.', '▁원래', '[MASK]', '[MASK]', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [16, 17, 18, 19, 20, 44, 45, 51, 52], 'mask_label': ['▁고', '분', '자', '물', '질', '▁연구', '하는', '▁유', '기']}\n",
      "{'tokens': ['[CLS]', '[MASK]', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁고', '분', '자', '물', '질', '[MASK]', '[MASK]', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '▁화', '합', '물을', '▁연구', '하는', '▁분', '과', '이다', '.', '▁원래', '▁유', '기', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [1, 10, 11, 12, 13, 14, 15, 21, 22], 'mask_label': ['으로', '▁합', '성', '섬', '유', '등', '의', '▁등', '도']}\n",
      "\n",
      "doc: 10 instances: 5\n",
      "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '許', '▁화', '합', '물을', '▁연구', '하는', '▁분', '과', '이다', '.', '▁원래', '▁유', '기', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '적', '惠', '▁동물', '로부터', '[MASK]', '[MASK]', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [40, 53, 54, 55, 56, 57, 58, 61, 62], 'mask_label': ['▁이루어진', '▁화', '합', '물', '은', '▁식물', '이나', '▁추', '출']}\n",
      "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '[MASK]', '[MASK]', '▁이루어진', '▁화', '합', '물을', '▁연구', '하는', '▁분', '과', '이다', '.', '▁원래', '[MASK]', '[MASK]', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [23, 24, 25, 26, 27, 38, 39, 51, 52], 'mask_label': ['▁유', '기', '화', '학', '에서', '▁탄', '소로', '▁유', '기']}\n",
      "\n",
      "doc: 10 instances: 5\n",
      "{'tokens': ['[CLS]', '[MASK]', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '▁화', '합', '물을', '▁연구', '하는', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁원래', '[MASK]', '[MASK]', '▁화', '합', '물', '은', '[MASK]', '[MASK]', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [1, 46, 47, 48, 49, 51, 52, 57, 58], 'mask_label': ['으로', '▁분', '과', '이다', '.', '▁유', '기', '▁식물', '이나']}\n",
      "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '▁화', '합', '물을', '[MASK]', '[MASK]', '▁분', '과', '이다', '.', '[MASK]', '▁유', '기', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '[MASK]', '[MASK]', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [44, 45, 50, 53, 54, 55, 56, 61, 62], 'mask_label': ['▁연구', '하는', '▁원래', '▁화', '합', '물', '은', '▁추', '출']}\n",
      "\n",
      "doc: 31 instances: 15\n",
      "{'tokens': ['[CLS]', '[MASK]', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '[MASK]', '▁화', '합', '물을', '▁연구', '하는', '▁분', '과', '이다', '.', '▁원래', '[MASK]', '[MASK]', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [1, 23, 24, 25, 26, 27, 40, 51, 52], 'mask_label': ['으로', '▁유', '기', '화', '학', '에서', '▁이루어진', '▁유', '기']}\n",
      "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '[MASK]', '[MASK]', '[MASK]', '▁연구', '하는', '▁분', '과', '이다', '.', '[MASK]', '▁유', '기', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [23, 24, 25, 26, 27, 41, 42, 43, 50], 'mask_label': ['▁유', '기', '화', '학', '에서', '▁화', '합', '물을', '▁원래']}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# instance 생성 기능 확인\n",
    "count = 5\n",
    "\n",
    "with open(corpus_file, 'r') as in_f:\n",
    "    doc = []  # 단락 단위로 문서 저장\n",
    "    for line in tqdm(in_f, total=total):\n",
    "        line = line.strip()\n",
    "        if line == \"\":  # line이 빈줄 일 경우 (새로운 단락)\n",
    "            if 0 < len(doc):\n",
    "                instances = create_pretrain_instances(vocab, doc, n_test_seq, 0.15, vocab_list)\n",
    "                # save\n",
    "                print(\"doc:\", len(doc), \"instances:\", len(instances))\n",
    "                print(instances[0])\n",
    "                print(instances[-1])\n",
    "                print()\n",
    "                doc = []\n",
    "                if 0 < count:  # 테스트를 위해서 부분 처리함\n",
    "                    count -= 1\n",
    "                else:\n",
    "                    break\n",
    "        else:  # doc에 저장\n",
    "            if 0 < len(pieces):\n",
    "                doc.append(pieces)\n",
    "    if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
    "        instances = create_pretrain_instances(doc, 128)\n",
    "        # save\n",
    "        print(\"doc:\", len(doc), \"instances:\", len(instances))\n",
    "        print(instances[0])\n",
    "        print(instances[-1])\n",
    "        print()\n",
    "        doc = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🔶 make_pretrain_data() : BERT pretrain 데이터셋 생성 메소드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pretrain_data(vocab, in_file, out_file, n_seq, mask_prob=0.15):\n",
    "    \"\"\" pretrain 데이터 생성 \"\"\"\n",
    "    def save_pretrain_instances(out_f, doc):\n",
    "        instances = create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list)\n",
    "        for instance in instances:\n",
    "            out_f.write(json.dumps(instance, ensure_ascii=False))\n",
    "            out_f.write(\"\\n\")\n",
    "\n",
    "    # 특수문자 7개를 제외한 vocab_list 생성\n",
    "    vocab_list = []\n",
    "    for id in range(7, len(vocab)):\n",
    "        if not vocab.is_unknown(id):        # 생성되는 단어 목록이 unknown인 경우는 제거합니다. \n",
    "            vocab_list.append(vocab.id_to_piece(id))\n",
    "\n",
    "    # line count 확인\n",
    "    line_cnt = 0\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        for line in in_f:\n",
    "            line_cnt += 1\n",
    "\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        with open(out_file, \"w\") as out_f:\n",
    "            doc = []\n",
    "            for line in tqdm(in_f, total=line_cnt):\n",
    "                line = line.strip()\n",
    "                if line == \"\":  # line이 빈줄 일 경우 (새로운 단락)\n",
    "                    if 0 < len(doc):\n",
    "                        save_pretrain_instances(out_f, doc)\n",
    "                        doc = []\n",
    "                else:  # line이 빈줄이 아닐 경우 tokenize 해서 doc에 저장\n",
    "                    pieces = vocab.encode_as_pieces(line)\n",
    "                    if 0 < len(pieces):\n",
    "                        doc.append(pieces)\n",
    "            if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
    "                save_pretrain_instances(out_f, doc)\n",
    "                doc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcee0437d54543479ce6fed4ea4f0d94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3957761 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pretrain_json_path = './bert_pretrain/data/bert_pre_train.json'\n",
    "\n",
    "make_pretrain_data(vocab, corpus_file, pretrain_json_path, 128)  # 4분 9초"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "918173"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 라인수\n",
    "total = 0\n",
    "with open(pretrain_json_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        total += 1\n",
    "total  # 862285  더 많네 918173 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 0,\n",
       " 0,\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_seq = 128\n",
    "# [CLS], tokens_a, [SEP], tokens_b, [SEP]\n",
    "max_seq = n_seq - 3\n",
    "\n",
    "# 만약 일반적인 Numpy Array에다 데이터를 로딩한다면 이렇게 되겠지만\n",
    "# enc_tokens = np.zeros((total, n_seq), np.int32)\n",
    "# dec_tokens = np.zeros((total, n_seq), np.int32)\n",
    "# labels_nsp = np.zeros((total,), np.int32)\n",
    "# labels_mlm = np.zeros((total, n_seq), np.int32)\n",
    "\n",
    "# np.memmap을 사용하면 메모리를 적은 메모리에서도 대용량 데이터 처리가 가능 함\n",
    "enc_tokens = np.memmap(filename='enc_tokens_8000.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq)) #\n",
    "segments = np.memmap(filename='segments_8000.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "labels_nsp = np.memmap(filename='labels_nsp_8000.memmap', mode='w+', dtype=np.int32, shape=(total,))\n",
    "labels_mlm = np.memmap(filename='labels_mlm_8000.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "\n",
    "\n",
    "enc_tokens[0], enc_tokens[-1], segments[0], segments[-1], labels_nsp[0], labels_nsp[-1], labels_mlm[0], labels_mlm[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48b22f27d7214dd39f406979163101c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/918173 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['[CLS]', '일', '▁~', '[MASK]', '[MASK]', '▁민주', '당', '▁출신', '▁미국', '▁3', '9', '번째', '▁대통령', '▁(19', '7', '7', '년', '[MASK]', '▁1981', '년', ')', '이다', '.', '▁지', '미', '▁카', '터', '는', '[MASK]', '[MASK]', '[MASK]', '▁섬', '터', '▁카운', '티', '▁플', '레', '인', '스', '▁마을', '에서', '▁태어났다', '.', '▁조지', '아', '▁공', '과', '대학교', '를', '▁졸업', '하였다', '.', '▁그', '▁후', '▁해', '군에', '▁들어가', '▁전', '함', '·', '원', '자', '력', '·', '잠', '수', '함', '의', '▁승', '무', '원으로', '▁일', '하였다', '.', '▁195', '3', '년', '▁미국', '▁해군', '▁대', '위로', '▁예', '편', '하였고', '▁이후', '▁땅', '콩', '·', '면', '화', '[MASK]', '▁가', '꿔', '▁많은', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁그의', '▁별', '명이', '▁\"', '땅', '콩', '▁농', '부', '\"', '▁(', 'P', 'e', 'an', 'ut', '▁F', 'ar', 'm', 'er', ')', '로', '[MASK]', '[MASK]', '[MASK]', '[SEP]', '▁지', '미', '▁카', '터', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [3, 4, 9, 10, 11, 17, 28, 29, 30, 90, 94, 95, 96, 97, 98, 119, 120, 121], 'mask_label': ['▁)', '는', '▁3', '9', '번째', '▁~', '▁조지', '아', '주', '▁등을', '▁돈', '을', '▁벌', '었다', '.', '▁알려', '졌다', '.']}\n",
      "enc_token: [5, 3629, 203, 6, 6, 1114, 3724, 788, 243, 49, 3632, 796, 663, 1648, 3682, 3682, 3625, 6, 3009, 3625, 3616, 16, 3599, 18, 3686, 207, 3714, 3602, 6, 6, 6, 630, 3714, 3568, 3835, 429, 3740, 3628, 3626, 1370, 10, 1607, 3599, 1756, 3630, 41, 3644, 830, 3624, 1135, 52, 3599, 13, 81, 87, 1502, 2247, 25, 3779, 3873, 3667, 3631, 3813, 3873, 4196, 3636, 3779, 3601, 249, 3725, 1233, 33, 52, 3599, 479, 3652, 3625, 243, 2780, 14, 1510, 168, 3877, 414, 166, 1697, 4290, 3873, 3703, 3683, 6, 21, 5007, 399, 6, 6, 6, 6, 6, 307, 587, 930, 104, 4313, 4290, 613, 3638, 3718, 99, 3878, 3656, 256, 2543, 309, 337, 3735, 181, 3616, 3603, 6, 6, 6, 4, 18, 3686, 207, 3714, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
      "label_nsp: 0\n",
      "label_mlm: [   0    0    0  241 3602    0    0    0    0   49 3632  796    0    0\n",
      "    0    0    0  203    0    0    0    0    0    0    0    0    0    0\n",
      " 1756 3630 3646    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0  593    0    0    0 1928 3607  813   17\n",
      " 3599    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0  489  376 3599    0    0    0    0\n",
      "    0    0]\n",
      "\n",
      "{'tokens': ['[CLS]', '아', '▁주', '▁상', '원', '▁의원', '▁선거', '에서', '▁낙', '선', '하나', '▁그', '▁선거', '가', '▁부정', '선거', '▁', '였', '음을', '▁입', '증', '하게', '▁되어', '▁당선', '되고', ',', '▁196', '6', '년', '▁조지', '아', '▁주', '▁지', '사', '▁선거', '에', '▁낙', '선', '하지만', '[MASK]', '[MASK]', '▁조지', '아', '▁주', '▁지', '사를', '▁역임', '했다', '.', '▁대통령', '이', '▁되', '기', '▁전', '▁조지', '아', '주', '總', '▁가르', '▁파', '▁두', '번', '▁연', '임', '했으며', ',', '[MASK]', '[MASK]', '▁1975', '년까지', '▁조지', '아', '▁지', '사로', '[MASK]', '[MASK]', '[MASK]', '▁조지', '아', '▁주', '지', '사로', '▁지', '내', '면서', ',', '▁미국', '에', '[MASK]', '▁흑', '인', '▁등', '용', '법을', '▁내', '세', '웠다', '.', '[SEP]', '▁1976', '년', '▁대통령', '▁선거', '에', '▁민주', '당', '▁후보', '로', '▁출', '마', '하여', '▁도', '덕', '주의', '[MASK]', '[MASK]', '▁내', '세', '워', ',', '[MASK]', '[MASK]', '▁누', '르고', '지', '▁달러', '잴', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [39, 40, 57, 58, 59, 66, 67, 74, 75, 76, 88, 114, 115, 120, 121, 124, 125, 126], 'mask_label': ['▁1970', '년', '▁상', '원의', '원을', '▁1971', '년부터', '▁근무', '했다', '.', '▁사는', '▁정책', '으로', '▁포', '드를', '▁당선', '되었다', '.']}\n",
      "enc_token: [5, 3630, 37, 76, 3667, 2378, 822, 10, 1569, 3668, 3293, 13, 822, 3608, 2386, 2163, 3596, 3671, 968, 213, 3929, 173, 607, 2387, 317, 3604, 386, 3673, 3625, 1756, 3630, 37, 18, 3620, 822, 3600, 1569, 3668, 1448, 6, 6, 1756, 3630, 37, 18, 451, 1399, 31, 3599, 663, 3597, 450, 3614, 25, 1756, 3630, 3646, 5400, 2190, 146, 157, 3821, 61, 3773, 531, 3604, 6, 6, 3408, 673, 1756, 3630, 18, 981, 6, 6, 6, 1756, 3630, 37, 3610, 981, 18, 3754, 151, 3604, 243, 3600, 6, 1734, 3628, 50, 3717, 2046, 115, 3692, 1854, 3599, 4, 3306, 3625, 663, 822, 3600, 1114, 3724, 957, 3603, 118, 3674, 54, 75, 4089, 238, 6, 6, 115, 3692, 3964, 3604, 6, 6, 807, 2056, 3610, 2178, 7877, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 1\n",
      "label_mlm: [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0 1922 3625    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0   76  954  927    0    0    0    0    0    0 3371  524    0    0\n",
      "    0    0    0    0 2711   31 3599    0    0    0    0    0    0    0\n",
      "    0    0    0    0 3554    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0 1422    9    0    0    0    0  120 1487    0    0 2387   43\n",
      " 3599    0]\n",
      "\n",
      "{'tokens': ['[CLS]', '[MASK]', '▁개발', '을', '▁촉', '구', '했으나', '▁공', '화', '당의', '[MASK]', '[MASK]', '▁무', '산', '되었다', '.', '▁카', '터', '는', '[MASK]', '[MASK]', '[MASK]', '▁이스라엘', '을', '[MASK]', '[MASK]', '[MASK]', '▁캠', '프', '▁데이', '비', '드에서', '▁안', '와', '르', '▁사', '다', '트', '[MASK]', '[MASK]', '▁메', '나', '헴', '▁베', '긴', '▁수상', '과', '▁함께', '▁중', '동', '▁평', '화를', '▁하나의', '▁캠', '프', '데', '이', '비', '드', '▁협', '정을', '▁체결', '했다', '.', '[SEP]', '▁그러나', '[MASK]', '▁공', '화', '당', '과', '[MASK]', '▁유대', '인', '▁단', '체의', '▁반', '발', '을', '▁일으', '켰', '다', '.', '▁1979', '년', '▁백', '악', '관', '에서', '▁양', '국', '▁간의', '▁평화', '조', '약', '으로', '▁이끌', '어졌다', '.', '▁또한', '▁소련', '과', '▁제', '2', '차', '▁전략', '▁무', '기', '▁제한', '▁협', '상에', '▁조', '인', '했다', '.', '▁카', '터', '는', '▁1970', '년대', '[MASK]', '[MASK]', '▁대한민국', '▁등', '▁인', '권', '▁후', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [1, 10, 11, 19, 20, 21, 24, 25, 26, 38, 39, 52, 66, 71, 100, 101, 120, 121], 'mask_label': ['지', '▁반', '대로', '▁이집', '트', '와', '▁조정', '하여', ',', '▁대통령', '과', '▁위한', '▁이것은', '▁미국의', '▁소련', '과', '▁후반', '▁당시']}\n",
      "enc_token: [5, 6, 570, 3607, 2270, 3653, 1002, 41, 3683, 1549, 6, 6, 108, 3726, 43, 3599, 207, 3714, 3602, 6, 6, 6, 3425, 3607, 6, 6, 6, 2432, 3721, 964, 3694, 3552, 172, 3665, 3699, 15, 3598, 3677, 6, 6, 334, 3637, 5887, 271, 4099, 1010, 3644, 280, 35, 3658, 232, 933, 1896, 2432, 3721, 3736, 3597, 3694, 3681, 617, 666, 2525, 31, 3599, 4, 330, 6, 41, 3683, 3724, 3644, 6, 2670, 3628, 165, 1315, 141, 3720, 3607, 1214, 4174, 3598, 3599, 2998, 3625, 456, 3928, 3708, 10, 230, 3643, 2714, 2793, 3676, 3827, 9, 1436, 2521, 3599, 276, 1303, 3644, 30, 3619, 3751, 2836, 108, 3614, 1956, 617, 1825, 53, 3628, 31, 3599, 207, 3714, 3602, 1922, 596, 6, 6, 410, 50, 42, 3830, 81, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 1\n",
      "label_mlm: [   0 3610    0    0    0    0    0    0    0    0  141  448    0    0\n",
      "    0    0    0    0    0 2703 3677 3665    0    0 3358   54 3604    0\n",
      "    0    0    0    0    0    0    0    0    0    0  663 3644    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0  522    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0 1488    0    0    0\n",
      "    0  679    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0 1303 3644    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0 1841  316    0    0    0    0\n",
      "    0    0]\n",
      "\n",
      "{'tokens': ['[CLS]', '[MASK]', '[MASK]', '▁부', '딪', '혀', '吾', '팡', '▁계열', 'ation', '▁완', '전', '철', '수', '▁대신', '▁6', ',000', '명을', '▁감', '축', '하는', '▁데', '▁그', '쳤다', '.', '▁또한', '▁박', '정', '희', '[MASK]', '[MASK]', '▁인', '권', '▁문제', '▁등', '과의', '▁논란', '으로', '▁불', '협', '화', '음을', '▁', '냈', '으나', ',', '▁1979', '년', '▁6', '월', '▁하', '순', ',', '▁대한민국', '을', '▁방문', '하여', '▁관계', '가', '▁다', '소', '[MASK]', '[MASK]', '[MASK]', '[SEP]', '▁그러나', '▁주', '[MASK]', '▁미국', '▁대사', '관', '▁인', '질', '▁사건', '에서', '▁인', '질', '▁구', '출', '▁실패', '를', '[MASK]', '▁1980', '년', '[MASK]', '[MASK]', '[MASK]', '▁공', '화', '당의', '▁로', '널', '드', '▁레이', '건', '▁후보', '에게', '▁', '져', '[MASK]', '▁재', '선에', '▁실패', '했다', '.', '▁또한', '▁임', '기', '▁말', '기에', '▁터', '진', '▁소련', '의', '▁아', '프가', '니', '스탄', '▁침공', '▁사건', '으로', '[MASK]', '▁1980', '년', '▁하계', '▁올림픽', '에', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [1, 2, 6, 7, 8, 9, 29, 30, 61, 62, 63, 67, 81, 84, 85, 86, 99, 121], 'mask_label': ['▁반', '대에', '▁주', '한', '미', '군은', '▁정', '권의', '▁회복', '되었다', '.', '▁이란', '▁이유로', '▁대통령', '▁선거', '에서', '▁결국', '▁인해']}\n",
      "enc_token: [5, 6, 6, 51, 5148, 4178, 6398, 4758, 2440, 1464, 443, 3640, 3917, 3636, 1083, 126, 847, 859, 209, 3909, 38, 189, 13, 1525, 3599, 276, 338, 3642, 4055, 6, 6, 42, 3830, 551, 50, 786, 2408, 9, 129, 3993, 3683, 968, 3596, 4121, 191, 3604, 2998, 3625, 126, 3662, 27, 3946, 3604, 410, 3607, 2017, 54, 704, 3608, 29, 3688, 6, 6, 6, 4, 330, 37, 6, 243, 2630, 3708, 42, 3892, 636, 10, 42, 3892, 73, 3771, 1580, 3624, 6, 1641, 3625, 6, 6, 6, 41, 3683, 1549, 194, 4044, 3681, 1169, 3803, 957, 114, 3596, 3944, 6, 174, 2087, 1580, 31, 3599, 276, 273, 3614, 150, 329, 870, 3713, 1303, 3601, 26, 2992, 3733, 1324, 3232, 636, 9, 6, 1641, 3625, 2219, 779, 3600, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 0\n",
      "label_mlm: [   0  141  867    0    0    0   37 3612 3686  940    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0   36 2649    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0 3333   43 3599    0    0    0 3289    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0 1828    0    0\n",
      "  663  822   10    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0  875    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0  751    0    0    0    0\n",
      "    0    0]\n",
      "\n",
      "{'tokens': ['[CLS]', '한', '▁뒤', '▁민주', '주의', '▁실', '현', '을', '▁위해', '▁제', '▁3', '세', '계의', '[MASK]', '▁감', '시', '▁활동', '▁및', '▁기', '니', '[MASK]', '[MASK]', '[MASK]', '▁의한', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁질', '병', '▁방', '재', '를', '▁위해', '▁힘', '썼', '다', '.', '▁미국의', '▁빈', '곤', '층', '▁지원', '▁활동', ',', '▁사랑', '의', '[MASK]', '[MASK]', '[MASK]', '▁운동', ',', '▁국제', '▁분', '쟁', '▁중', '재', '▁등의', '[MASK]', '[MASK]', '▁했다', '.', '[SEP]', '[MASK]', '[MASK]', '▁~', '▁1980', '년', '▁대한민국의', '▁정치적', '▁격', '변', '기', '[MASK]', '▁대통령', '이었던', '▁그는', '▁이에', '▁대해', '▁애', '매', '한', '▁태', '도를', '▁보', '였고', ',', '▁이는', '▁후에', '▁대한민국', '▁내에서', '▁고', '조', '되는', '▁반', '미', '▁운동', '의', '▁한', '▁원', '인이', '▁', '됐다', '.', '▁10', '월', '▁26', '일', ',', '▁박', '정', '희', '▁대통령', '이', '▁김', '재', '규', '▁중앙', '정보', '부', '장에', '▁의해', '▁살해', '된', '▁것에', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [13, 20, 21, 22, 24, 25, 26, 27, 28, 29, 49, 50, 51, 60, 61, 65, 66, 75], 'mask_label': ['▁선거', '▁벌', '레', '에', '▁드', '라', '쿤', '쿠', '르', '스', '▁집', '짓', '기', '▁활동', '도', '▁1979', '년', '▁당시의']}\n",
      "enc_token: [5, 3612, 339, 1114, 238, 158, 3756, 3607, 231, 30, 49, 3692, 1655, 6, 209, 3623, 375, 228, 24, 3733, 6, 6, 6, 1333, 6, 6, 6, 6, 6, 6, 761, 3886, 96, 3729, 3624, 231, 946, 4437, 3598, 3599, 679, 1413, 4234, 4083, 770, 375, 3604, 1424, 3601, 6, 6, 6, 886, 3604, 605, 147, 3972, 35, 3729, 507, 6, 6, 345, 3599, 4, 6, 6, 203, 1641, 3625, 447, 2844, 1032, 3889, 3614, 6, 663, 1278, 202, 695, 433, 442, 3823, 3612, 227, 701, 47, 2470, 3604, 594, 1140, 410, 3427, 70, 3676, 267, 141, 3686, 886, 3601, 34, 130, 828, 3596, 1027, 3599, 160, 3662, 980, 3629, 3604, 338, 3642, 4055, 663, 3597, 200, 3729, 3958, 782, 2275, 3638, 1313, 355, 2591, 3711, 2057, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 0\n",
      "label_mlm: [   0    0    0    0    0    0    0    0    0    0    0    0    0  822\n",
      "    0    0    0    0    0    0  813 3740 3600    0  311 3635 4956 3937\n",
      " 3699 3626    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0  313 4333 3614    0    0    0    0\n",
      "    0    0    0    0  375 3627    0    0    0 2998 3625    0    0    0\n",
      "    0    0    0    0    0 3198    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "\n",
      "{'tokens': ['[CLS]', '▁미국', '이', '▁북', '핵', '▁위', '기', ',', '▁코', '소', '보', '▁전쟁', ',', '▁이', '라크', '▁전쟁', '과', '▁같이', '▁미국', '이', '▁군사', '적', '▁행', '동을', '[MASK]', '[MASK]', '[MASK]', '▁선택', '하는', '[MASK]', '[MASK]', '▁사고', '를', '▁버', '리고', '▁군사', '적', '▁행', '동을', '[MASK]', '[MASK]', '[MASK]', '▁행', '위에', '[MASK]', '▁깊', '은', '▁유', '감을', '▁표시', '▁하며', '▁미국의', '▁군사', '적', '[MASK]', '[MASK]', '▁강한', '▁반대', '[MASK]', '[MASK]', '▁보', '이고', '▁있다', '.', '[SEP]', '▁특히', '▁국제', '▁분', '쟁', '▁조', '정을', '▁위해', '▁북한', '의', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁아이', '티', '의', '▁세', '드', '라스', '▁장', '군', ',', '▁팔', '레', '인', '스타', '인의', '▁하', '마', '스', ',', '▁보', '스', '니아', '의', '▁세르', '비아', '계', '▁정', '권', '▁같이', '[MASK]', '▁정부', '에', '▁대해', '▁협', '상을', '▁거부', '하면서', '▁사', '태', '의', '▁위', '기를', '▁초', '래', '한', '▁인물', '▁및', '▁단', '체를', '▁직접', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [24, 25, 26, 29, 30, 39, 40, 41, 44, 54, 55, 58, 59, 74, 75, 76, 77, 106], 'mask_label': ['▁최', '후', '로', '▁전통', '적', '▁선', '행', '하는', '▁대해', '▁활동', '에', '▁입', '장을', '▁김', '일', '성', ',', '▁미국']}\n",
      "enc_token: [5, 243, 3597, 251, 4166, 45, 3614, 3604, 258, 3688, 3672, 506, 3604, 8, 3553, 506, 3644, 733, 243, 3597, 1251, 3657, 236, 1630, 6, 6, 6, 1716, 38, 6, 6, 1647, 3624, 407, 998, 1251, 3657, 236, 1630, 6, 6, 6, 236, 1157, 6, 1911, 3613, 46, 2196, 2466, 1369, 679, 1251, 3657, 6, 6, 2632, 1217, 6, 6, 47, 458, 28, 3599, 4, 698, 605, 147, 3972, 53, 666, 231, 1877, 3601, 6, 6, 6, 6, 521, 3835, 3601, 74, 3681, 1952, 105, 3722, 3604, 960, 3740, 3628, 935, 692, 27, 3674, 3626, 3604, 47, 3626, 491, 3601, 3191, 852, 3704, 36, 3830, 733, 6, 513, 3600, 433, 617, 460, 2324, 421, 15, 3800, 3601, 45, 333, 192, 3808, 3612, 1179, 228, 165, 1397, 1069, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 1\n",
      "label_mlm: [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0  131 3706 3603    0\n",
      "    0 1307 3657    0    0    0    0    0    0    0    0   57 3752   38\n",
      "    0    0  433    0    0    0    0    0    0    0    0    0  375 3600\n",
      "    0    0  213  480    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0  200 3629 3650 3604    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0  243    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peunj\\AppData\\Local\\Temp\\ipykernel_23612\\767648317.py:16: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
      "C:\\Users\\peunj\\AppData\\Local\\Temp\\ipykernel_23612\\767648317.py:17: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
      "C:\\Users\\peunj\\AppData\\Local\\Temp\\ipykernel_23612\\767648317.py:18: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n"
     ]
    }
   ],
   "source": [
    "# 라인 단위로 처리\n",
    "with open(pretrain_json_path, \"r\") as f:\n",
    "    for i, line in enumerate(tqdm(f, total=total)):\n",
    "        if 5 < i:  # 테스트를 위해서 5개만 확인\n",
    "            break\n",
    "        data = json.loads(line)\n",
    "        # encoder token\n",
    "        enc_token = [vocab.piece_to_id(p) for p in data[\"tokens\"]]\n",
    "        enc_token += [0] * (n_seq - len(enc_token))\n",
    "        # segment\n",
    "        segment = data[\"segment\"]\n",
    "        segment += [0] * (n_seq - len(segment))\n",
    "        # nsp label\n",
    "        label_nsp = data[\"is_next\"]\n",
    "        # mlm label\n",
    "        mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
    "        mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
    "        label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n",
    "        label_mlm[mask_idx] = mask_label\n",
    "\n",
    "        print(data)\n",
    "        print(\"enc_token:\", enc_token)\n",
    "        print(\"segment:\", segment)\n",
    "        print(\"label_nsp:\", label_nsp)\n",
    "        print(\"label_mlm:\", label_mlm)\n",
    "        print()\n",
    "\n",
    "        assert len(enc_token) == len(segment) == len(label_mlm) == n_seq\n",
    "\n",
    "        enc_tokens[i] = enc_token\n",
    "        segments[i] = segment\n",
    "        labels_nsp[i] = label_nsp\n",
    "        labels_mlm[i] = label_mlm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🔶 load_pre_train_data() : 학습에 필요한 데이터를 로딩하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pre_train_data(vocab, filename, n_seq, count=None):\n",
    "    \"\"\"\n",
    "    학습에 필요한 데이터를 로드\n",
    "    :param vocab: vocab\n",
    "    :param filename: 전처리된 json 파일\n",
    "    :param n_seq: 시퀀스 길이 (number of sequence)\n",
    "    :param count: 데이터 수 제한 (None이면 전체)\n",
    "    :return enc_tokens: encoder inputs\n",
    "    :return segments: segment inputs\n",
    "    :return labels_nsp: nsp labels\n",
    "    :return labels_mlm: mlm labels\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            total += 1\n",
    "            # 데이터 수 제한\n",
    "            if count is not None and count <= total:\n",
    "                break\n",
    "    \n",
    "    # np.memmap을 사용하면 메모리를 적은 메모리에서도 대용량 데이터 처리가 가능 함\n",
    "    enc_tokens = np.memmap(filename='enc_tokens_8000.memmap' , dtype=np.int32, shape=(total, n_seq)) #, , shape=(total, n_seq)\n",
    "    segments = np.memmap(filename='segments_8000.memmap', dtype=np.int32, shape=(total, n_seq)) # , mode='w+'\n",
    "    labels_nsp = np.memmap(filename='labels_nsp_8000.memmap', dtype=np.int32, shape=(total,))\n",
    "    labels_mlm = np.memmap(filename='labels_mlm_8000.memmap', dtype=np.int32, shape=(total, n_seq))\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        for i, line in enumerate(tqdm(f, total=total)):\n",
    "            if total <= i:\n",
    "                print(\"data load early stop\", total, i)\n",
    "                break\n",
    "            data = json.loads(line)\n",
    "            # encoder token\n",
    "            enc_token = [vocab.piece_to_id(p) for p in data[\"tokens\"]]\n",
    "            enc_token += [0] * (n_seq - len(enc_token))\n",
    "            # segment\n",
    "            segment = data[\"segment\"]\n",
    "            segment += [0] * (n_seq - len(segment))\n",
    "            # nsp label\n",
    "            label_nsp = data[\"is_next\"]\n",
    "            # mlm label\n",
    "            mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
    "            mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
    "            label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n",
    "            label_mlm[mask_idx] = mask_label\n",
    "\n",
    "            assert len(enc_token) == len(segment) == len(label_mlm) == n_seq\n",
    "\n",
    "            enc_tokens[i] = enc_token\n",
    "            segments[i] = segment\n",
    "            labels_nsp[i] = label_nsp\n",
    "            labels_mlm[i] = label_mlm\n",
    "\n",
    "    return (enc_tokens, segments), (labels_nsp, labels_mlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b70b688defd4a0ca756f7786034bc5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peunj\\AppData\\Local\\Temp\\ipykernel_23612\\349315509.py:42: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
      "C:\\Users\\peunj\\AppData\\Local\\Temp\\ipykernel_23612\\349315509.py:43: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
      "C:\\Users\\peunj\\AppData\\Local\\Temp\\ipykernel_23612\\349315509.py:44: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data load early stop 128000 128000\n"
     ]
    }
   ],
   "source": [
    "# 128000건만 메모리에 로딩\n",
    "pre_train_inputs, pre_train_labels = load_pre_train_data(vocab, pretrain_json_path, 128, count=128000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(memmap([   5, 3629,  203,    6,    6, 1114, 3724,  788,  243,   49, 3632,\n",
       "          796,  663, 1648, 3682, 3682, 3625,    6, 3009, 3625, 3616,   16,\n",
       "         3599,   18, 3686,  207, 3714, 3602,    6,    6,    6,  630, 3714,\n",
       "         3568, 3835,  429, 3740, 3628, 3626, 1370,   10, 1607, 3599, 1756,\n",
       "         3630,   41, 3644,  830, 3624, 1135,   52, 3599,   13,   81,   87,\n",
       "         1502, 2247,   25, 3779, 3873, 3667, 3631, 3813, 3873, 4196, 3636,\n",
       "         3779, 3601,  249, 3725, 1233,   33,   52, 3599,  479, 3652, 3625,\n",
       "          243, 2780,   14, 1510,  168, 3877,  414,  166, 1697, 4290, 3873,\n",
       "         3703, 3683,    6,   21, 5007,  399,    6,    6,    6,    6,    6,\n",
       "          307,  587,  930,  104, 4313, 4290,  613, 3638, 3718,   99, 3878,\n",
       "         3656,  256, 2543,  309,  337, 3735,  181, 3616, 3603,    6,    6,\n",
       "            6,    4,   18, 3686,  207, 3714,    4]),\n",
       " memmap([   5,   41, 2429, 4143,  405,    6,    6, 2975,  173,  351, 3599,\n",
       "          269,  820,  477, 3921,   66, 1358, 3715, 3633,  116, 3600, 2975,\n",
       "          173, 1346, 3604,   13, 4183,  947,   15, 3784, 1117, 3238, 3617,\n",
       "         3706, 3596, 4639, 1365, 3627, 3648, 3643,  990, 2227,    6,    6,\n",
       "            6,    6, 2142, 1148,  919,  112, 3314, 3187, 3596, 4153,  658,\n",
       "          171, 3599,    6,   81, 3604,    6,    6,   68, 3238, 3602,   13,\n",
       "          316,    6,    6,  305, 3620, 1396,    6,    6,   19,  805, 3596,\n",
       "         4904, 3750, 3603, 4065,  116, 3617, 3756, 3596, 4639, 1365, 3627,\n",
       "          990, 3616, 3600,    7, 3614, 3746,    9, 2975,  173, 1346, 3604,\n",
       "          848, 3784, 3833,    8, 3637, 2263,   12, 3614, 3746,  836, 3596,\n",
       "         4904, 3750, 3603, 4065,  116, 3600, 2975,  173,  351, 3599,    4,\n",
       "          848, 3784, 3833,    6,    6,    6,    4]),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 0,\n",
       " 0,\n",
       " memmap([   0,    0,    0,  241, 3602,    0,    0,    0,    0,   49, 3632,\n",
       "          796,    0,    0,    0,    0,    0,  203,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0, 1756, 3630, 3646,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,  593,    0,    0,    0, 1928, 3607,  813,   17, 3599,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,  489,  376,\n",
       "         3599,    0,    0,    0,    0,    0,    0]),\n",
       " memmap([   0,    0,    0,    0,    0, 1185, 3600,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,  574,   10,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,  262, 3651,\n",
       "         3616, 3607,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,   13,    0,    0,   15, 3784,    0,    0,    0,    0,\n",
       "            0, 1425,  173,    0,    0,    0,  149, 3607,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    8, 3637, 3676,    0]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 처음과 마지막 확인\n",
    "pre_train_inputs[0][0], pre_train_inputs[0][-1], pre_train_inputs[1][0], pre_train_inputs[1][-1], pre_train_labels[0][0], pre_train_labels[0][-1], pre_train_labels[1][0], pre_train_labels[1][-1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.BERT 모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def get_pad_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    pad mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: pad mask (pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    mask = tf.cast(tf.math.equal(tokens, i_pad), tf.float32)\n",
    "    mask = tf.expand_dims(mask, axis=1)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_ahead_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    ahead mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: ahead and pad mask (ahead or pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    n_seq = tf.shape(tokens)[1]\n",
    "    ahead_mask = 1 - tf.linalg.band_part(tf.ones((n_seq, n_seq)), -1, 0)\n",
    "    ahead_mask = tf.expand_dims(ahead_mask, axis=0)\n",
    "    pad_mask = get_pad_mask(tokens, i_pad)\n",
    "    mask = tf.maximum(ahead_mask, pad_mask)\n",
    "    return mask\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "@tf.function(experimental_relax_shapes=True)\n",
    "def gelu(x):\n",
    "    \"\"\"\n",
    "    gelu activation 함수\n",
    "    :param x: 입력 값\n",
    "    :return: gelu activation result\n",
    "    \"\"\"\n",
    "    return 0.5*x*(1+tf.tanh(np.sqrt(2/np.pi)*(x+0.044715*tf.pow(x, 3))))\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def kernel_initializer(stddev=0.02):\n",
    "    \"\"\"\n",
    "    parameter initializer 생성\n",
    "    :param stddev: 생성할 랜덤 변수의 표준편차\n",
    "    \"\"\"\n",
    "    return tf.keras.initializers.TruncatedNormal(stddev=stddev)\n",
    "\n",
    "\n",
    "def bias_initializer():\n",
    "    \"\"\"\n",
    "    bias initializer 생성\n",
    "    \"\"\"\n",
    "    return tf.zeros_initializer\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class Config(dict):\n",
    "    \"\"\"\n",
    "    json을 config 형태로 사용하기 위한 Class\n",
    "    :param dict: config dictionary\n",
    "    \"\"\"\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, file):\n",
    "        \"\"\"\n",
    "        file에서 Config를 생성 함\n",
    "        :param file: filename\n",
    "        \"\"\"\n",
    "        with open(file, 'r') as f:\n",
    "            config = json.loads(f.read())\n",
    "            return Config(config)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class SharedEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Weighed Shaed Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"weight_shared_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.n_vocab = config.n_vocab\n",
    "        self.d_model = config.d_model\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        shared weight 생성\n",
    "        :param input_shape: Tensor Shape (not used)\n",
    "        \"\"\"\n",
    "        with tf.name_scope(\"shared_embedding_weight\"):\n",
    "            self.shared_weights = self.add_weight(\n",
    "                \"weights\",\n",
    "                shape=[self.n_vocab, self.d_model],\n",
    "                initializer=kernel_initializer()\n",
    "            )\n",
    "\n",
    "    def call(self, inputs, mode=\"embedding\"):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :param mode: 실행 모드\n",
    "        :return: embedding or linear 실행 결과\n",
    "        \"\"\"\n",
    "        # mode가 embedding일 경우 embedding lookup 실행\n",
    "        if mode == \"embedding\":\n",
    "            return self._embedding(inputs)\n",
    "        # mode가 linear일 경우 linear 실행\n",
    "        elif mode == \"linear\":\n",
    "            return self._linear(inputs)\n",
    "        # mode가 기타일 경우 오류 발생\n",
    "        else:\n",
    "            raise ValueError(f\"mode {mode} is not valid.\")\n",
    "    \n",
    "    def _embedding(self, inputs):\n",
    "        \"\"\"\n",
    "        embedding lookup\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        embed = tf.gather(self.shared_weights, tf.cast(inputs, tf.int32))\n",
    "        return embed\n",
    "\n",
    "    def _linear(self, inputs):  # (bs, n_seq, d_model)\n",
    "        \"\"\"\n",
    "        linear 실행\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        n_batch = tf.shape(inputs)[0]\n",
    "        n_seq = tf.shape(inputs)[1]\n",
    "        inputs = tf.reshape(inputs, [-1, self.d_model])  # (bs * n_seq, d_model)\n",
    "        outputs = tf.matmul(inputs, self.shared_weights, transpose_b=True)\n",
    "        outputs = tf.reshape(outputs, [n_batch, n_seq, self.n_vocab])  # (bs, n_seq, n_vocab)\n",
    "        return outputs\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class PositionEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Position Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"position_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(config.n_seq, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :return embed: position embedding lookup 결과\n",
    "        \"\"\"\n",
    "        position = tf.cast(tf.math.cumsum(tf.ones_like(inputs), axis=1, exclusive=True), tf.int32)\n",
    "        embed = self.embedding(position)\n",
    "        return embed\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class ScaleDotProductAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Scale Dot Product Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, name=\"scale_dot_product_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        attn_score = tf.matmul(Q, K, transpose_b=True)\n",
    "        scale = tf.math.sqrt(tf.cast(tf.shape(K)[-1], tf.float32))\n",
    "        attn_scale = tf.math.divide(attn_score, scale)\n",
    "        attn_scale -= 1.e9 * attn_mask\n",
    "        attn_prob = tf.nn.softmax(attn_scale, axis=-1)\n",
    "        attn_out = tf.matmul(attn_prob, V)\n",
    "        return attn_out\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Multi Head Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"multi_head_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.d_model = config.d_model\n",
    "        self.n_head = config.n_head\n",
    "        self.d_head = config.d_head\n",
    "\n",
    "        # Q, K, V input dense layer\n",
    "        self.W_Q = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_K = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_V = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        # Scale Dot Product Attention class\n",
    "        self.attention = ScaleDotProductAttention(name=\"self_attention\")\n",
    "        # output dense layer\n",
    "        self.W_O = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        # reshape Q, K, V, attn_mask\n",
    "        batch_size = tf.shape(Q)[0]\n",
    "        Q_m = tf.transpose(tf.reshape(self.W_Q(Q), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, Q_len, d_head)\n",
    "        K_m = tf.transpose(tf.reshape(self.W_K(K), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        V_m = tf.transpose(tf.reshape(self.W_V(V), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        attn_mask_m = tf.expand_dims(attn_mask, axis=1)\n",
    "        # Scale Dot Product Attention with multi head Q, K, V, attn_mask\n",
    "        attn_out = self.attention(Q_m, K_m, V_m, attn_mask_m)  # (bs, n_head, Q_len, d_head)\n",
    "        # transpose and liner\n",
    "        attn_out_m = tf.transpose(attn_out, perm=[0, 2, 1, 3])  # (bs, Q_len, n_head, d_head)\n",
    "        attn_out = tf.reshape(attn_out_m, [batch_size, -1, config.n_head * config.d_head])  # (bs, Q_len, d_model)\n",
    "        attn_out = self.W_O(attn_out) # (bs, Q_len, d_model)\n",
    "\n",
    "        return attn_out\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class PositionWiseFeedForward(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Position Wise Feed Forward Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"feed_forward\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.W_1 = tf.keras.layers.Dense(config.d_ff, activation=gelu, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_2 = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: inputs\n",
    "        :return ff_val: feed forward 실행 결과\n",
    "        \"\"\"\n",
    "        ff_val = self.W_2(self.W_1(inputs))\n",
    "        return ff_val\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Encoder Layer Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"encoder_layer\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.self_attention = MultiHeadAttention(config)\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.ffn = PositionWiseFeedForward(config)\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    " \n",
    "    def call(self, enc_embed, self_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param enc_embed: enc_embed 또는 이전 EncoderLayer의 출력\n",
    "        :param self_mask: enc_tokens의 pad mask\n",
    "        :return enc_out: EncoderLayer 실행 결과\n",
    "        \"\"\"\n",
    "        self_attn_val = self.self_attention(enc_embed, enc_embed, enc_embed, self_mask)\n",
    "        norm1_val = self.norm1(enc_embed + self.dropout(self_attn_val))\n",
    "\n",
    "        ffn_val = self.ffn(norm1_val)\n",
    "        enc_out = self.norm2(norm1_val + self.dropout(ffn_val))\n",
    "\n",
    "        return enc_out\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class BERT(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    BERT Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"bert\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.i_pad = config.i_pad\n",
    "        self.embedding = SharedEmbedding(config)\n",
    "        self.position = PositionEmbedding(config)\n",
    "        self.segment = tf.keras.layers.Embedding(2, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "        self.norm = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "        \n",
    "        self.encoder_layers = [EncoderLayer(config, name=f\"encoder_layer_{i}\") for i in range(config.n_layer)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: (enc_tokens, segments)\n",
    "        :return logits: dec_tokens에 대한 다음 토큰 예측 결과 logits\n",
    "        \"\"\"\n",
    "        enc_tokens, segments = inputs\n",
    "\n",
    "        enc_self_mask = tf.keras.layers.Lambda(get_pad_mask, output_shape=(1, None), name='enc_self_mask')(enc_tokens, self.i_pad)\n",
    "\n",
    "        enc_embed = self.get_embedding(enc_tokens, segments)\n",
    "\n",
    "        enc_out = self.dropout(enc_embed)\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            enc_out = encoder_layer(enc_out, enc_self_mask)\n",
    "\n",
    "        logits_cls = enc_out[:,0]\n",
    "        logits_lm = self.embedding(enc_out, mode=\"linear\")\n",
    "        return logits_cls, logits_lm\n",
    "    \n",
    "    def get_embedding(self, tokens, segments):\n",
    "        \"\"\"\n",
    "        token embedding, position embedding lookup\n",
    "        :param tokens: 입력 tokens\n",
    "        :param segments: 입력 segments\n",
    "        :return embed: embedding 결과\n",
    "        \"\"\"\n",
    "        embed = self.embedding(tokens) + self.position(tokens) + self.segment(segments)\n",
    "        embed = self.norm(embed)\n",
    "        return embed\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Encoder Layer class 정의\n",
    "class PooledOutput(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, n_output, name=\"pooled_output\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.dense1 = tf.keras.layers.Dense(config.d_model, activation=tf.nn.tanh, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.dense2 = tf.keras.layers.Dense(n_output, use_bias=False, activation=tf.nn.softmax, name=\"nsp\", kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    " \n",
    "    def call(self, inputs):\n",
    "        outputs = self.dense1(inputs)\n",
    "        outputs = self.dense2(outputs)\n",
    "        return outputs\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def build_model_pre_train(config):\n",
    "    enc_tokens = tf.keras.layers.Input((None,), name=\"enc_tokens\")\n",
    "    segments = tf.keras.layers.Input((None,), name=\"segments\")\n",
    "\n",
    "    bert = BERT(config)\n",
    "    logits_cls, logits_lm = bert((enc_tokens, segments))\n",
    "\n",
    "    logits_cls = PooledOutput(config, 2, name=\"pooled_nsp\")(logits_cls)\n",
    "    outputs_nsp = tf.keras.layers.Softmax(name=\"nsp\")(logits_cls)\n",
    "\n",
    "    outputs_mlm = tf.keras.layers.Softmax(name=\"mlm\")(logits_lm)\n",
    "\n",
    "    model = tf.keras.Model(inputs=(enc_tokens, segments), outputs=(outputs_nsp, outputs_mlm))\n",
    "    return model\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d_model': 256,\n",
       " 'n_head': 4,\n",
       " 'd_head': 64,\n",
       " 'dropout': 0.1,\n",
       " 'd_ff': 1024,\n",
       " 'layernorm_epsilon': 0.001,\n",
       " 'n_layer': 3,\n",
       " 'n_seq': 256,\n",
       " 'n_vocab': 8007,\n",
       " 'i_pad': 0}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config({\"d_model\": 256, \"n_head\": 4, \"d_head\": 64, \"dropout\": 0.1, \"d_ff\": 1024, \"layernorm_epsilon\": 0.001, \"n_layer\": 3, \"n_seq\": 256, \"n_vocab\": 0, \"i_pad\": 0})\n",
    "config.n_vocab = len(vocab)\n",
    "config.i_pad = vocab.pad_id()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2/2 [==============================] - 10s 51ms/step - loss: 9.7069 - nsp_loss: 0.7140 - mlm_loss: 8.9929 - nsp_acc: 0.4000 - mlm_acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 8.5415 - nsp_loss: 0.6240 - mlm_loss: 7.9176 - nsp_acc: 0.8000 - mlm_acc: 0.0000e+00\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 7.6445 - nsp_loss: 0.5656 - mlm_loss: 7.0788 - nsp_acc: 1.0000 - mlm_acc: 0.2300\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 6.7269 - nsp_loss: 0.4385 - mlm_loss: 6.2884 - nsp_acc: 1.0000 - mlm_acc: 0.4900\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 5.9079 - nsp_loss: 0.3484 - mlm_loss: 5.5595 - nsp_acc: 1.0000 - mlm_acc: 0.8000\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 5.2098 - nsp_loss: 0.3241 - mlm_loss: 4.8857 - nsp_acc: 1.0000 - mlm_acc: 0.9200\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 4.5287 - nsp_loss: 0.3188 - mlm_loss: 4.2099 - nsp_acc: 1.0000 - mlm_acc: 0.9700\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 3.8876 - nsp_loss: 0.3178 - mlm_loss: 3.5699 - nsp_acc: 1.0000 - mlm_acc: 0.9700\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 3.2951 - nsp_loss: 0.3177 - mlm_loss: 2.9774 - nsp_acc: 1.0000 - mlm_acc: 0.9700\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 2.7713 - nsp_loss: 0.3190 - mlm_loss: 2.4523 - nsp_acc: 1.0000 - mlm_acc: 0.9800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20006027be0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_seq = 10\n",
    "\n",
    "# make test inputs\n",
    "enc_tokens = np.random.randint(0, len(vocab), (10, n_seq))\n",
    "segments = np.random.randint(0, 2, (10, n_seq))\n",
    "labels_nsp = np.random.randint(0, 2, (10,))\n",
    "labels_mlm = np.random.randint(0, len(vocab), (10, n_seq))\n",
    "\n",
    "test_model = build_model_pre_train(config)\n",
    "test_model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy, optimizer=tf.keras.optimizers.Adam(), metrics=[\"acc\"])\n",
    "\n",
    "# test model fit\n",
    "test_model.fit((enc_tokens, segments), (labels_nsp, labels_mlm), epochs=10, batch_size=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.pretrain 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def lm_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    loss 계산 함수\n",
    "    :param y_true: 정답 (bs, n_seq)\n",
    "    :param y_pred: 예측 값 (bs, n_seq, n_vocab)\n",
    "    \"\"\"\n",
    "    # loss 계산\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)(y_true, y_pred)\n",
    "    # pad(0) 인 부분 mask\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    return loss * 20  # mlm을 더 잘 학습하도록 20배 증가 시킴\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def lm_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    acc 계산 함수\n",
    "    :param y_true: 정답 (bs, n_seq)\n",
    "    :param y_pred: 예측 값 (bs, n_seq, n_vocab)\n",
    "    \"\"\"\n",
    "    # 정답 여부 확인\n",
    "    y_pred_class = tf.cast(K.argmax(y_pred, axis=-1), tf.float32)\n",
    "    matches = tf.cast(K.equal(y_true, y_pred_class), tf.float32)\n",
    "    # pad(0) 인 부분 mask\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=matches.dtype)\n",
    "    matches *= mask\n",
    "    # 정확도 계산\n",
    "    accuracy = K.sum(matches) / K.maximum(K.sum(mask), 1)\n",
    "    return accuracy\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class CosineSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"\n",
    "    CosineSchedule Class\n",
    "    \"\"\"\n",
    "    def __init__(self, train_steps=4000, warmup_steps=2000, max_lr=2.5e-4):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param train_steps: 학습 step 총 합\n",
    "        :param warmup_steps: warmup steps\n",
    "        :param max_lr: 최대 learning rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        assert 0 < warmup_steps < train_steps\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.train_steps = train_steps\n",
    "        self.max_lr = max_lr\n",
    "\n",
    "    def __call__(self, step_num):\n",
    "        \"\"\"\n",
    "        learning rate 계산\n",
    "        :param step_num: 현재 step number\n",
    "        :retrun: 계산된 learning rate\n",
    "        \"\"\"\n",
    "        state = tf.cast(step_num <= self.warmup_steps, tf.float32)\n",
    "        lr1 = tf.cast(step_num, tf.float32) / self.warmup_steps\n",
    "        progress = tf.cast(step_num - self.warmup_steps, tf.float32) / max(1, self.train_steps - self.warmup_steps)\n",
    "        lr2 = 0.5 * (1.0 + tf.math.cos(math.pi * progress))\n",
    "        return (state * lr1 + (1 - state) * lr2) * self.max_lr\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compute lr \n",
    "test_schedule = CosineSchedule(train_steps=4000, warmup_steps=500)\n",
    "lrs = []\n",
    "for step_num in range(4000):\n",
    "    lrs.append(test_schedule(float(step_num)).numpy())\n",
    "\n",
    "# draw\n",
    "plt.plot(lrs, 'r-', label='learning_rate')\n",
    "plt.xlabel('Step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " enc_tokens (InputLayer)        [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " segments (InputLayer)          [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " bert (BERT)                    ((None, 256),        4485632     ['enc_tokens[0][0]',             \n",
      "                                 (None, None, 8007)               'segments[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pooled_nsp (PooledOutput)      (None, 2)            66304       ['bert[0][0]']                   \n",
      "                                                                                                  \n",
      " nsp (Softmax)                  (None, 2)            0           ['pooled_nsp[0][0]']             \n",
      "                                                                                                  \n",
      " mlm (Softmax)                  (None, None, 8007)   0           ['bert[0][1]']                   \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,551,936\n",
      "Trainable params: 4,551,936\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 생성\n",
    "pre_train_model = build_model_pre_train(config)\n",
    "pre_train_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_steps: 20000\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "# optimizer\n",
    "train_steps = math.ceil(len(pre_train_inputs[0]) / batch_size) * epochs\n",
    "print(\"train_steps:\", train_steps)\n",
    "learning_rate = CosineSchedule(train_steps=train_steps, warmup_steps=max(100, train_steps // 10))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "# compile\n",
    "pre_train_model.compile(loss=(tf.keras.losses.sparse_categorical_crossentropy, lm_loss), optimizer=optimizer, metrics={\"nsp\": \"acc\", \"mlm\": lm_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - ETA: 0s - loss: 19.5877 - nsp_loss: 0.6503 - mlm_loss: 18.9374 - nsp_acc: 0.5901 - mlm_lm_acc: 0.1103\n",
      "Epoch 1: mlm_lm_acc improved from -inf to 0.11035, saving model to ./bert_pretrain/models\\bert_pre_train.hdf5\n",
      "2000/2000 [==============================] - 236s 117ms/step - loss: 19.5877 - nsp_loss: 0.6503 - mlm_loss: 18.9374 - nsp_acc: 0.5901 - mlm_lm_acc: 0.1103\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 17.4683 - nsp_loss: 0.6242 - mlm_loss: 16.8440 - nsp_acc: 0.6182 - mlm_lm_acc: 0.1311\n",
      "Epoch 2: mlm_lm_acc improved from 0.11035 to 0.13109, saving model to ./bert_pretrain/models\\bert_pre_train.hdf5\n",
      "2000/2000 [==============================] - 332s 166ms/step - loss: 17.4683 - nsp_loss: 0.6242 - mlm_loss: 16.8440 - nsp_acc: 0.6182 - mlm_lm_acc: 0.1311\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 15.8595 - nsp_loss: 0.6185 - mlm_loss: 15.2411 - nsp_acc: 0.6238 - mlm_lm_acc: 0.1548\n",
      "Epoch 3: mlm_lm_acc improved from 0.13109 to 0.15480, saving model to ./bert_pretrain/models\\bert_pre_train.hdf5\n",
      "2000/2000 [==============================] - 334s 167ms/step - loss: 15.8595 - nsp_loss: 0.6185 - mlm_loss: 15.2411 - nsp_acc: 0.6238 - mlm_lm_acc: 0.1548\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 14.1874 - nsp_loss: 0.6141 - mlm_loss: 13.5733 - nsp_acc: 0.6275 - mlm_lm_acc: 0.1884\n",
      "Epoch 4: mlm_lm_acc improved from 0.15480 to 0.18840, saving model to ./bert_pretrain/models\\bert_pre_train.hdf5\n",
      "2000/2000 [==============================] - 315s 158ms/step - loss: 14.1874 - nsp_loss: 0.6141 - mlm_loss: 13.5733 - nsp_acc: 0.6275 - mlm_lm_acc: 0.1884\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 13.4775 - nsp_loss: 0.6088 - mlm_loss: 12.8688 - nsp_acc: 0.6357 - mlm_lm_acc: 0.2081\n",
      "Epoch 5: mlm_lm_acc improved from 0.18840 to 0.20809, saving model to ./bert_pretrain/models\\bert_pre_train.hdf5\n",
      "2000/2000 [==============================] - 297s 148ms/step - loss: 13.4775 - nsp_loss: 0.6088 - mlm_loss: 12.8688 - nsp_acc: 0.6357 - mlm_lm_acc: 0.2081\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 13.0369 - nsp_loss: 0.6040 - mlm_loss: 12.4329 - nsp_acc: 0.6464 - mlm_lm_acc: 0.2214\n",
      "Epoch 6: mlm_lm_acc improved from 0.20809 to 0.22145, saving model to ./bert_pretrain/models\\bert_pre_train.hdf5\n",
      "2000/2000 [==============================] - 359s 179ms/step - loss: 13.0369 - nsp_loss: 0.6040 - mlm_loss: 12.4329 - nsp_acc: 0.6464 - mlm_lm_acc: 0.2214\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 12.7363 - nsp_loss: 0.5995 - mlm_loss: 12.1367 - nsp_acc: 0.6560 - mlm_lm_acc: 0.2311\n",
      "Epoch 7: mlm_lm_acc improved from 0.22145 to 0.23109, saving model to ./bert_pretrain/models\\bert_pre_train.hdf5\n",
      "2000/2000 [==============================] - 379s 189ms/step - loss: 12.7363 - nsp_loss: 0.5995 - mlm_loss: 12.1367 - nsp_acc: 0.6560 - mlm_lm_acc: 0.2311\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 12.5340 - nsp_loss: 0.5944 - mlm_loss: 11.9396 - nsp_acc: 0.6649 - mlm_lm_acc: 0.2378\n",
      "Epoch 8: mlm_lm_acc improved from 0.23109 to 0.23780, saving model to ./bert_pretrain/models\\bert_pre_train.hdf5\n",
      "2000/2000 [==============================] - 339s 169ms/step - loss: 12.5340 - nsp_loss: 0.5944 - mlm_loss: 11.9396 - nsp_acc: 0.6649 - mlm_lm_acc: 0.2378\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 12.4070 - nsp_loss: 0.5901 - mlm_loss: 11.8169 - nsp_acc: 0.6732 - mlm_lm_acc: 0.2421\n",
      "Epoch 9: mlm_lm_acc improved from 0.23780 to 0.24213, saving model to ./bert_pretrain/models\\bert_pre_train.hdf5\n",
      "2000/2000 [==============================] - 316s 158ms/step - loss: 12.4070 - nsp_loss: 0.5901 - mlm_loss: 11.8169 - nsp_acc: 0.6732 - mlm_lm_acc: 0.2421\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 12.3476 - nsp_loss: 0.5881 - mlm_loss: 11.7594 - nsp_acc: 0.6763 - mlm_lm_acc: 0.2439\n",
      "Epoch 10: mlm_lm_acc improved from 0.24213 to 0.24393, saving model to ./bert_pretrain/models\\bert_pre_train.hdf5\n",
      "2000/2000 [==============================] - 313s 157ms/step - loss: 12.3476 - nsp_loss: 0.5881 - mlm_loss: 11.7594 - nsp_acc: 0.6763 - mlm_lm_acc: 0.2439\n"
     ]
    }
   ],
   "source": [
    "# save weights callback  # 20분\n",
    "save_weights = tf.keras.callbacks.ModelCheckpoint(f\"{model_dir}/bert_pre_train.hdf5\", monitor=\"mlm_lm_acc\", verbose=1, save_best_only=True, mode=\"max\", save_freq=\"epoch\", save_weights_only=True)\n",
    "# train\n",
    "with tf.device('/GPU:0'):\n",
    "    history = pre_train_model.fit(pre_train_inputs, pre_train_labels, epochs=epochs, batch_size=batch_size, callbacks=[save_weights])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.프로젝트 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA90AAAFzCAYAAAAjR2oxAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABobklEQVR4nO3deXhTVf7H8U+atmkoXcCylLYICFKgyiJY2Sw4IOIoso3ggMoPRgdUFlGURUAELTPDiDJuVNwrKCMygwsuiBVZ1CqCIKsKtpR9aUOXpFt+f2SaNrSFFpKmy/v1PHma3HvuzTdBOXx6zz3HYLfb7QIAAAAAAG7n4+0CAAAAAACorQjdAAAAAAB4CKEbAAAAAAAPIXQDAAAAAOAhhG4AAAAAADyE0A0AAAAAgIcQugEAAAAA8BBCNwAAAAAAHkLoBgAAAADAQwjdAADgoiQkJCgmJkYdOnTQwIEDlZaWVma7Dz/8UDExMS6Pdu3aqWnTplVcMQAAVc9gt9vt3i7iUhQWFurw4cMKCgqSwWDwdjkAAMhut+vs2bNq1qyZfHxq5++3165dq8cee0xffPGFQkNDlZiYqGeffVbJyckVOv7999/X22+/rVWrVl2wLX09AKC6qUxfX+ND96FDhxQVFeXtMgAAKCU1NVWRkZHeLsMjhgwZonvuuUc333yzc9t1112nF154QV26dLng8TfeeKMeeughDRgw4IJt6esBANVVRfp63yqqxWOCgoIkOT5scHCwl6sBAECyWCyKiopy9lG10fr165WYmOiyrU+fPlq3bt0FQ/evv/6qX375Rf379y9zv81mk81mc74uuj5AXw8AqC4q09fX+NBdNMwsODiYjhgAUK3U1qHQmZmZMhqNCgwMdNkeFRWlnTt3XvD4pUuXauzYseUOx4uPj9e8efNKbaevBwBUNxXp62vnjWYAAMBj0tPTZTabS203m83Kzs4+77G5ublKTEzU2LFjy20zY8YMZWRkOB+pqamXXDMAAN5S4690AwCAqmUymWS1Wkttt1qtZYbxkt577z1de+21atas2XnPbzKZLrlOAACqA650AwCASgkLC1NOTo6ysrJctldkMpmXXnpJ99xzjyfLAwCgWuFKNwBUE3a7Xfn5+SooKPB2KbgAo9EoX1/fWnvP9oUYDAbFxsZqw4YNGjhwoHN7UlKSFixYUO5xu3bt0u+//+5yDAAAtR2hGwCqgdzcXB05cuSC98Oi+qhXr57Cw8Pl7+/v7VK8YtKkSZo9e7Z69OihkJAQLV++XJmZmerbt2+5x7z00kvnnUANAIDaiNANAF5WWFioAwcOyGg0qlmzZvL396+zV1BrArvdrtzcXJ04cUIHDhxQmzZt6mSIHDJkiFJSUhQbGyuDwaCIiAitWbNGPj4+ysvL09ChQ5WQkKDw8HBJjmXA/v3vf+u7777zcuUAAFQtg71o8csaymKxKCQkRBkZGSwjAqBGslqtOnDggC6//HLVq1fP2+WggrKzs/X777+rZcuWCggIcNlH3+RefJ8AgOqmMn1T3fvVPABUU3XxamlNxp8XAACoCP7FAAAAAACAh3BP97kKCiQfH4n7KQEAAACgxsgvzFdOXo6y87KVk/+/n3k5Ls8NBoMGtR1UpXURus81e7b088/Sa69JDRt6uxoAwDmefPJJ5efna+7cud4uBQAAnIfdbpetwOYShC/qeX7p8FzW8/zC/AvWFFYvTCemnaiCT1+M0F1SWpq0eLFktUqdO0vvvitdd523qwIAlJCXl6f8/At3qgAAwD1y8nJ0NPOojmYe1ZHMI46fZ484X5/OOV1uYLbLO/N2m33NMvuZZfY1q55fPZn9HD8bmqv+wiqhu6SICGnTJun226Vff5V695YWLpSmTmW4OYAqZbdL3liyu149/roDAKAusNvtOp1zulSQdj4vsS3DlnHJ7+dj8HGE33NCcFE4dnnuW69UYK7Qcf97HuAbUK2WXyV0n6tLF2nrVumee6SVK6WHH5aSkqTXX5cuu8zb1QGoI7Kzpfr1q/59MzOlwMCKt4+Li9OYMWO0ZMkSWa1WmUwmPfvss4qLi9PWrVs1YcIEZWVlydfXVwsXLtRNN92ke+65R3FxcXr55Zd1/Phx5efna/78+Ro5cuRF1fzLL79oypQp2rFjhwwGg3r27KlnnnlGjRo1kiQlJiYqPj5ePj4+8vPz0yeffKLGjRtr0aJFWrZsmfz9/WU2m/XNN99Uqw4aAICLkVuQW3xV+uw5Afqcq9R5hXkVPq/JaFLT+k0VHhTu+Fm/+GdYvbALBmI/H786288SussSHCy9847Up480ZYr04YeOq94//ST58pUBQBGDwaBnnnlGa9euVbNmzbRx40bdfvvtOnDggKZMmaLnnntO3bp1kyQVFhZKcgwPf/zxx/Xf//5XHTp00P79+xUXF6fWrVura9eulXp/q9Wqfv366fHHH9eHH34ou92uv/3tbxo8eLA2bdokq9Wq2bNna9u2bQoJCVFhYaF8fHz022+/afny5dq+fbtMJpMKCwvr7D8EAADVn91uV4Yto9wgfe5Q78poaG5YKkSXFa5DA0LpKy8SCbI8BoM0YYLjnu7bb5emTydwA6gy9eo5rjp7430ra9KkSWrWrJkkqVevXgoODtbevXtlMBicQVtyXdd6zJgx6tChgySpTZs2uv/++/XGG29UOnQvX75cHTt21JgxYyQ5fgkwffp0vfPOO/rqq6907bXXutRx7tradru9zO0AAFSFojB9+OxhHT57WGmWNMfPs46fJcO1Nd9a4fP6+viWCtKlrlAHhatJYBOZfE0e/ISQCN0X1rmz4wq32Vy8bds2KSqK4eYAPMZgqNwwb29q3ry5y+uwsDCdOXNGzz77rMaNG6du3bpp1qxZioqKcrbp3LmzyzFXX321tmzZUun33rFjh3r16lVqe8+ePbV9+3bFxcXpqaeeUs+ePXXXXXdp4sSJCgwMVKtWrXTXXXepa9eueuCBBzRu3Dj5+flV+v0BAChPTl6OjmQeKTNMl3yenVfxSVxCTCHlDvEuub2huaF8DPxCubogdFdEycB94oR0yy2OtbzfeUfq0cN7dQFANVDWUDO73a5OnTopOTlZiYmJ6tatm1asWKG+fftKknJzc13aZ2dny1zy79pLeO+i9zcajZKkkSNH6o9//KPi4+PVoUMHffvtt2rSpImmTJmiUaNGacaMGVq6dKk2b958UTUAAOqW/MJ8Hc867gzSJQN0yVBdmWHeoQGhahbUTBFBEc6f4UHhahbUzBmqm9Rvonp+FzEkDV5H6K6s06cdl5/27ZOuv1566inHZGsMTQSAUnx8fHTXXXfJaDTq6aefdobuH3/8UUOGDHG2+/7779W+fftKn79jx45atWqVpk2b5rJ98+bNGjVqlPN1UFCQnnrqKZ04cUKJiYl66KGHJEmNGjXSsmXL1L9/f3388ccaNmzYxXxMAEAtYLfbdcZ65rxXpYuGfBfaCy98QkkBvgGlwnSzoGaO58HFzwnTtRuhu7LatpW+/17661+lFSukRx91zG7+5ptSWJi3qwOAauPEiRNq1KiR7Ha7tm3b5rzvW5Jee+01jRgxQh06dNCPP/6ot956S8nJyZV+j5EjR2r+/Pl69dVXNXbsWBUWFuqpp55SaGioevbsKZvNptzcXAUFBSknJ0d79uzRDTfcoMzMTPn6+iogIEBnzpzR77//7lIfAKB2seXblHY2TakZqWWG6aKgbSuwVeh8PgYfhdcPLw7QZYTpiKAIJh+DJEL3xQkKkt5+W+rbV5o0SVq7VurUyTHcvIx7CwGgtvL395e/v7/LNpPJJH9/f/Xr108Wi0V+fn6KiYnRyy+/7Gwza9YsTZgwQUeOHJHBYNCKFStc7vm+0HsWTXxmMpm0efNmTZgwQU888YQkacCAAfroo48kSQcPHtQf/vAH57DxoUOHasSIEfrqq680atQo1a9fXwaDQePHj1f37t0v+fsAAFS9okB9yHJIqRmpjp8W15/Hs45X+HwNzQ1LXZkueVU6IihCjQMby+hj9OCnQm1isBdN3VpDWSwWhYSEKCMjQ8HBwVVfwE8/SX/6k2O4+ciRjqvfAFAJVqtVBw4cUMuWLRUQEODtcjxuzJgxGjNmjPr06ePtUi7J+f7cvN431TJ8n0DdZcu36fDZw8UhuoxQXdFAHeAboIigCEUGRzpCdP3SV6bDg8IV4Fv7+2Jcusr0TVzpvlRXXy398IP0+OPS7NnergYAqj2j0VjuTOHbt293uRe7JIPBoNWrV6t169aeLA8AUEXKC9SHzhY/P5Z1rELnCvANUGRwpCKDIxUVHOX6M8Tx8zLzZQz1hlcQut2hfn1p0aLi13a7Y9j57bdLvXt7ry4AqIZeeeWVcvd17NhRO3furMJqAACekFuQqzRLmutV6YzUiwrUJqPJGZwJ1KiJCN2e8Oab0nPPSS+8IM2fL02fzuzmAAAAqBXsdrtOZp/UwfSD+j3jdx1MP6iUjBSXcH0pgfrcUE2gRk1H6PaEYcOkL76Q3npLmjVL+uorx/PGjb1dGQAAAHBedrtdx7OO62D6QZdgXfJ1dl72Bc9jMppcrkaXdZWaQI26gNDtCfXrS2+84Zjd/P77pc8+c8xuvmKFFBfn7eoAAABQhxXaC3Us81ipIF3yuTXfet5zGGRQeFC4WoS2UIvQFmoe3FxRIVEuV6vD6oURqAERuj3HYJD+7/+ka691zG6+e7d0ww3SkiWOIA4AAAB4QEFhgY5kHtHv6aWvUBf9zC3IPe85DDIoMjhSl4de7gjWIY5wXfQ6KjhKJl9TFX0ioGYjdHtahw5ScrL0wAOOIeYdO3q7IgAAANRgBYUFSjub5gjQJYN1huN1SkaK8grzznsOH4OPooKjioP0/0J10evI4Ej5G/2r6BMBtdtFh+7XX39dEyZM0N69e9W8eXNJ0ty5c7Vq1SqXdhaLRT169NA777xT5nlWrFihCRMmKDIy0rktICBA3377rYzGWrLgfGCg9Npr0kMPSTExxdtPnpTCwrxXFwAAAKoVu92us7lndTTzqA6fPVwcrDOKr1gfshxSfmH+ec/j6+PrDNUtQlvo8pDLnc9bhLZQRHCEfH24/gZUhYv6P23WrFnaunWrQkJClJtbPDRl3rx5mjdvnkvbSZMmqU2bNuWey2az6ZZbblFiYuLFlFKzlAzcu3ZJ110nPfywY7K12vILBgAAAJSSV5Cn41nHdTTzqI5kHtHRzKPOx7mvKzJJmZ+Pn5qHNHcJ0iWDdbOgZjL68O9LoDqodOguLCxURESEnnjiCV1xxRXnbZudna333ntPu3btuugCa61Vq6SzZ6W5cx2zm7/9ttS0qberAgCPSUtLU+/evfXbb7955PxPPvmk8vPzNXfuXI+cHwDOZbfblWHLcATns+UH6SOZR3Qy+2Slzh3kH6TwoHCXIF3yedP6TQnVQA1R6dDt4+Oj++67r0Jt33nnHfXv31+hoaGVfZty2Ww22Ww252uLxeK2c1ep2bOlFi2k8eOl9esds5svX+6YbA0AimRllb/PaJQCAirW1sdHMpvP3zYwsPL1VUJeXp7L6ChPnD8///zDLQGgInILcl0CtEuoznJ9bSuwXfiE/+Pr46smgU3UtH5T5yO8frjr66BwNQlsokB/z/6dDKDqePRGjpdeeklPP/20W88ZHx9fagh7jXXnnVLXrtLtt0s7d0r9+jnC+Jw5DDcH4FC/fvn7br5Z+uij4teNG0vZ5QxJjIuTkpKKX7do4ZhXoiS7/WKrBIBqq9BeqOy8bGXlZikrL0uZuZk6kXXivMO8T+ecrtR7hAaElh+iS7y+rN5l8jH4eOiTAqiuPBa6f/zxR2VlZalXr17nbWcwGLRhwwb16tVLp06dUuvWrTVz5kx17969zPYzZszQ1KlTna8tFouioqLcWnuVatdO+vZbafJkadky6YknpMhI6Z57vF0ZAFxQnz599H//9396+umnZbPZ1K5dO73++uuKj4/X6tWrZTQa9fDDD2vs2LFuPbYifvnlF02ZMkU7duyQwWBQz5499cwzz6hRo0aSpMTERMXHx8vHx0d+fn765JNP1LhxYy1atEjLli2Tv7+/zGazvvnmG9aZBTwsryBPWXlZzmBc4Z95WS6BuqyfOfk5F1WTn4+fy9XnpoGuV6NLBusA34ALnxBAneWx0P3iiy/qngoEx+HDh2vIkCEKDg6W3W7X2rVrddttt2nz5s1q3bp1qfYmk0kmUy1bE7BePenllx1XolascKzvDQCSlJlZ/r5zR8QcP15+W59zrqwcPHjRJZ3rhRdeUFJSkho0aKAnn3xS/fr1080336y9e/cqKytLvXv3Vt++fcsMrhU9tmXLlpWqyWq1ql+/fnr88cf14Ycfym63629/+5sGDx6sTZs2yWq1avbs2dq2bZtCQkJUWFgoHx8f/fbbb1q+fLm2b98uk8mkwsJCAjdwjkJ7oTKsGUq3puuM9YzO5JzRGesZnbWdLTcYlxeKiwLzhZa3cpd6fvUU6BeoRoGNXK5AlzXEu0FAA/7/B+AWHgndZ8+e1fvvv6/4+PgLtg0scQ+hwWDQzTffrEGDBunjjz/WpEmTPFFe9TV6tDRqlFT0F7zNJr3yivTXvzLcHKirKnOftafaXsDEiRPVoEEDSdLQoUP1zDPP6LHHHvvf2wSqX79++u677xQbG3vRx1Y2dC9fvlwdO3bUmDFjJDn6l+nTp+udd97RV199pWuvvVYGg0GFhYWSHPOVlGT/31D7c7cDtUVeQZ7OWM84gvP/QvOZnDOlgnRZrzOsGbLLM7ej+Pr4KtAvUIH+gaV+FgXm8vaf72c9v3oy+5kZ2g3AKzwSut966y0NGDBAl1122UUdn5+fL1/fOrpuYMnfqD7yiLRkibRypWOStWbNvFcXAJSjaYmVF8xms9q0aSM/Pz/ntnr16iknp+zhnZdy7Pns2LGjzNubevbsqe3btysuLk5PPfWUevbsqbvuuksTJ05UYGCgWrVqpbvuuktdu3bVAw88oHHjxrnUA1QXdrtdOfk55Qfl84TmMzlnlJV3nokXK6ieXz01CGig0IBQNTA3ULAp2DXwlhN+zxeQ/Y3+bvh2AKB68UiyXbp0qZYsWVKhtikpKWratKn8/f1lt9v1/vvv69NPP63QVfJa77rrpFdfdSwp1qmTlJgo3Xijt6sCgPPy97/4fzRfyrEllTck1G63y/i/kUMjR47UH//4R8XHx6tDhw769ttv1aRJE02ZMkWjRo3SjBkztHTpUm3evFnmkjO/A1Ug3ZquD/Z+oG/TvnUJzSVDdm7Bpa8GEGIKcYbmBgEN1MDcQKGmc14HhDqfF4Xs0IBQmXxr2e1+AOAhlxS6/f39S10B+O6771RQUKC4uLgyj1m0aJGaNGmiO++8U5L0+eefa+HChfL395fBYFD79u21fv16hYeHX0pptcMdd0jXXOOY3Xz7dummm6SZM6XHH5fq6kgAAKiAjh07atWqVZo2bZrL9s2bN2vUqFHO10FBQXrqqad04sQJJSYm6qGHHpIkNWrUSMuWLVP//v318ccfa9iwYVVaP+qmk9kn9d89/9Wq3au07rd1FbrP2WgwuoRmlwB97utzAnSIKYR1ngGgClxSctu3b1+pbddee6127txZ7jEPP/ywy+tx48Zp3Lhxl1JG7XblldKWLdKDD0pLl0pPPil9/bVjwjWGmwNAmUaOHKn58+fr1Vdf1dixY1VYWKinnnpKoaGh6tmzp2w2m3JzcxUUFKScnBzt2bNHN9xwgzIzM+Xr66uAgACdOXNGv//+u5rxdy086MjZI1q9Z7VW7V6lrw5+pQJ7gXNf+0btdXPrm52TepV11bm+f30m+wKAao7LpTWB2Sy99JLUp49jKbFt2ySr1dtVAUCpFSX8/PxKDREv2ubn5+fStjLHVoS/v79z4jOTyaTNmzdrwoQJeuKJJyRJAwYM0Ef/W9f84MGD+sMf/uAcNj506FCNGDFCX331lUaNGqX69R1BZvz48eUuYQlcrJSMFL2/+32t2r1Km1I2uUxK1rlpZw1rN0zD2g9TdFi0F6sEALiLwV40RWsNZbFYFBISooyMDAUHB3u7HM/bv1/67TdpwADHa7tdOnxYiojwbl0ALprVatWBAwfUsmVLBQSw1mtNcb4/tzrXN3lYbfg+fzn9i1btWqVVu1cp+XCyy77YiFhn0G7VoJWXKgQAVEZl+iaudNc0bdo4HkU+/FAaPlyaPFl67DGphv5jBADOZ/v27S73YpdkMBi0evVqtW7duoqrAs5v14ldzqC9/dh253aDDOrVvJeGtx+uIdFDFBUS5cUqAQCeRuiu6daulXJzpX/8Q3rjDcc93//3f6zrDaBW6dix43nnC4F3JCQkaMmSJbLb7WrevLmWLVumiPOMvNq1a5fmzp2r3bt3q7CwUPXr19d3331XhRV7lt1u1/Zj2/Xerve0avcq7Tm5x7nPaDCqb8u+GtZumAZHD1bT+k3PcyYAQG1C6K7pnn9euuUWaepUae9exz3fzz8vPfOMVM4M8gCqpxp+t0+dU9f/vNauXaulS5dq48aNCg0NVWJiogYPHqzk5OQy22/btk3Dhw9XQkKCbrjhBkmOIfo1nd1u13dp32nVbscV7d/O/Obc5+fjp/5X9NfwdsM1qO0gXVbvMi9WCgDwFkJ3TWcwSDffLPXvL73wgmM5sW3bHJOuPfSQtGiRlwsEcCFFSy9mZ2ezHnQNkp2dLUmlls6sKxISEjR//nyFhoZKkkaPHq3nnntOW7duVZcuXUq1nzx5sv7xj384A7ekGjuHQUFhgTanbtZ7u97T+3ve1yHLIee+AN8ADWw9UMPaDdMtV96ikIAQL1YKAKgOCN21hZ+f477uUaOkuXOLZzsHUO0ZjUaFhobq+PHjkqR69eqxBFA1ZrfblZ2drePHjys0NFTGOno7z/r165WYmOiyrU+fPlq3bl2p0H3kyBHt379fgwYNqtC5bTabbDab87XFYrn0gi9RfmG+kg4madWuVVq9Z7WOZR1z7qvvX19/bPNHDWs3TAPbDFR9//perBQAUN0QumubsDDH8PKHHpJatize/uqrjp9jxkj/W1IHQPXRtKnj/s6i4I3qLzQ01PnnVtdkZmbKaDQqMDDQZXtUVFSZ995v375d0dHReu+997R48WLl5OSoa9eumj9/fpnroMfHx2vevHkeq7+ibPk2fXHgC63atUr/3ftfnco55dwXYgrRoLaDNLz9cN14xY0K8K2ZV+0BAJ5H6K6tWpVYcuTkScc93xkZxfd79+7ttdIAlGYwGBQeHq7GjRsrLy/P2+XgAvz8/OrsFW5JSk9PL/NWCLPZ7Bx2X9KpU6e0a9cubdq0SevXr5fJZNILL7ygP/zhD/rpp59KDdGfMWOGpk6d6nxtsVgUFVU1M3zn5OXok18+0ardq/TBvg9ksRVfZQ+rF6bBbQdrWPthuqHlDfI3VmwNeQBA3UborguCgqTZs6UnnpC2bpWuv17605+kv/9datHC29UBKMFoNNbpMIeawWQylTkJmtVqLTOM+/j4yM/PT4sXL3b+9z1x4kS9+uqr+vrrr13u8y46v8lk8kzxZThrO6uP93+sVbtX6eP9HysrL8u5r2n9phoaPVTD2g/T9ZdfL18f/ukEAKgceo66wGRyDDe/805pzhzp5Zelf/9bWrPGsX3GDKk+958BAComLCxMOTk5ysrKchlinpqaqsjIyFLtGzdurFatWpX6hVKrVq104sQJj9dblnRrutbsXaNVu1fp018+la2g+B7y5iHNNTR6qIa3H67uUd3lY+C2LADAxSN01yWNGzsmWLvvPmnKFOnLL6WFC6U77pBiYrxdHQCghjAYDIqNjdWGDRs0cOBA5/akpCQtWLCgVPvOnTtr//79ys3Nlb9/8ZDsffv2qXXr1lVSc5EP9n6gF75/QV/89oXyCotv5WjdsLWGtRumYe2GqWuzrkxmCABwG0J3XXT11dIXXziudO/c6Rq4Dx5kyDkA4IImTZqk2bNnq0ePHgoJCdHy5cuVmZmpvn37lmrbsGFD/eEPf9C0adO0ePFi+fj46J///KcaNmyoa665pkrr3nVilz755RNJUvtG7TWs3TANbz9cVzW+iqANAPAIQnddZTBIt93meBT5+WepUydp+HDpb3+Tmjf3WnkAgOptyJAhSklJUWxsrAwGgyIiIrRmzRr5+PgoLy9PQ4cOVUJCgsLDwyVJL7zwgu6//361aNFCPj4+uvbaa7Vy5coqr/v2DrcrvzBfw9oPU3RYdJW/PwCg7jHY7Xa7t4u4FBaLRSEhIcrIyFBwcLC3y6nZ/vUvx1rfdrsUECBNmyY9+qh0zpIwAIDzo29yL75PAEB1U5m+iZlBUGziROmHHxyzm1ut0vz5Utu2UmKiVFjo7eoAAAAAoMYhdMNV585SUpL03ntSy5ZSWppj1vObb3ZcAQcAAAAAVBihG6UZDNKwYdKuXVJ8vGM5sQEDHNsBAAAAABVG6Eb5AgKk6dOl/ful++8v3v7pp9Ljj0vZ2V4rDQAAAABqAkI3LqxpU6loXdW8PMdka/PmOe73Xr6cYecAAAAAUA5CNyrH19cxwdrll0uHDkmjRkk9ekjffeftygAAAACg2iF0o3IMBulPf5J275YWLHAsJ/bNN1JsrHTXXY6J1wAAAAAAkgjduFhmszRrlrRvn3T33Y5tb73lWHIMAAAAACCJ0I1L1ayZ9PrrjuHlDz0k3Xpr8b5ffuF+bwAAAAB1GqEb7tGtm7RoUfGyYqdOSddeK/XqJSUne7c2AAAAAPASQjc8IzlZstmkzZsd4XvsWOn4cW9XBQAAAABV6qJD9+uvvy6z2ayUlBSX7SaTSTExMS6Pjz766LznSkhIUExMjDp06KCBAwcqjcm4ar6bbpL27pXuvNPx+rXXpCuvlP71Lyk/37u1AQAAAEAV8b2Yg2bNmqWtW7cqJCREubm5Lvtyc3O1bds2+fpW7NRr167V0qVLtXHjRoWGhioxMVGDBw9WMkOSa77ISOnNN6UJE6QHHpC2bpUmTZJefdUx47nJ5O0KAQAAAMCjKn2lu7CwUBEREfrwww8VEBBwyQUkJCRo/vz5Cg0NlSSNHj1aRqNRW7duveRzo5ro3t0x0dqLL0oNGkjXXUfgBgAAAFAnVDp0+/j46L777pPRaHRLAevXr1dcXJzLtj59+mjdunVltrfZbLJYLC4P1ABGozR+vGOJsaeeKt6+d6/0z39KeXneqw0AAAAAPMSrE6llZmbKaDQqMDDQZXtUVJQOHDhQ5jHx8fEKCQlxPqKioqqiVLhLWJjjarfkWE5s4kTp4YelTp2kL7/0amkAAAAA4G4eCd033XSTrrrqKsXGxuqZZ55RYWFhme3S09NlNptLbTebzcrOzi7zmBkzZigjI8P5SE1NdWvtqGIjRzqC+K5d0g03SCNGSIcOebsqAAAAAHALt4fuI0eOaN26ddqxY4dWrlyp1atXa+HChWW2NZlMslqtpbZbrdYyw3jRMcHBwS4P1FAGg2MpsX37pPvvl3x8pJUrpeho6W9/k86ZpA8AAAAAahq3h+6mTZs6n19++eV68skn9d5775XZNiwsTDk5OcrKynLZnpqaqsjISHeXhuqqQQPpueekH36QevaUsrKk6dOlZcu8XRkAAAAAXBKP39Odn59f7vJhBoNBsbGx2rBhg8v2pKQkde/e3dOlobrp1En6+mvpjTek66+X/vKX4n3l3KIAAAAAANWZW0N3VlaWjh496nx94MABTZs2TWPHji33mEmTJmn27NnKyMiQJC1fvlyZmZnq27evO0tDTWEwSHfdJSUlSf7+jm15eY5lxhYskMq4HQEAAAAAqquyL0FXkL+/v/z8/Jyv09PT9cc//lFWq1V+fn4KDAzU5MmTNXr0aGebRYsWqUmTJrrzzjslSUOGDFFKSopiY2NlMBgUERGhNWvWyMfHqxOrw9sMhuLn770nJSc7Hq+/Lj37rPTHP3qtNAAAAACoKIPdbrd7u4hLYbFYFBISooyMDCZVq63sdumddxxLix0+7Nh2663SM89IrVp5tTQAKAt9k3vxfQIAqpvK9E1cTkb1ZzBId9wh7dkjTZsm+fpKH3wgtW8vzZ0r5ed7u0IAAAAAKBOhGzVHUJD0979LO3ZI/fpJNptj4jWj0duVAQAAAECZLumebsAroqOlzz6T3n9fateu+P7v9HTp+HHpyiu9Wh4AAAAAFOFKN2omg0EaNswxxLzInDnSVVdJM2c61voGAAAAAC8jdKN2KCyUDh6UcnOl+HjH1fB//9sxCRsAAAAAeAmhG7WDj4/03/86Hi1aSIcOSbffLvXvL+3e7e3qAAAAANRRhG7UHgaDNGiQtGuXY1Zzk0n64gvp6qsdV70BAAAAoIoRulH7mM3S4487wvegQVJgoBQX5+2qAAAAANRBhG7UXq1aOYab79wpNW5cvH3OHMeyYwAAAADgYYRu1H6RkcXPP/xQmj9f6txZmjzZscwYAAAAAHgIoRt1y9VXO5YaKyiQliyR2raV3njDMfs5AAAAALgZoRt1S/Pm0nvvSZ995gjcx49LY8ZIvXpJW7d6uzoAAAAAtQyhG3VT//7STz9Jf/+7Y6K1LVukkSMdV8ABAAAAwE0I3ai7/P2ladOkvXulO+6Qnn5aMhod+/bulQYMkOLjpW++kfLyvFsrAAAAgBrJ19sFAF4XESEtX+66bf16xxD0zz5zvK5f3zEEvU8fqW9fqUsXyZf/fQAAAACcH1e6gbLcdJNjorUhQ6SGDaXMTOmTT6Tp06XY2OIwLklnz0r5+d6rFQC8JCEhQTExMerQoYMGDhyotLS0ctsOGDBALVu2VExMjPPx+OOPV12xAAB4CZfqgLK0bClNnOh4FBY61vVOSpK+/FLavFnq3bu47ZNPSi++6NjWt6/janinTsVD1QGgFlq7dq2WLl2qjRs3KjQ0VImJiRo8eLCSk5PLbG+z2fTyyy+rX79+VVwpAADexZVu4EJ8fKSOHR3rev/nP9KxY1JQUPH+5GTJYpE++kh6+GGpa1fpssukQYOkxYslm81rpQOApyQkJGj+/PkKDQ2VJI0ePVpGo1FbWQkCAAAXhG6gsgwG19effSZ9/720aJF0yy1ScLCUkSF98IH01FOOCduKrF0rbd/OuuAAarz169crLi7OZVufPn20bt26Sz63zWaTxWJxeQAAUFMxvBy4VEajdM01jsdDDznu7962zTEcPT+/OKTb7dK4cdKRI477xOPiHEPR+/SRYmIcV9QBoAbIzMyU0WhUYGCgy/aoqCjt3Lnzks8fHx+vefPmXfJ5AACoDvhXPuBuvr6OIeYPP+yYeK3I2bOOYeqBgdLp09Lq1Y4h6x07So0bu7YFgGosPT1dZrO51Haz2azs7OwyjzEYDJo5c6a6dOmijh07asqUKTp9+nSZbWfMmKGMjAznIzU11a31AwBQlQjdQFUJDnYMLz9zRtqyxbEG+I03SvXqSadOua4FnpUljRghvfCCtHu34yo5AFQTJpNJVqu11Har1VpmGJeklStXasuWLdq6dau+/vprFRQUaOTIkeWePzg42OUBAEBNxfByoKr5+UnXXed4TJ/uCNvff++YfK3I5s3SypWOhyQ1aVI8FL1PH6lt29L3lgNAFQkLC1NOTo6ysrJchpinpqYqMjKyzGMaNWrkfB4cHKzFixcrODhYGRkZCgkJ8XjNAAB4C1e6AW/z85O6d5euvLJ42xVXSPPnSzfcIAUEOGZMf/ddacIEqV076aWXitsePSr98INjLXEAqAIGg0GxsbHasGGDy/akpCR17969QucoKCiQJPkwnwUAoJajpwOqo1atpMcek774QkpPl776Spo3z3GV22SSevYsbvv++457yIOCpMhIqV8/6YEHpOeekz7/3LGcGQC42aRJkzR79mxlZGRIkpYvX67MzEz17du3zPa//PKL83lGRoYmTJigQYMGKajkEowAANRCDC8HqjuTSbr+esdjzhzJanVdhiwvzzER2/HjUlqa4/HFF8X7N24sDulfful4HR3teLRp47iSDgCVNGTIEKWkpCg2NlYGg0ERERFas2aNfHx8lJeXp6FDhyohIUHh4eGSpAcffFB79uyRyWSS0WjUsGHDNG3aNC9/CgAAPM9gt1/cDE2vv/66JkyYoL1796p58+aSpMOHD+uxxx7TN998I4PBoEaNGumZZ55Rp06dyj3PihUrNGHCBJd7wAICAvTtt9/KaDResA6LxaKQkBBlZGQw0QrqtjNnpL17pT17HI+i519/LYWFOdo89JD09NPFxxgMUosWxSF82jTpf/9ABnDx6Jvci+8TAFDdVKZvuqgr3bNmzdLWrVsVEhKi3Nxc5/bCwkKNHj1ar7zyigwGg/773/9q0KBB2r9/v0wmU5nnstlsuuWWW5SYmHgxpQAo0qBB8QRt5eneXRozpjiYp6dLBw44HmvXSo88Utz2ySeljz5yhPG2bYuDeatWjvvQAQAAAFxQpUN3YWGhIiIi9MQTT+iKK65w2RcZGelyxfq2227TnDlztGvXLnXu3PnSqwVwaYYPdzwkxzJkx48XXxH/9VfHLOlFkpMdS5tt2eJ6Dl9fx0RvmzYVz7ieluZYfzw0tEo+BgAAAFBTVDp0+/j46L777qtw+/T0dLcOBbPZbLLZbM7XFiaJAi6OweAI2U2aOO4XP9fChdKf/+w6VH3PHik7Wzp0yHFlvciUKdJ77znOVXRFvOTj8stZ4gwAAAB1kkcnUvv444/VuHHjUlfEL0V8fLzmzZvntvMBKEdRYC6psNBxVfvQIankMj+nTzt+HjvmeHz1VfG+wEDp7Nni1089JR0+LEVEuD4iI6X69T33eQAAAAAv8FjozsrK0uTJk7V06dLztjMYDNqwYYN69eqlU6dOqXXr1po5c2a563zOmDFDU6dOdb62WCyKiopya+0AyuHjI0VFOR4lffGFY2myffuKr4gXPXx8XK9yv/ee9OOPZZ8/IsIR6Iu89ZYjsEdGFofzxo1dAz8AAABQjXksdI8bN05Dhw7VDTfccN52w4cP15AhQxQcHCy73a61a9fqtttu0+bNm9W6detS7U0mU7mTsgHwouBgx3rhXbu6bj93gYTJkx3D1YuWNyt6nD1b+kr34sWlA7qvr2OG9eho6bPPircnJTl+FoXzevXc8rEAAACAS+GR0L1gwQJZLBbFx8dfsG1gYKDzucFg0M0336xBgwbp448/1qRJkzxRHoCqdO693HffXXa7s2cds6mXNHCg437woiHtx45J+flSamrpUP3gg9K2bcWvQ0OLr5BfeaW0ZEnxvt9+k4KCHEupca85AAAAPMjtofudd97Ru+++q02bNsnnIoeA5ufny9fXo7ebA6hugoIcj5KefNL1dX6+dPSoI4SXWK5QkmMps6wsx77sbEeAT0+Xdu6Ufv/dte2wYY6A7u8vNWvmem95y5bSxInFbfftcyyRdtlljvoI6QAAAKgEtybbLVu26NFHH9VXX31V4RnLU1JS1LRpU/n7+8tut+v999/Xp59+WqGr5ADqGF9fx9XrEksTOq1a5fhpt0sZGa5D18/9JV5eniM85+ZKBw86HkXat3cN3UOHSj//7HhuNEoNGxY/2rSR3njDtQabzbVNw4ZSSIjjWAAAANQ5lxS6/f395efn53y9cOFCZWdn65ZbbnFpN2nSJN17772SpEWLFqlJkya68847JUmff/65Fi5cKH9/fxkMBrVv317r169XeHj4pZQGoK4yGBxDy0NDpQ4dym6zc6cjcB854hrODx92hOSSTCbJbJZycqSCAunECcdDcoT7kubOLQ7o59bUtq20e3fxtieecKyTfm5Ab9jQMez9yisv9hsAAABANWKw28+d5ahmsVgsCgkJUUZGhlvXAwcAFzk50pkzjuXRTp1y/PT1lW69tbjNX/8q/fKLY1/RIzPTsa9DB0fYLxITU3ZAlxwTxR0+XPx6xAjHeUsG89BQx3JsYWHS+PHFbbdudVxtDwx0TEwXGOh41KvHrO9ViL7Jvfg+AQDVTWX6Jm6cBoCKMJsdj2bNym9T1hKJubmOsG61um6fPFlKSXEN6EWPJk1c2+7cKe3aVfZ7NmvmGrofeEDasqXstk2bOq7uF5kyRdqxoziYlwzpQUHS9OnFbX/4wXFl/9x2gYGO74V73QEAAMpE6AYAT/L3Lx2iJemeeyp+jrfecszcXnSF/fRpxyRxWVmlJ59r1swxGVxWVvGjPN9/L23aVPa+wEDX0D17trR2bdltfXwcv1woum/90Uelr78uDuUBAY7vwc/P8fjXvxw/Jcd98D//7Hhdsk3R8xEjHEP8JUe7I0fKb9uiRfH9+1arVFjo2O7ryy8FAACA1xC6AaC669Kl4m3fe8/1dWGhY2h8VpZj2HlJTz7pGMaemeka0rOySg9Fj4pyTDJXtD8zs/jqvb+/60RxP/9c/tV2SXruOdd633mn/La33VYcuhcvll55pfy2hw45ZqCXHMG/5DJxRQG9KKR/841jxntJevZZ6bXXpCFDHPflAwAAuBGhGwBqMx+f4ivO54qLq/h5yho6X1DgWJ4tO9t1++OPS+PGFQd0m81xJTwvz/EoGdD79XPM7p6X59qm6HlR4JYcgfqqq0q3LXrt71/cNi/PtaaidkVK/lLh0CFp+3apW7eKfx8AAAAVxERqAIDaJy/PEfbPDfFFz6+8sjik798vHTjgGJofE+OWt6dvci++TwBAdcNEagCAuq1oKHlFtGnjeAAAAHgA68cAAAAAAOAhhG4AAAAAADyE0A0AAAAAgIcQugEAAAAA8BBCNwAAAAAAHkLoBgAAAADAQwjdAAAAAAB4CKEbAAAAAAAPIXQDAAAAAOAhhG4AAAAAADyE0A0AAAAAgIcQugEAAAAA8BBCNwAAAAAAHkLoBgAAAADAQwjdAAAAAAB4CKEbAAAAAAAPIXQDAAAAAOAhhG4AAAAAADyE0A0AAAAAgIcQugEAAAAA8JCLDt2vv/66zGazUlJSXLbv2rVLcXFxiomJUceOHbVq1aoLnishIUExMTHq0KGDBg4cqLS0tIstCwAAVJGL7b8XLFggg8GggwcPerZAAACqAd+LOWjWrFnaunWrQkJClJub69xutVo1aNAgvfzyy+rbt6/S0tIUFxenK664Qp06dSrzXGvXrtXSpUu1ceNGhYaGKjExUYMHD1ZycvJFfSAAAOB5F9t/Hzx4UB9++KEiIyOVn59fRdUCAOA9lb7SXVhYqIiICH344YcKCAhw2ffpp5+qS5cu6tu3ryQpIiJCDz/8sF599dVyz5eQkKD58+crNDRUkjR69GgZjUZt3bq1sqUBAIAqcrH995QpUxQfHy+j0VgFVQIA4H2VDt0+Pj667777yuwsv/jiC/Xp08dlW58+fbRu3bpyz7d+/XrFxcVV+BibzSaLxeLyAAAAVauy/bckffLJJ/L19XX+cr489PUAgNrErROpHT58WJGRkS7boqKidODAgTLbZ2Zmymg0KjAwsMLHxMfHKyQkxPmIiopyT/EAAKBCLqb/ttlseuSRR7Ro0aILnp++HgBQm7g1dKenp8tsNrtsM5vNslqtstvtFWpfdEx2dnaZ7zFjxgxlZGQ4H6mpqe4pHgAAVMjF9N+LFi3SoEGD1KJFiwuen74eAFCbXNREauUxmUyyWq0u26xWq0wmkwwGQ4XaFx1TVmdedIzJZHJPwQAAoNIq23+npKTo9ddf17Zt2yp8fvp6AEBt4dYr3ZGRkaV+G52amlpqyHmRsLAw5eTkKCsrq8LHAAAA76ps//3II49ozpw5pYajAwBQF7g1dPfo0UNfffWVy7akpCR17969zPYGg0GxsbHasGFDhY8BAADeVdn++8iRI1qwYIGio6Odj7S0NA0YMEBTp06tqrIBAPAKt4bu4cOH65tvvlFSUpIkx8Rq//jHP3T//feXe8ykSZM0e/ZsZWRkSJKWL1+uzMzMC85sCgAAvKcy/fdXX32lvXv3as+ePc5HRESEPv30Uz399NNVXToAAFXqku7p9vf3l5+fn/N1YGCg1qxZowkTJig9PV2SNG/ePF133XXONosWLVKTJk105513SpKGDBmilJQUxcbGymAwKCIiQmvWrJGPj1t/HwAAANzofP13Xl6ehg4dqoSEBIWHh5d5vJ+fn3x93Tq1DAAA1ZLBXta04jWIxWJRSEiIMjIyFBwc7O1yAACgb3Izvk8AQHVTmb6Jy8kAAAAAAHgIoRsAAAAAAA8hdAMAAAAA4CGEbgAAAAAAPITQDQAAAACAhxC6AQAAAADwEEI3AAAAAAAeQugGAAAAAMBDCN0AAAAAAHgIoRsAAAAAAA8hdAMAAAAA4CGEbgAAAAAAPITQDQAAAACAhxC6AQAAAADwEEI3AAAAAAAeQugGAAAAAMBDCN0AAAAAAHgIoRsAAAAAAA8hdAMAAAAA4CGEbgAAAAAAPITQDQAAAACAhxC6AQAAAADwEEI3AAAAAAAeQugGAAAAAMBDCN0AAAAAAHgIoRsAAAAAAA/xdefJCgoKFBsbK6vV6rI9JSVFK1eu1E033VTqmAEDBmjfvn0KDAx0bhs+fLgef/xxd5YGAAAAAECVc2voNhqN+v7771225ebmqlWrVuratWuZx9hsNr388svq16+fO0sBAAAAAMDrPD68/L333lPv3r0VFhbm6bcCAAAAAKBaceuV7rK89NJLmjdvntvOZ7PZZLPZnK8tFovbzg0AAAAAgDt59Er3rl27dPToUfXp08dt54yPj1dISIjzERUV5bZzAwAAAADgTh4N3S+99JL+8pe/yGAwlNvGYDBo5syZ6tKlizp27KgpU6bo9OnT5bafMWOGMjIynI/U1FRPlA4AAAAAwCXz2PDynJwcvfvuu9qxY8d5261cuVINGzaU0WiUxWLRrFmzNHLkSH322WdltjeZTDKZTJ4oGQAAAAAAt/JY6F6xYoWuv/56NW7c+LztGjVq5HweHBysxYsXKzg4WBkZGQoJCfFUeQAAAAAAeJzHhpe/9NJLuvfeeyt9XEFBgSTJx8fjE6sDAAAAAOBRHkm2P/74o06ePFmhtbd/+eUX5/OMjAxNmDBBgwYNUlBQkCdKAwAAAACgyngkdL/88su67777Sk2glpeXp1tvvVVHjhxxbnvwwQfVpk0bxcTE6Prrr1eLFi302muveaIsAAAAAACqlMFut9u9XcSlsFgsCgkJUUZGhoKDg71dDgAA9E1uxvcJAKhuKtM3ceM0AAC4KAkJCYqJiVGHDh00cOBApaWlldkuNzdXgwcPVvv27dW+fXvFxMRoyZIlquG/9wcAoEI8Nns5AACovdauXaulS5dq48aNCg0NVWJiogYPHqzk5ORSbf38/LRgwQLFxMRIkg4fPqxbbrlFkjRp0qQqrRsAgKrGlW4AAFBpCQkJmj9/vkJDQyVJo0ePltFo1NatW0u1NRgMzsAtSc2aNdOMGTP04YcfVlW5AAB4DaEbAABU2vr16xUXF+eyrU+fPlq3bl2Fjs/IyFB4eHiZ+2w2mywWi8sDAICaitANAAAqJTMzU0ajUYGBgS7bo6KidODAgfMea7Va9d///leLFy/WzJkzy2wTHx+vkJAQ5yMqKspttQMAUNW4pxsAAFRKenq6zGZzqe1ms1nZ2dllHpOVlaXY2FgdOHBARqNRK1asUNu2bctsO2PGDE2dOtX52mKxELwBADUWoRsAAFSKyWSS1Wottd1qtZYZxiUpMDBQO3fulCT9+OOPGjt2rEwmk/r161fm+U0mk3uLBgDASxheDgAAKiUsLEw5OTnKyspy2Z6amqrIyMgLHt+5c2fNnDlTL774oqdKBACg2iB0AwCASjEYDIqNjdWGDRtcticlJal79+4VOkdGRoYKCgo8UR4AANUKoRsAAFTapEmTNHv2bGVkZEiSli9frszMTPXt27dU29TUVJer4t98842eeOIJTZ48ucrqBQDAW7inGwAAVNqQIUOUkpKi2NhYGQwGRUREaM2aNfLx8VFeXp6GDh2qhIQEhYeHKykpSfPnz5ePj4/8/f3VuHFjvfnmm+rTp4+3PwYAAB5nsNvtdm8XcSksFotCQkKUkZGh4OBgb5cDAAB9k5vxfQIAqpvK9E0MLwcAAAAAwEMI3QAAAAAAeAihGwAAAAAADyF0AwAAAADgIYRuAAAAAAA8hNANAAAAAICHELoBAAAAAPAQQjcAAAAAAB5C6AYAAAAAwEMI3QAAAAAAeAihGwAAAAAADyF0AwAAAADgIYRuAAAAAAA8hNANAAAAAICHuD10r1ixQqGhoYqJiXE+unbtqoKCgjLbWywWjRo1Su3atVN0dLQef/xx2e12d5cFAAAAAECVc3vottlsuuWWW7Rz507n4/vvv5fRaCyz/T333KN27dpp9+7d2r59u77//ns9//zz7i4LAAAAAIAq59Xh5adPn9bmzZs1Y8YMSZLJZNKiRYuUkJDgzbIAAAAAAHALX2++eVJSkrp37+5yFTw6OlrHjx/XsWPH1KRJk1LH2Gw22Ww252uLxVIltQIAAAAAUFleDd2HDx9WZGRkqe1RUVE6ePBgmaE7Pj5e8+bNq4ryAAAAAADVxJkzZ5Sbm6u8vDzl5eW5PA8MDNSVV17pbLt27Vrl5OS4tDWbzbrjjjuqvG63h26DwaANGzaoV69eOnXqlFq3bq2ZM2eqe/fupdqmp6fLbDaX2m42m5WdnV3m+WfMmKGpU6c6X1ssFkVFRbnvAwAAAAAAnHJzc7V7926dPXu2zMfVV1+t2267TZKUmZmpKVOmlArFRc9vuOEGzZw5U5JjFHOnTp3KDNG5ubm69dZbtXLlSmcdl112WbmTbt9444369NNPna9HjBihs2fPurRp3rx57Qjdw4cP15AhQxQcHCy73a61a9fqtttu0+bNm9W6dWuXtiaTSWfOnCl1DqvVWmYYLzrGZDK5u2wAAAAAqNEKCgp0+vRpZxjOzMx0CccdOnRQbGysJOno0aN65JFHXPaXbH/PPffo73//uyTp2LFj6tSpU7nvO27cOGfoLigo0CuvvFJu20aNGjmfG41G7dmzp9y2OTk5Lq/9/PyUm5srPz8/+fv7y8/Pz/n8sssuc2kbGxurnJwclzZljaSuCm4P3YGBgc7nBoNBN998swYNGqSPP/5YkyZNcmkbGRmp5OTkUudITU0tc9g5AAAAANQ2hYWFysjI0MmTJ3Xy5Ek1a9ZMl19+uSQpLS1Nzz33XJlXmDMzM3XPPfdo8uTJkqTdu3frqquuKvd9pk6d6gzdubm5euutt8ptW/LiaHBwsJo2baqgoCAFBQWpfv36zudBQUGKi4tztjWbzXryySfLDMZ+fn5q0aKFs63RaFRSUlKpNkXP69ev71JTVlaWjEajDAbDBb/Tzz///IJtqkqV3NOdn58vX9/Sb9W9e3c9/PDDKigocE6mtnfvXvn5+RG6AQAAANQ4drtdFovFGaBPnTqlkydPqmPHjurYsaMkaefOnbrvvvtc2hQWFjrPsWDBAs2aNUuSY8WnhQsXlvt+qampzudBQUGSpHr16pUKxkFBQYqOjna2DQsL09///vdSbYpCdckr0iEhITpy5EiFPr+/v79z+PiFGAwGl8B+IWVlyprA7VWnpKSoadOm8vf3l91u1/vvv69PP/1U8fHxpdq2aNFCXbt21cKFCzVr1izZbDZNmzZNEydOdHdZAAAAAFApdrtdWVlZznB87qN///7O0JicnKxBgwbp5MmTys/PL3Wu+fPnO0O3JH399del2gQFBSksLMzlVtvw8HBNmTKlzCvM9evXV6tWrZxtmzdvrvz8fJfVocpTr149TZs2rVLfBy6O20P3559/roULF8rf318Gg0Ht27fX+vXrFR4erry8PA0dOlQJCQkKDw+XJL322msaP3682rZtq8LCQg0dOlQPPfSQu8sCAAAAAGVlZWnfvn3lBulRo0Zp0KBBkqQNGzaoT58+5Z7LZDI5Q7fZbNbRo0ed++rVq6ewsDDno3nz5s59LVq00Lvvvuuy/7LLLitz7qqwsDAtXry4Qp/NYDBUKHCjark9dI8bN07jxo0rc5+fn58++OADl20NGzZ0mZEOAAAAACrLarUqLS1NqampOnTokFJTU53Px44dq8GDB0uSfvjhh/MOaY6JiXGG7rCwMEmOcN2oUSNnOC4Kyl27dnUe17p1a/3444/ONuVNDC1J9evX1+233+6GT42aoGYOigcAAABQZ9hsNqWlpTnD9KFDh9SrVy/17NlTkuOK9PmCdJcuXZyhu1GjRgoPD3e5ylwyTBedU5Kio6OVmZmpevXqXXDyroCAgPPO8I26i9ANAAAAwGtyc3N1+PBhpaamqlmzZrriiiskOSYbGzNmjFJTU3X8+PFSx82ZM8cZkItuXTWbzYqKilJkZKTLz+7duzuPa9eunQ4fPlyh2oxGo8vqTMDFIHQDAAAA8Ii8vDzl5uY6g2tqaqoWLVrkMgT82LFjstvtkqTZs2friSeekOS4cvzDDz84zxUQEOASpktOStaqVSudOnVKDRo0qNByUkBVInQDAAAAuGhnzpzRZ599Vuo+6tTUVB09elSPPfaYM0jbbDYtWbKk1Dn8/f0VGRmpevXqObc1b95ca9ascQbtyy67rNxAbTQa1bBhQ898QOASEboBAAAAlKuwsFApKSnas2eP9uzZo71796p3797685//LEk6fvy4Ro4cWe7xaWlpzueRkZF69NFHSw0Bb9SoUalA7e/vr1tvvdUzHwqoQoRuAAAAACosLJSPj48k6eTJk7r//vu1Z88e7du3T1ar1aVtVlaWM3S3atVKcXFxioiIcAbpkqG6aAZwyTFEfOHChVX3oYBqgNANAAAA1BGFhYVKS0tzuWpd9HPAgAFatmyZJCkoKEjvvfeeCgsLJTmuOrdp00bR0dFq27atevXq5Tynn5+fkpKSvPFxgBqB0A0AAADUMtnZ2dq/f7/y8vKca0nbbDaFhYUpMzOzzGN2797tfG4ymfTiiy8qIiJC0dHRatGihYxGY5XUDtQ2hG4AAACgBvvyyy9LXbn+/fffJUlxcXHOq9Amk0kNGzaU1WrVFVdcoejoaOeV66KfJd17771V/VGAWonQDQAAAFRjVqtVv/zyizNY+/r6avr06c79o0aN0pEjR0od17BhQ4WGhrps27Rpk5o0aSI/Pz9Plw3gfwjdAAAAQDUTHx+vjRs3as+ePTp48KDz3mrJMQN4ydDdv39/nT592uWqdXR0tMsEZiWPBVC1CN0AAOCiJCQkaMmSJbLb7WrevLmWLVumiIiIUu0KCwv12GOP6aOPPlJBQYF8fX01c+ZM3X777V6oGqg+7Ha7Dhw4oE2bNunAgQOaM2eOc99HH32kTZs2OV+HhIQ4w3R0dLTsdrtzia033nijymsHUHGEbgAAUGlr167V0qVLtXHjRoWGhioxMVGDBw9WcnJyqbYGg0GdOnXSnDlzFBAQoN9++009e/ZU27Zt1bFjRy9UD3iHzWbT1q1btXnzZm3atEmbN2/WsWPHJDn+P5kyZYqCg4MlSRMnTtSf//xndejQQdHR0WrcuHGpdawB1AyEbgAAUGkJCQmaP3++837R0aNH67nnntPWrVvVpUsXl7YGg8HlqnarVq10++23a/369YRu1GrHjx9XWFiYc+3re++9V2+++aZLGz8/P11zzTXq2bOnrFarM3SPGDGiyusF4BmEbgAAUGnr169XYmKiy7Y+ffpo3bp1pUJ3Wc6cOaOrr766zH02m002m8352mKxXFqxQBUoLCzU7t27nVewN2/erP379+unn37SVVddJUm67rrr9PHHH6tnz57q0aOHevbsqWuuuUYBAQFerh6AJxG6AQBApWRmZspoNCowMNBle1RUlHbu3HnB40+cOKFPPvlE//jHP8rcHx8fr3nz5rmlVsDTNmzYoPj4eG3ZskUZGRml9u/cudMZuv/yl79o/PjxDBMH6hhCNwAAqJT09HSZzeZS281ms7Kzsy94/MSJEzVhwgQ1adKkzP0zZszQ1KlTna8tFouioqIuvmDADVJSUpz3Yg8fPlxxcXGSHMt5ffLJJ5KkwMBAxcbGqkePHurRo4euu+46NWjQwHkOlukC6iZCNwAAqBSTySSr1Vpqu9VqLTOMl/Tiiy/q0KFDpYamn3t+k8l0yXUCFys/P1/btm1zGSp+6NAh5/569eo5Q/d1112nf/3rX+rRo4euvvpq+fryz2sArvhbAQAAVEpYWJhycnKUlZXlMsQ8NTX1vGsAf/nll1q0aJE2b95MMEG1cvr0aVksFrVo0UKSdODAAXXr1s2ljdFoVOfOndWjRw8NGDDAuT04OFgPPPBAVZYLoIahxwMAAJViMBgUGxurDRs2aODAgc7tSUlJWrBgQZnH7NmzR3fffbc++OCDcoeVA1XBbrdr3759Lst27d69W3/605+0cuVKSVLr1q115ZVXqnXr1s5Jz7p161ZqHgMAqAhCNwAAqLRJkyZp9uzZ6tGjh0JCQrR8+XJlZmaqb9++pdqePHlSgwYN0gsvvMASYfCavLw8TZ8+XW+//bZzbeySTpw44XxuMBi0Z88eJjwD4BaEbgAAUGlDhgxRSkqKYmNjZTAYFBERoTVr1sjHx0d5eXkaOnSoEhISFB4errfeekuHDh3S9OnTNX36dOc5unfvrpdfftmLnwK1XX5+vvNWBj8/P23YsEHHjh2TyWRSt27dnFexu3fvrkaNGrkcS+AG4C4Gu91u93YRl8JisSgkJEQZGRkKDg72djkAANA3uRnfJyrDbrfr22+/1SuvvKI1a9Zo3759CgkJkSR9+umnysvL04033ih/f38vVwpIBQUFysvL83YZKIOfn5+MRmO5+yvTN3GlGwAAADXeyZMnlZiYqGXLlunnn392bv/Pf/6ju+++W5JcJkADvMlut+vo0aNKT0/3dik4j9DQUDVt2vSSR74QugEAAFBj/frrr5o5c6b+85//KDc3V5Jjzfjhw4frL3/5i3r37u3lCoHSigJ348aNVa9ePW5nqGbsdruys7N1/PhxSVJ4ePglnY/QDQAAgBql5L3aZrNZ7733ngoLC3XNNddo3LhxuuOOOxQaGurdIoFyFBQUOAP3ZZdd5u1yUA6z2SxJOn78uBo3bnzeoeYXQugGAABAtZebm6sPPvhAy5Ytk91u1yeffCJJatasmZ5//nnFxsaqc+fOXq4SuLCie7jr1avn5UpwIUV/Rnl5edUrdH/88cdatGiRjh49Kknq27evFi1a5PxNwbkGDBigffv2uax7OHz4cD3++OPuLg0AAAA1zO7du/XKK6/ozTffdC7r5ePjo6NHj6pp06aSpPHjx3uzROCiMKS8+nPXn5HbQ7fZbNbrr7+u5s2bKy8vT3feeafmzJmjf/zjH2W2t9lsevnll9WvXz93lwIAAIAaau3atXryySe1adMm57bw8HCNGTNGY8eOdQZuAKjufNx9wr59+6p58+aSHNOsP/roo/rss8/c/TYAAACoRex2u/Lz852vjx49qk2bNsloNGrQoEFas2aNUlJS9NRTT6l169ZerBQAKsfj93SfOXPGrWtq2mw22Ww252uLxeK2cwMAAKBqnT592rnU17hx4zR58mRJ0p/+9CcdP35cd9111yXPHAwA3uTx0P3SSy9pxIgRbjtffHy85s2b57bzAQAAoGoVFhZq/fr1euWVV7R69WrnBZXExERn6K5fv74effRRb5YJVAm73a7svGyvvHc9P5YrqwoeDd2ffPKJtm/frrfeeqvcNgaDQTNnztQjjzyigoIC9e3bV3PmzFHDhg3LbD9jxgxNnTrV+dpisSgqKsrttQMAAMD9/v73v+vFF1/UwYMHnds6deqkv/zlL/rzn//svcIAL8nOy1b9+Ppeee/MGZkK9A+8cENJcXFxGjNmjJYsWSKr1SqTyaRnn31WcXFx2rp1qyZMmKCsrCz5+vpq4cKFuummm3TPPfcoLi5OL7/8so4fP678/HzNnz9fI0eOrNB7HjhwQOPHj9fvv/8uo9GoyMhILVu2zJn/CgoK9Pe//10JCQkymUySpC+++EIRERFKT0/X1KlT9dlnnyk4OFhNmzbV+vXrL+6LukQeC90pKSn661//qtWrVzu/gLKsXLlSDRs2lNFolMVi0axZszRy5Mhy7wM3mUznPR8AAACqj4KCApeldr777jsdPHhQISEhGjVqlMaNG6cuXbp4sUIAFWEwGPTMM89o7dq1atasmTZu3Kjbb79dBw4c0JQpU/Tcc8+pW7dukhyjWSTHUluPP/64/vvf/6pDhw7av3+/4uLi1Lp1a3Xt2rVC77t48WK1b99ekrRgwQJNnz5db7/9tiRpypQpOn36tHbs2KH69Yt/cVF0Mffuu+/WsmXL5OPj9qnMKsUjoTszM1O33XabFi5ceMG/RBs1auR8HhwcrMWLFys4OFgZGRkKCQnxRHkAAADwsL179zqX+tqwYYOuvPJKSdLDDz+s2267TcOGDWOdYkCOId6ZMzK99t6VMWnSJDVr1kyS1KtXLwUHB2vv3r0yGAzOoC3JJeSOGTNGHTp0kCS1adNG999/v954440Khe6WLVu6vB4yZIiWL18uSTp48KBWr16t/fv3l1qeesWKFWrevLmmTJlSqc/nKW4P3QUFBbrjjjt066236o477rio4yV5/bcRAAAAqJzs7Gz9+9//1iuvvKKvv/7auf3tt992zslz3XXX6brrrvNWiUC1YzAYKjzE29uKVqkqEhYWpjNnzujZZ5/VuHHj1K1bN82aNcvl9t/OnTu7HHP11Vdry5YtFXo/q9WqZ599VmvXrtWxY8dkt9tltVolScnJyerUqVOpwC1JW7ZsUe/evSv78TzG7cl26tSpCgwMrPBkZ7/88ovzeUZGhiZMmKBBgwYpKCjI3aUBAADAA06ePKkJEyY419H++uuv5ePjo1tuuUX/+c9/9Nhjj3m7RABuUNaka3a7XZ06dVJycrJ69Oihbt266csvv3Tuz83NdWmfnZ1dZlAuyz333KPk5GQ9//zz2rVrl1atWuXcZzabXZYZLOl8+7zBraH7zJkzWrJkiX744QddddVViomJUUxMjK666iodO3ZMeXl5uvXWW3XkyBHnMQ8++KDatGmjmJgYXX/99WrRooVee+01d5YFAAAAN/r9999drmTXr19f7777riwWi1q1aqUnn3xSKSkp+uCDD3TbbbfJz8/Pi9UCqAo+Pj6666679M9//lNPP/20c/uPP/7o0u7777933qN9IatXr9bSpUvVoUMHGQwG7dy507mvY8eOSk5OVkZGRqnjunTpUu4cYd7g1uHlDRo0kN1uP2+bDz744LyvAQAAUL1kZmYqKSlJn332mT799FPt27dPzZs318GDB2UwGBQQEKBnnnlGUVFRiouL4zZBoI45ceKEGjVqJLvdrm3btjnv+5ak1157TSNGjFCHDh30448/6q233lJycnKFztu0aVP9+OOP6tevn9LS0vT8888790VFRWnIkCEaO3as3njjDZeJ1G6//XbNnz9ff/vb3zRt2jSv/53k8XW6AQAAUDO9+uqrevPNN7V582bl5eU5txuNRkVFRSkjI0OhoaGSpLvuustLVQKoCv7+/vL393fZZjKZ5O/vr379+sliscjPz08xMTF6+eWXnW1mzZqlCRMm6MiRIzIYDFqxYkWFl3x+8803NXHiRNlsNgUGBuqf//yny981L774oubOnavo6GjVr19feXl5SkpKUlRUlL766is98MADioiIUIMGDdSwYUNt3LjRPV9GJRnsF7o0Xc1ZLBaFhIQoIyNDwcHB3i4HAAD6Jjfj+6waaWlp+vzzz3XHHXc4l2edPHmylixZIskxi/CAAQN044036oYbbmCVGeAiWa1WHThwQC1btlRAQIC3y/GoMWPGaMyYMerTp4+3S7ko5/uzqkzfxJVuAACAOignJ0cbNmxwDhn/+eefJUmXX365+vbtK0kaPXq0rrzySg0YMEBXXHFFmZMoAUB5jEZjuXM6bN++XaNGjSpzn8Fg0OrVq9W6dWtPlldlCN0AAAB1yHfffafHHntMGzZskM1mc243GAzq2rWryzDybt26qVu3bt4oE0At8Morr5S7r2PHji4To9VmhG4AAIBa6vjx41q3bp1atmyp7t27S5L8/Pz0+eefS5IiIyN14403asCAAfrDH/6gyy67zJvlAkCtROgGAACoJWw2mzZv3uwcMl60VM/dd9/tDN0dO3bUc889pxtuuEHR0dEMGQcADyN0AwAA1HC5ubkaOnSokpKSlJWV5bKvU6dOLmvi+vj46P7776/qEgGgziJ0AwAA1CBnzpzRF198odTUVD344IOSHEv5pKSkKCsrS02aNNGNN96oG2+8Uf3791eTJk28XDEA1G2EbgAAgGosPz9f3377rXPIeHJysgoLCxUQEKDx48fLbDZLkpYsWaIGDRro6quvZsg4AFQjhG4AAIBqav78+Vq0aJEsFovL9vbt22vAgAHKzs52hu6aug4uANR2hG4AAAAvys3N1Q8//KANGzbo66+/1vPPP6/LL79cklSvXj1ZLBY1bNhQ/fv314ABA9S/f39FRkZ6uWoAQEURugEAAKpQdna2tmzZ4gzZ33zzjXJycpz7R4wYoTvvvFOS9Oc//1lxcXHq3LmzjEajt0oGgEpJS0tT79699dtvv3m7lGqB0A0AAOBBp0+fVmFhocLCwiRJa9eu1fDhw13ahIWFqXfv3rr++uvVs2dP5/bw8HCFh4dXab0AvOPclQdKMhqNCggIqFBbHx8f520n52sbGBh4EVVWTF5ennJzcz12/pqG0A0AAOBGhw8f1tdff+28kr1jxw498cQTmj17tiSpV69eioqK0vXXX+8M2qyXDaB+/frl7rv55pv10UcfOV83btxY2dnZZbaNi4tTUlKS83WLFi108uTJUu3sdvvFF4tK8fF2AQAAADXdmTNnNHbsWLVu3VoREREaOXKkXnjhBe3YsUOSXIZYNmnSRCkpKUpMTNRf//pXtWvXjsANoNrr06eP3njjDXXs2FHR0dEaMmSIMjIyNH36dLVt21bt27fXq6++6vZjy3LgwAENGDBA0dHR6tChgwYMGKDU1FTn/oKCAsXHx6tly5aKjo5WdHS00tLSJEnp6ekaO3asIiMj1b59e91www2X9sVUAFe6AQAAKqiwsFA///yzNmzYID8/P917772SpKCgIP373/9WZmamfHx81LFjR+eV7F69erFWNoALyszMLHffuXM6HD9+vNy2Pj6u11UPHjx4SXWV9MILLygpKUkNGjTQk08+qX79+unmm2/W3r17lZWVpd69e6tv375l/iKxose2bNmyQrUsXrxY7du3lyQtWLBA06dP19tvvy1JmjJlik6fPq0dO3a4jCAoKChQ3759dffdd2vZsmWlvitPIXQDAACUIy8vT1u3bnUOFd+4caPOnDkjSWrTpo0zdPv6+urZZ59VeHi4evTooZCQEG+WDaAGqsw91p5qeyETJ05UgwYNJElDhw7VM888o8cee8z5Pv369dN3332n2NjYiz62IqH73DZDhgzR8uXLJTl+ybB69Wrt37/f5d52SVqxYoWaN2+uKVOmVO6DXyJCNwAAwP/k5eXJz8/P+fq6667T1q1bXdoEBgaqR48euv7661VYWOi8UjJ27NgqrRUAqlrTpk2dz81ms9q0aePyd2a9evVcVmNw17HnslqtevbZZ7V27VodO3ZMdrtdVqtVkpScnKxOnTqVCtyStGXLFvXu3btC7+FOhO4SfvhBOn1aMholHx/Ho6znF7P/fMcYDI4HAACoWunp6dq0aZPzSvauXbt0/Phx+fv7S5K6du2qgwcPqnfv3s5Jzzp37ixfX/4JBQBFf1dW9bH33HOPcnJy9Pzzz6t9+/batWuX/vjHP0pyBPr8/PwyjzvfPk+ixyhh+nRp3TrvvHdRCK9MaC8K60WBvTI/vXVMWe2q+zZv/3T3Me5oX5fO5Yn/Rjx1bE3Ydq7y9rlre2WPiYqSrr22/HOhdti4caNWrlypDRs26Keffio1g++2bdt07f/+Q1i0aJFefPHFKrvvDwBwYatXr9bvv/+uyy67TJK0c+dO576OHTsqOTlZGRkZpW716dKli5YtW6bp06dXab2E7hJatZKuvloqLHQ8Cgpcf17MtorOxF90HADAe0aOlFas8HYV8LSvv/5a//rXv5yv27Rp47J8V4sWLZz7goKCvFAhAOB8mjZtqh9//FH9+vVTWlqann/+eee+qKgoDRkyRGPHjtUbb7zhMpHa7bffrvnz5+tvf/ubpk2bxkRq3rB0qfvPabe7N8SX3FYy1HvzZ0Xb1sRt3v7p6WO93aa6b/PEfyOeOrYmbCuprO3VoW3btmW3Re1y00036ciRI84h4yXvMwQAlM1kMslkMjlf+/n5lRoiXrTNz8/PpW1ljq2IN998UxMnTpTNZlNgYKD++c9/6q677nLuf/HFFzV37lxFR0erfv36ysvLU1JSkqKiovTVV1/pgQceUEREhBo0aKCGDRtq48aNlfouKstgr+GrolssFoWEhCgjI0PBwcHeLgcAgDrTNyUkJGjJkiWy2+1q3ry5li1bpoiIiHLbZ2dna/To0crIyNAXX3xR4fepK98ngLrBarXqwIEDatmypQICArxdDs7jfH9WlembuNINAAAqbe3atVq6dKk2btyo0NBQJSYmavDgwUpOTi6z/dGjRzV48GC1adPmvOvLAgDqtu3bt2vUqFFl7jMYDFq9erVat25dxVVdGkI3AACotISEBM2fP1+hoaGSpNGjR+u5557T1q1b1aVLl1LtT506pQULFsjX19e5LisAAOfq2LGjy8RotQFTcQIAgEpbv3694uLiXLb16dNH68pZBqRDhw7q169fhc5ts9lksVhcHgBQ29Twu3zrBHf9GXkkdCckJCgmJkYdOnTQwIEDlZaWVm5bi8WiUaNGqV27doqOjtbjjz/Of4AAAFRjmZmZMhqNCgwMdNkeFRWlAwcOXPL54+PjFRIS4nxERUVd8jkBoLrw8/OT5JjnAtVb0Z9R0Z/ZxXL78PLK3uN1zz336KqrrtLbb78tm82mYcOG6fnnn9cDDzzg7tIAAIAbpKeny2w2l9puNpvd8o/IGTNmaOrUqc7XFouF4A2g1jAajQoNDXXOb1GvXj0ZDAYvV4WS7Ha7srOzdfz4cYWGhspoNF7S+dweuitzj9fp06e1efNmLV++XJJjKvlFixbp9ttvJ3QDAFBNmUwmWa3WUtutVmuZYfxizl9yaRkAqG2KlipkYsnqLTQ01C3LSro9dK9fv16JiYku24ru8To3dCclJal79+4uvzmIjo7W8ePHdezYMTVp0qTU+W02m2w2m/M193kBAFC1wsLClJOTo6ysLJch5qmpqYqMjPRiZQBQMxgMBoWHh6tx48bKy8vzdjkog5+f3yVf4S7i1tB9vnu8ypqB7vDhw2V2zlFRUTp48GCZoTs+Pl7z5s1zX9EAAKBSDAaDYmNjtWHDBg0cONC5PSkpSQsWLPBiZQBQsxiNRrcFO1Rfbp1IrbL3eF3MPWEzZsxQRkaG85GamnrphQMAgEqZNGmSZs+erYyMDEnS8uXLlZmZqb59+3q5MgAAqhe3Xumu7D1eJpNJZ86cqXD7omO4zwsAAO8aMmSIUlJSFBsbK4PBoIiICK1Zs0Y+Pj7Ky8vT0KFDlZCQoPDwcJfj/P395e/v76WqAQCoem4N3ZW9xysyMrLMWc25JwwAgOpv8uTJmjx5cqntfn5++uCDD8o8pkePHlq/fr2nSwMAoNpwa+iu7D1e3bt318MPP6yCggLnvQx79+6Vn59fhUN30ZreTKgGAKguivqkoj4Kl4a+HgBQ3VSmr3f77OVF93j16NFDISEh573Hq0WLFuratasWLlyoWbNmyWazadq0aZo4cWKF3+/s2bOSxPqdAIBq5+zZswoJCfF2GTUefT0AoLqqSF9vsHvg1/DPPvusXnzxRec9Xi+//LJatmxZ5j1ep0+f1vjx47V9+3YVFhZq6NChio+Pl49PxeZ4Kyws1OHDhxUUFHTJi8pbLBZFRUUpNTVVwcHBl3QuFON7dT++U8/ge3W/uvqd2u12nT17Vs2aNatwf4byubOvl+ruf5eexHfqfnynnsH36n519TutTF/vkdBdU1ksFoWEhCgjI6NO/QfjaXyv7sd36hl8r+7Hd4rqiP8u3Y/v1P34Tj2D79X9+E4vjF+/AwAAAADgIYRuAAAAAAA8hNBdgslk0ty5c1kH3M34Xt2P79Qz+F7dj+8U1RH/Xbof36n78Z16Bt+r+/GdXhj3dAMAAAAA4CFc6QYAAAAAwEMI3QAAAAAAeAihGwAAAAAADyF0l5CQkKCYmBh16NBBAwcOVFpamrdLqtE+/vhj3XDDDWrfvr3at2+v+++/Xzk5Od4uq9b45ZdfZDabNW/ePG+XUuPl5ORo7ty56tSpk6666iq1bdtWX375pbfLqtGys7M1adIkxcTEKCYmRj179uQ7RbVAX+9+9PeeRX/vPvT37kd/XzGE7v9Zu3atli5dqo0bN+rnn3/WqFGjNHjwYG+XVaOZzWa9/vrr2rVrl7Zv365Tp05pzpw53i6r1pg8ebL69u2rvLw8b5dSo+Xn52vgwIGy2+3asmWLduzYoT179qhnz57eLq1Gu+OOO9SoUSNt375dO3fu1OLFizVq1CilpqZ6uzTUYfT1nkF/71n09+5Bf+8Z9PcVQ+j+n4SEBM2fP1+hoaGSpNGjR8toNGrr1q3eLawG69u3r5o3by5J8vPz06OPPqrPPvvMy1XVDv/5z38UFhama6+91tul1HhvvfWWQkJC9MQTT8hsNkuSDAaD/P39vVxZzbZ27VpNmjRJRqNRknTttdeqS5cuSk5O9nJlqMvo6z2D/t5z6O/dh/7eM+jvK4bQ/T/r169XXFycy7Y+ffpo3bp1Xqqo9jlz5oyCg4O9XUaNl5OTozlz5mjhwoXeLqVWePfdd/XXv/7V22XUOrGxsXruueecr7/55htt2bKFfzjCq+jrqwb9vXvQ37sX/b1n0N9XjK+3C6gOMjMzZTQaFRgY6LI9KipKO3fu9FJVtc9LL72kESNGeLuMGi8+Pl5//vOfFR4e7u1SaoVt27bJbDZr2LBh2r9/v8LCwvTII4/opptu8nZpNdobb7yhgQMH6ttvv1WHDh306quv6q233lJkZKS3S0MdRV9fdejv3YP+3r3o7z2D/r5iCN2S0tPTncNMSjKbzcrOzvZCRbXPJ598ou3bt+utt97ydik12m+//aZVq1YxFNKNTp06pQULFuj5559XdHS0duzYoVtuuUVvvPGG+vTp4+3yaqwWLVrovvvu00MPPaQPP/xQI0eOVLdu3bxdFuow+vqqQX/vHvT37kd/7xn09xXD8HJJJpNJVqu11Har1VpmB43KSUlJ0V//+letWLFCJpPJ2+XUaJMmTdKCBQv4Ht3Ix8dHjz76qKKjoyVJV111laZOnapXX33Vy5XVbKNHj9bKlSu1efNmHT58WMHBwbr66quZKRpeQ1/vefT37kN/7370955Bf18xhG5JYWFhysnJUVZWlsv21NRUhkZcoszMTN12221auHChunTp4u1yarRPPvlEOTk5GjJkiLdLqVUaN26sNm3auGxr1aqVTpw44aWKar5ff/1Va9eu1SeffKJrr71WTZs21UsvvaSbb75Zzz//vLfLQx1FX+9Z9PfuQ3/vGfT37kd/X3GEbjlmLoyNjdWGDRtcticlJal79+5eqqrmKygo0B133KFbb71Vd9xxh7fLqfEOHjyoX3/9VdHR0c7Hc889p5deekkxMTEMj7xI3bp10/bt21227d27V61bt/ZSRTVfenq6mjZtqqCgIJft7dq10+nTp71UFeo6+nrPob93L/p7z6C/dz/6+0qww2632+3vv/++/ZprrrGnp6fb7Xa7/e2337bHxMTYCwoKvFxZzTVp0iT7iBEj7IWFhd4updaaO3eufdasWd4uo0b7/PPP7dHR0fZDhw7Z7Xa7fceOHfbmzZvbd+/e7eXKaq78/Hx7165d7U899ZQ9Ly/Pbrfb7Xv37rW3bt3anpSU5OXqUJfR13sG/b3n0d9fOvp796O/rzgmUvufIUOGKCUlRbGxsTIYDIqIiNCaNWvk48NggItx5swZLVmyRK1bt9ZVV13l3G4wGLRu3To1adLEi9XVHn5+fjIYDN4uo0br16+fHnroIecyQsHBwVq6dKnzni9UntFo1EcffaSZM2fqqquukq+vr4KCgvTPf/6z1HJNQFWir3c/+vuqQX9/6ejv3Y/+vuIMdrvd7u0iAAAAAACojfjVLgAAAAAAHkLoBgAAAADAQwjdAAAAAAB4CKEbAAAAAAAPIXQDAAAAAOAhhG4AAAAAADyE0A0AAAAAgIcQuoFa4N5771VERIRiYmKcj/vuu8/j75uWlqZWrVp5/H0AAKjr6OuBmsvX2wUAuHS5ubmaN2+e/vKXv1Tp++bl5Sk3N7dK3xMAgLqIvh6oubjSDQAAAACAhxC6gVquS5cu+s9//qOOHTsqOjpa11xzjX766SeXNp999pmuueYaXXHFFWrVqpUee+wxFRQUOPcXFBQoPj5eLVu2VHR0tKKjo5WWliZJys/P18SJE3XllVeqbdu2Gjx4sE6fPl2lnxEAgLqMvh6o3gjdQC2XmZmpf/3rX9qwYYP27Nmj2bNna8CAAbJarZKkn376Sf/3f/+nF198Ub/++qt27Nihn3/+WbNnz3aeY8qUKdq5c6d27NihPXv2aM+ePYqIiJAkHTt2TE2aNNHevXu1d+9eNWrUSE899ZRXPisAAHURfT1QvRnsdrvd20UAuDRjxozR+vXr1bBhQ+e28ePHa/z48WrRooUSExPVq1cv577+/ftr/PjxGjZsmO666y517txZDz74oHP/sWPHnL/hPn78uHr16qX9+/fLbDa7vO/BgwfVoUMHnT17Vj4+jt/hbdy4UQ899JC+/fZbD39qAADqDvp6oOZiIjWglpgzZ065k6t07tzZ5fXVV1+tAwcOSJJ27NihiRMnuuxv0qSJmjVrpl9++UV79+5Vp06dSnXCRRo0aODshIuOPXHixKV8FAAAUAb6eqBmYng5UAecO+todna2s2M1GAxlHmO322U0GmU2m5Wfn1/h9zIYDCosLLz4YgEAQKXR1wPVF6EbqAN+/PFHl9fff/+92rdvL0nq2LGjvv76a5f9x44d07Fjx9S6dWt17NhRycnJysjIqLJ6AQBA5dDXA9UXoRuoA+bPn+/sSF977TVZrVb16dNHkvTQQw9p8eLF+u677yQ5JmO59957NXHiRJlMJkVFRWnIkCEaO3asMjMzvfURAADAedDXA9UX93QDtYCvr6/mzp2rZ555xrktIiJCn376qSTpwQcfVO/evWWxWBQVFaWPP/7YOdQsJiZGq1ev1v33369Tp07Jbrfr/vvv19SpU53nevHFFzV37lxFR0erfv36ysvLU1JSkvz8/GQymVxq8ff3L7UNAABcGvp6oOZi9nKglmvRooUOHjzo7TIAAICH0NcD1RvDy4FaLiAgwNslAAAAD6KvB6o3rnQDAAAAAOAhXOkGAAAAAMBDCN0AAAAAAHgIoRsAAAAAAA8hdAMAAAAA4CGEbgAAAAAAPITQDQAAAACAhxC6AQAAAADwEEI3AAAAAAAe8v9w6u8ZyXW58QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training result\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['nsp_loss'], 'b-', label='nsp_loss')\n",
    "plt.plot(history.history['mlm_loss'], 'r--', label='mlm_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['nsp_acc'], 'g-', label='nsp_acc')\n",
    "plt.plot(history.history['mlm_lm_acc'], 'k--', label='mlm_acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🧐 mlm loss는 epoch가 진행 될 수록 잘 감소하는 것으로 보이는데, nsp loss는 0.65~0.58로 그다지 변화가 없어보였다. \n",
    "accuracy는 둘 다 상승했다.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 회고"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- np.memmap이라는 함수를 처음 써봤다. 대용량 데이타를 로딩 시키기 위해서 유용하게 사용할 수 있겠다. \n",
    "- 파라미터가 맞지 않아 에러가 나서 좀 고생을 했지만, 금방 해결했다. \n",
    "- Transformer기반의 BERT를 익히기에 좋은 예제였다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

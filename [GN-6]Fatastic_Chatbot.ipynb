{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [GN-6]Fantastic Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ë²ˆ í”„ë¡œì íŠ¸ëŠ” Transformerë¥¼ ì´ìš©í•˜ì—¬ Chatbotì„ ë§Œë“œëŠ” ê²ƒì´ë‹¤.    \n",
    "- ë°ì´íƒ€ ì¦ê°•(Agumented)ë¥¼ í†µí•´ì„œ ë¶€ì¡±í•œ ë°ì´íƒ€ë¥¼ ì¶”ê°€\n",
    "- BLEU Scoreë¥¼ ê³„ì‚°í•´ ë³´ê¸°\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.21.4\n",
      "1.3.3\n",
      "2.6.0\n",
      "3.6.5\n",
      "3.8.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "import gensim\n",
    "import re\n",
    "from konlpy.tag import Mecab\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "print(np.__version__)\n",
    "print(pd.__version__)\n",
    "print(tf.__version__)\n",
    "print(nltk.__version__)\n",
    "print(gensim.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. ë°ì´í„° ë‹¤ìš´ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>questions</th>\n",
       "      <th>answers</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12ì‹œ ë•¡!</td>\n",
       "      <td>í•˜ë£¨ê°€ ë˜ ê°€ë„¤ìš”.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1ì§€ë§ í•™êµ ë–¨ì–´ì¡Œì–´</td>\n",
       "      <td>ìœ„ë¡œí•´ ë“œë¦½ë‹ˆë‹¤.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3ë°•4ì¼ ë†€ëŸ¬ê°€ê³  ì‹¶ë‹¤</td>\n",
       "      <td>ì—¬í–‰ì€ ì–¸ì œë‚˜ ì¢‹ì£ .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3ë°•4ì¼ ì •ë„ ë†€ëŸ¬ê°€ê³  ì‹¶ë‹¤</td>\n",
       "      <td>ì—¬í–‰ì€ ì–¸ì œë‚˜ ì¢‹ì£ .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL ì‹¬í•˜ë„¤</td>\n",
       "      <td>ëˆˆì‚´ì´ ì°Œí‘¸ë ¤ì§€ì£ .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         questions      answers  label\n",
       "0           12ì‹œ ë•¡!   í•˜ë£¨ê°€ ë˜ ê°€ë„¤ìš”.      0\n",
       "1      1ì§€ë§ í•™êµ ë–¨ì–´ì¡Œì–´    ìœ„ë¡œí•´ ë“œë¦½ë‹ˆë‹¤.      0\n",
       "2     3ë°•4ì¼ ë†€ëŸ¬ê°€ê³  ì‹¶ë‹¤  ì—¬í–‰ì€ ì–¸ì œë‚˜ ì¢‹ì£ .      0\n",
       "3  3ë°•4ì¼ ì •ë„ ë†€ëŸ¬ê°€ê³  ì‹¶ë‹¤  ì—¬í–‰ì€ ì–¸ì œë‚˜ ì¢‹ì£ .      0\n",
       "4          PPL ì‹¬í•˜ë„¤   ëˆˆì‚´ì´ ì°Œí‘¸ë ¤ì§€ì£ .      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'transformer/ChatbotData.csv'\n",
    "\n",
    "df = pd.read_csv(file_path, names=['questions','answers', 'label'], skiprows=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. ë°ì´í„° ì •ì œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    \n",
    "    sentence = sentence.lower().strip()\n",
    "    #sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    #sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^ê°€-í£ã„±-ã…ã…-ã…£a-zA-Z0-9]+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ§ ê¸°ë³¸ì ìœ¼ë¡œ Tokenizerì—ì„œ ê¸°ëŠ¥ìœ¼ë¡œ í•´ì£¼ê¸°ì— íŠ¹ìˆ˜ë¬¸ìì™€ ê³µë°±ì²˜ë¦¬ëŠ” ì œì™¸í•¨. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. ë°ì´í„° í† í°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11823"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df) #11823"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab=Mecab()\n",
    "\n",
    "max_len = 50\n",
    "\n",
    "que_corpus = []\n",
    "ans_corpus = []\n",
    "\n",
    "def build_corpus(df, max_len=50):\n",
    "    que = []\n",
    "    ans = []\n",
    "\n",
    "    unique_question = set()\n",
    "    unique_answer = set()\n",
    " \n",
    "    for row in df.iterrows():\n",
    "        question = row[1]['questions']\n",
    "        answer = row[1]['answers']\n",
    "\n",
    "        que_temp = preprocess_sentence(question)\n",
    "        que_temp = tuple(mecab.morphs(que_temp))\n",
    "        ans_temp = preprocess_sentence(answer)\n",
    "        ans_temp = tuple(mecab.morphs(ans_temp))\n",
    "\n",
    "        if que_temp not in unique_question:\n",
    "            if ans_temp not in unique_answer:\n",
    "                if len(que_temp) <= max_len and len(ans_temp) <= max_len:\n",
    "                    unique_question.add(que_temp)\n",
    "                    unique_answer.add(ans_temp)\n",
    "                    que.append(list(que_temp))\n",
    "                    ans.append(list(ans_temp))\n",
    "\n",
    "    return que, ans \n",
    "\n",
    "que_corpus, ans_corpus = build_corpus(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ§ preprocess_sentenceë¥¼ í†µí•´ ì „ì²˜ë¦¬ë¥¼ ì§„í–‰í•˜ê³ , Mecabì„ í†µí•´ì„œ í˜•íƒœì†Œ ë¶„ì„ í–ˆë‹¤. \n",
    "Questionê³¼ Answerì—ì„œ ì¤‘ë³µê°’ì„ ê±¸ëŸ¬ë‚´ê³ , ë¬¸ì¥ê¸¸ì´ë„ max_len = 50ìœ¼ë¡œ ì„¤ì •í•´ì„œ ë¶„ë¥˜í•´ ëƒˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ê°€ë‚œ', 'í•œ', 'ì', 'ì˜', 'ì„¤ì›€'],\n",
       " ['ê°€ë§Œ', 'ìˆ', 'ì–´ë„', 'ë•€', 'ë‚œë‹¤'],\n",
       " ['ê°€ìƒ', 'í™”í', 'ì«„ë”±', 'ë§í•¨'],\n",
       " ['ê°€ìŠ¤', 'ë¶ˆ', 'ì¼œ', 'ê³ ', 'ë‚˜ê°”', 'ì–´'],\n",
       " ['ê°€ìŠ¤', 'ë¹„', 'ë„ˆë¬´', 'ë§ì´', 'ë‚˜ì™”', 'ë‹¤'],\n",
       " ['ê°€ìŠ¤', 'ë¹„', 'ë¹„ì‹¼ë°', 'ê°ê¸°', 'ê±¸ë¦¬', 'ê² ', 'ì–´'],\n",
       " ['ê°€ì¥', 'í™•ì‹¤', 'í•œ', 'ê±´', 'ë­˜ê¹Œ'],\n",
       " ['ê°€ì¡±', 'ì—¬í–‰', 'ê°€', 'ê¸°', 'ë¡œ', 'í–ˆ', 'ì–´'],\n",
       " ['ê°€ì¡±', 'ìˆ', 'ì–´'],\n",
       " ['ê°€ì¡±', 'ë¼ë¦¬', 'ì—¬í–‰', 'ê°„ë‹¤']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "que_corpus[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7640, 7640)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(que_corpus), len(ans_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n"
     ]
    }
   ],
   "source": [
    "total_sentence_count = len(que_corpus)\n",
    "test_sentence_count = int(total_sentence_count*0.01)\n",
    "print(test_sentence_count)# 100 ê°œë°˜ í…ŒìŠ¤íŠ¸ ë°ì´íƒ€ë¡œ ë‚¨ê²¨ë‘ì\n",
    "test_sentence_count = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test , trans ë¶„ë¦¬\n",
    "test_que_corpus = que_corpus[-test_sentence_count:]\n",
    "que_corpus = que_corpus[:-test_sentence_count]\n",
    "\n",
    "test_ans_corpus = ans_corpus[-test_sentence_count:]\n",
    "ans_corpus = ans_corpus[:-test_sentence_count]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100, 7540, 7540)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_que_corpus),len(test_ans_corpus),len(que_corpus),len(ans_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ§ Test ë°ì´íƒ€ë¡œ ì•½ 5%ì •ë„ì˜ ë°ì´íƒ€ì¸ 100ê°œë¥¼ ë¶„ë¥˜í•´ ë‘ê³ , BLEU Score ê³„ì‚°ì— ì´ìš©í•  ì˜ˆì •ì´ë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://yeon22.tistory.com/158   \n",
    "pip install --upgrade gensim==3.8.3\n",
    "\n",
    "Gensim  ë²„ì „ì´ 4.1.2ë²„ì „ì´ì—ˆëŠ”ë°, ko.binì— ë§Œë“¤ì–´ì§„ ë²„ì „ê³¼ ë§ì§€ ì•Šì•„ downgrade gensim==3.8.3ë¡œ ì§„í–‰í–ˆë‹¤. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "wv=Word2Vec.load('transformer/ko.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('í•­í•´', 0.6469576358795166),\n",
       " ('íƒì‚¬', 0.6048390865325928),\n",
       " ('ì—¬í–‰', 0.5845088362693787),\n",
       " ('ì •ë³µ', 0.5619137287139893),\n",
       " ('ê°œì²™', 0.5535061955451965),\n",
       " ('ë‚¨ê·¹', 0.5296272039413452),\n",
       " ('íƒí—˜ê°€', 0.5190513134002686),\n",
       " ('íƒí—˜ëŒ€', 0.511405348777771),\n",
       " ('íš¡ë‹¨', 0.4878459572792053),\n",
       " ('ëŒ€ì„œì–‘', 0.4818343222141266)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(\"íƒí—˜\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: ì€\n",
      "To: ì€ë° \n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "sample_sentence = \"ì€\"\n",
    "sample_tokens = sample_sentence.split()\n",
    "\n",
    "selected_tok = random.choice(sample_tokens)\n",
    "\n",
    "result = \"\"\n",
    "for tok in sample_tokens:\n",
    "    if tok is selected_tok:\n",
    "        if wv.most_similar(tok)[0][0] is not None:\n",
    "            result += wv.most_similar(tok)[0][0] + \" \"\n",
    "    else:\n",
    "        result += tok + \" \"\n",
    "\n",
    "print(\"From:\", sample_sentence)\n",
    "print(\"To:\", result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_sub(src_sentence, wv):\n",
    "    sample_tokens = src_sentence\n",
    "\n",
    "    selected_tok = random.choice(sample_tokens)\n",
    "\n",
    "    result = \"\"\n",
    "    \n",
    "    try:\n",
    "        sim_word = wv.most_similar(selected_tok)[0][0]\n",
    "    except:\n",
    "        sim_word = selected_tok\n",
    "\n",
    "    for tok in sample_tokens:\n",
    "        if tok is selected_tok: \n",
    "            result += sim_word + \" \"\n",
    "        else:\n",
    "            result += tok + \" \"\n",
    "\n",
    "    return result.strip().split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ§ ë°ì´íƒ€ ì¦ê°•ì„ ìœ„í•œ ê¸°ë³¸ í•¨ìˆ˜ë¥¼ ì •ì˜í–ˆë‹¤. ë¬¸ì¥ í† í° ì¤‘ì—ì„œ ì„ì˜ë¡œ í•˜ë‚˜ë¥¼ ì„ ì •í•´ most_similarë¥¼ ì°¾ê³  ìˆìœ¼ë©´ ê·¸ ë‹¨ì–´ë¥¼ ì‚¬ìš©í•˜ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ í† í°ì„ ì‚¬ìš©í•˜ë„ë¡ í–ˆë‹¤. ë§Œì¼ ê³µë°±ë¬¸ìë‚˜ '-'ì™€ ê°™ì€ ë¬¸ìë¥¼ ë°˜í™˜í•˜ë©´, ì˜ë¯¸ì—†ëŠ” ë°ì´íƒ€ê°€ ìƒì„±ë˜ê³ , ì•„ì˜ˆ ë¹¼ë©´ Questionê³¼ Answerê³¼ ìŒì´ ë§ì§€ ì•Šê¸°ì— ìœ ì‚¬í•œ ë‹¨ì–´ë¥¼ ì°¾ì§€ ëª»í•˜ë”ë¼ë„ ì—ëŸ¬ë‚˜ ë¹ˆë¬¸ìë¥¼ ë°˜í™˜í•˜ê¸° ë³´ë‹¤, ê¸°ì¡´ ë¬¸ì¥ì„ ë°˜í™˜í•˜ë„ë¡ í–ˆë‹¤. \n",
    "ì´ë ‡ê²Œ í•˜ë©´ ì „ì²´ì ìœ¼ë¡œ ì¤‘ë³µ ë°ì´íƒ€ê°€ ìƒê¸¸ ìˆ˜ ìˆì§€ë§Œ, ë”°ë¡œ ì´í›„ì— ì¤‘ë³µë°ì´íƒ€ ì œê±°ëŠ” ì§„í–‰í•˜ì§€ ì•Šì•˜ë‹¤. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['íƒí—˜', 'ë¥¼', 'ë– ë‚ ', 'ì‹œê°„', 'ì´ë‹¤.']\n"
     ]
    }
   ],
   "source": [
    "temp = lexical_sub(['í•­í•´','ë¥¼', 'ë– ë‚ ', 'ì‹œê°„','ì´ë‹¤.'],wv)\n",
    "\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7540\n"
     ]
    }
   ],
   "source": [
    "# questin Argumented\n",
    "que_a_corpus=[]\n",
    "\n",
    "for idx, sen in enumerate(que_corpus):\n",
    "    new_src = lexical_sub(sen, wv)\n",
    "    que_a_corpus.append(new_src)  \n",
    "    \n",
    "print(len(que_a_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7540\n"
     ]
    }
   ],
   "source": [
    "# answer argumanted \n",
    "ans_a_corpus=[]\n",
    "\n",
    "for idx, sen in enumerate(ans_corpus):\n",
    "    new_src = lexical_sub(sen, wv)\n",
    "    ans_a_corpus.append(new_src)  \n",
    "    \n",
    "print(len(ans_a_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ê°„ì¥', 'ì¹˜í‚¨', 'ì‹œì¼œì•¼', 'ì§€'],\n",
       " ['ê°„ì ‘í¡ì—°', 'ì‹«', 'ì–´'],\n",
       " ['ê°ˆê¹Œ', 'ë§', 'ê¹Œ', 'ê³ ë¯¼', 'ë¼'],\n",
       " ['ê°ê¸°', 'ê°™ì• '],\n",
       " ['ê°ê¸°', 'ê±¸ë¦°', 'ê²ƒ', 'ê°™', 'ì•„'],\n",
       " ['ê°ê¸°', 'ê°€', 'ì˜¤', 'ë ¤ë‚˜'],\n",
       " ['ê°ë¯¸', 'ë¡œìš´', 'ëª©ì†Œë¦¬', 'ì¢‹', 'ì•„'],\n",
       " ['ê°ì •', 'ì´', 'ì“°ë ˆê¸°í†µ', 'ì²˜ëŸ¼', 'ì—‰ë§ì§„ì°½', 'ì´', 'ì•¼'],\n",
       " ['ê°ì •', 'ì»¨íŠ¸ë¡¤', 'ì„', 'ëª»', 'í•˜', 'ê² ', 'ì–´'],\n",
       " ['ê°íˆ', 'ë‚˜', 'ë¥¼', 'ë¬´ì‹œ', 'í•˜', 'ëŠ”', 'ì• ', 'ê°€', 'ìˆ', 'ì–´']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "que_corpus[30:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ê³ ì¶”ì¥', 'ì¹˜í‚¨', 'ì‹œì¼œì•¼', 'ì§€'],\n",
       " ['ê°„ì ‘í¡ì—°', 'í˜ë“¤', 'ì–´'],\n",
       " ['ê°ˆê¹Œ', 'ë§ë¡œ', 'ê¹Œ', 'ê³ ë¯¼', 'ë¼'],\n",
       " ['ë‹¬ë¼ë¶™', 'ê°™ì• '],\n",
       " ['ê°ê¸°', 'ê±¸ë¦°', 'ê²ƒ', 'ë˜‘ê°™', 'ì•„'],\n",
       " ['ê°ê¸°', 'ê°€', 'ì˜¤', 'ì—¬ëŸ¬í•´'],\n",
       " ['ê°ë¯¸', 'ë¡œìš´', 'ëª©ì†Œë¦¬', 'ê´œì°®', 'ì•„'],\n",
       " ['ê°ì •', 'ì´', 'ì“°ë ˆê¸°í†µ', 'ì²˜ëŸ¼', 'ì—‰ë§ì§„ì°½', 'ê·¸ëŸ¬', 'ì•¼'],\n",
       " ['ìš•ë§', 'ì»¨íŠ¸ë¡¤', 'ì„', 'ëª»', 'í•˜', 'ê² ', 'ì–´'],\n",
       " ['ê°íˆ', 'ë‚˜ì˜', 'ë¥¼', 'ë¬´ì‹œ', 'í•˜', 'ëŠ”', 'ì• ', 'ê°€', 'ìˆ', 'ì–´']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "que_a_corpus[30:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ë§›ìˆ', 'ê²Œ', 'ë“œì„¸ìš”'],\n",
       " ['ì €', 'ë„', 'ì‹«', 'ì–´ìš”'],\n",
       " ['ê°€ì„¸', 'ìš”'],\n",
       " ['ë³‘ì›', 'ê°€', 'ì„¸ìš”'],\n",
       " ['ì´ëŸ´', 'ë•Œ', 'ì˜', 'ì‰¬', 'ëŠ”', 'ê²Œ', 'ì¤‘ìš”', 'í•´ìš”'],\n",
       " ['ë”°ëœ»', 'í•˜', 'ê²Œ', 'ê´€ë¦¬', 'í•˜', 'ì„¸ìš”'],\n",
       " ['ì €', 'ë„', 'ë“£', 'ê³ ', 'ì‹¶', 'ë„¤ìš”'],\n",
       " ['ìì‹ ', 'ì„', 'ë”', 'ì‚¬ë‘', 'í•´', 'ì£¼', 'ì„¸ìš”'],\n",
       " ['ê·¸ê±´', 'ìŠµê´€', 'ì´', 'ì—ìš”'],\n",
       " ['ì½•', 'ì§‘', 'ì–´ì„œ', 'ë¬¼', 'ì–´', 'ë³´', 'ì„¸ìš”']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_corpus[30:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ë§›ìˆ', 'ë„ë¡', 'ë“œì„¸ìš”'],\n",
       " ['ì €', 'ë„', 'í˜ë“¤', 'ì–´ìš”'],\n",
       " ['ê²©ë¶„', 'ìš”'],\n",
       " ['ë³‘ì›', 'ë†€ë“œ', 'ì„¸ìš”'],\n",
       " ['ì´ëŸ´', 'ë•Œ', 'ì˜', 'ì‰¬', 'ëŠ”', 'ê²Œ', 'í•µì‹¬ì ', 'í•´ìš”'],\n",
       " ['ë”°ëœ»', 'í•˜', 'ê²Œ', 'ê´€ë¦¬', 'í•˜', 'ã…‚ì‹œì˜¤'],\n",
       " ['ì œ', 'ë„', 'ë“£', 'ê³ ', 'ì‹¶', 'ë„¤ìš”'],\n",
       " ['ìì‹ ', 'ì„', 'ë”', 'ì‚¬ë‘', 'í•´', 'ì£¼ê¸°ë„', 'ì„¸ìš”'],\n",
       " ['ê·¸ê±´', 'ìŠµê´€', 'ê·¸ëŸ¬', 'ì—ìš”'],\n",
       " ['ì½•', 'ì§‘', 'ì–´ì„œ', 'ë¬¼', 'ì–´', 'ì‚´í´ë³´', 'ì„¸ìš”']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_a_corpus[30:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´íƒ€ ê²°í•©\n",
    "\n",
    "question_corpus=[]\n",
    "answer_corpus = []\n",
    "\n",
    "question_corpus = que_corpus + que_a_corpus + que_corpus + que_a_corpus\n",
    "answer_corpus = ans_corpus + ans_corpus + ans_a_corpus + ans_a_corpus\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAekAAABpCAYAAAAeCdv7AAAgAElEQVR4nO2dXUxU1/rwf7753/zVi2nltE4YZVCPTk/hAuWtMJVjUIk2GQ9EcsBjvRATeMkJrdYPLoyxU9N4QcVqJSdGEvWitoKx0eMkLVEkHjgDNtS5EE+pxTLAkOkHtlwo57Lvxd4zs/fM3nv2HmAYdP0SEmev7/Vs17PX86yPBb///vvvCAQCgUAgyDj+z1xXQCAQCAQCgTZCSQsEAoFAkKEIJS0QCAQCQYYilLRAIBAIBBmKUNICgUAgEGQoQkkLBAKBQJCh/I9R4IIFC9JVD4FAIBAIXkiMdkIbKmmALQd6ZrQyAoFAn9unNrD/2ta5roZAIEgTpys7DMOFuVsgEAgEggxFKGmBQCAQCDIUoaRng2ATtz9vZ2qu6yEQzFcCDzl9ZITJua6HQDDHJPVJC7To49tThxhXPMne0cNrzmTpQox+vpNnxTpxg03c7nXi/lsVC2eusgJBBjJBZ+U3PFA8yT+6lc0FydJNETjSza9/1YkbeMjpq4vZcyIH28xVViCYM+a9kn5yawMBTrKlrChNJUoKmh09bHEqn22gv/QKhQWONNVDIJivSAqao1vZX6B81kFbTQnVHvGJKhBEmF9KOhNmmsF/MW5/F7dqJlxETmke/okQkFxJj3+xQTULV2F/d/p1FAgymcBPPFjjYo9qJpzFuhobl0JTYOJ/94MPO1SzcBVrXNOvo0CQIcwvJa3BkrIetqSzQOefyf7iECPBKoXJuo+RrgFspeZm0bqm8WATt3tnqqICQYZS8Cr5H37DN4Echcl6gm8uTmKvMff5rWsaDzzk9NWZqqhAMPdYVtJPbm0gEP2E/QsFOyAQnd1q+1ynAvX4J/aoTNJTgXr8XQPyrzxW7z3H8qgTSe3zzd7Rwyvfx8r1n/oE8iUTd/K8icZVtmE46wp/4sNYPPu7JmfoRbx24CTfntrAbcXT7B09FCb1SUvM1Ew6PbIwINjE7S/+GSv/QCNLNPMkTgaRul1hUe9OHoXzWL33KHy5k59dRnIx26bE9yf5eoFMRfLB3v0u8ttBxbXXiTQneK6De44StvKASxflZVZrXHE+WbX/N//oVlbek9LFTMsxE3RE+U367nEptJL99Vmx35EysLGxZT0F9lgdf/1rCS9f7ebud8owLbLYfG0dnZUdnFY8zT+6leqkPmmJmZlJz07fJvWrBx5y+sOQZpnqPgbK1kX7X7uf8+FsN4/cRvXU9uPHyzeltghmHUtKeipQT2DiXdwH5EFzsp3+C59YNtFOBerxD27CfeCclE+widsXmlh0oJElGj7fJ4F2/reshy1/TG7ulpTDCgoOnJMVhjSw376lVtSTXTv5T+kVthxwROP8J+A26VMu4rUDPbxmqdUADpb/rYfl0d99fHvqX7yiUG5mSY8skqTrWkHBgR45Xh+jgRBLChymZfCs9zMWvdXDFpsch3i5SB8i/s+x4OLQfn+mnPNzMd6kbxje2cp+WeEFz3Vw/dyrioEVwhe76agpYf+1hUQG5A7fH2QFnOj/DfpGsK13EL76C5MeeSAP/MTEGhuMT0GB1FOToUny1ysUtH8pe66tl+M/5HTDQ15SKJiJq8O8rKirMVlsvraVzZZ7ZCEFJ7YS0x0TdFb+xEpFPcwyW307WaC/aE1SwoupuLZVru8EAd8UTs9CRdh6OUwq7/S5dao6qft5ikBCPaW2XDqChQV01tsiSA8WlHQfI12weq9isLNV8afSO/gHrRSpkY9zF6vtO/k52MgSJ0AeixRvxpKCKot5KxWMg+VvvcvPFy4x+n+LYjPE/JMKhewgy5XHo0E/UwXJPgAGdEJj+W75o8nqpkw6ZWGUTtnPRSwv0AvTkYFrV+KMXSUXWFJ2kuxTl5iYrDI3uwdSf38yD5vndZSTGed6B1x9xiRZscGzbJ1iRryQXLeNu36FAsbGy0sVeXhyIDyC/bunTAI2IHjvKav/upRHUcU9weNbNl4uB8kUDRtbFAN2QS4b13TzOPA6zkgF3bkGs2eJhJmiFmXr2L/eOMpMMGt9q0ukH5UfFFkUePTCFlLwjotHDY8JlGfF+larn1X1BGf9OvIrHzMczkkqkxhW2iJIF+aV9OQoz1jBK3ED5cKXVlgrcXKUZwwwfmEDj+KCbL+FwFlETukl/Bc28Cjf4qptnTpic/OK/ROeRUYkwJalnjGbacfCgnNsUf6v1lvIFvxXXMrELVtKxk/9U+Op2nysIm2y0LEq6PWzUZiGDBa9lJh/vFzAwSL7gCqdMdN4fzKUBMUWZ861O9SflbbsxYpfWayrecylhg7uKk2n9j+wes2grGSn+G10MS/V/4HVV4clxR34iQdrlrLHDoSfMcEkDxo6uBtXN/v4FBFNl5Wd3FZh86xnv0fxQG/LVOCnuJSJW7aUPKgMaTxVm5K1mJW+1SP8jAkWs1JLaeqFyXL69UdADtPq5/h6wkJeXjOpSmeMxbYI0sYcLRwzUEDElOGTWxu4fYoEn3JGovLPEmd2TtU8ng6MZaHJZDCNh0yEeBa2lmJevj+ayIqpbB37r8mDZgoLoyKKMXiug9OVRP2cuW4bj8anYOkvPFr+KtUshOUhHgdexzb+FLs7V6E4jRTeDB3bo/LVEqcwUzWP6zG7favJj0+x+CpPgyl+/S55LCWW2iJIGxZPHPtBmtEoePJ94izw2W/qr9r/TihMxLblLNLIR4slZT1s2fsutgeXGDWjFfTynvTzc1htAp0OU4F6bt/qUz90NrLlQI/0t+MvOin7+PZUvbm2JCW9slBhc2LTSzfTMpgc5Rl/4RWFdjBskwLL70+mEfiJBzioUAyUk+NPU87OWb+V/S0u7LceEwiDbd1S8P9C8JsfyZJ9z871DibGJxj2w+p18uzMvogsnvLbDGqYSd89Tp+bUD8seJ3917ZKf0f11oZM0Fl5j8B06zLLfavJ0sXY9fpRr4/Dv/DoO7UZ2hThZ0zgYKXC8jcxrv6Ymgxp/6cw1RZB2jCvpG1V5OYP8OhLxXGXwSaGJ/IUkSTf7mTXZzxRxAmobFRFvJI/wKMLTbE4wJNbkd/SAiQ1K2KDezjIf3UrWUROKXF5hxj98hMm8/dY8GmaxNnIFq0FTXrPZ4q0ySJJ+ap0EblNTwaTXTv5Nhif7s/yTN9Mm5K8P/OJ+EE9PEJHMn9uAtLCJDWLecmOpBi+e8rjEDElsHQx+B/ziKXkRs2kWawsm+Ruw0OCilyC59S/U6LgdfZrLW7Sez5TzHbfamHPYX1CP0byyGJdDXFhUwTODhIuW5nUrxy+2E1nID7dq7LlQ/Klhy8Ox/IOPOT6rWm0RZA2LJm7l5RdYfXnO6UtUCCZEYtR7e1dWHCOgokNBCJ+1vyTuEt/wD+hzKeHAhRxkLfJAFBEFvXcPqXeErQEoouaAqf+qWvCXFhwDjf1+E9tiD6zlV5hy3N2Elh6ZGFUfg/uLGU/S2bzSLmpysBWepJFvRu4/QWxdinknLxNBu/PfMOew9aaHyU/IUjbao46uGTJJJtFLvc4XaneOuWUw1aWfcP1UZfkewbJB8ogd5evVClIZ/1WKujgusL3m390Js3PaWbW+1YbZ/1W9jjucSl6PaHkRgDJ3LwHZRjYa0rYb+IENnvNOl6+2sHpD+UHcaZqm2c9FSGF/MrWsafmKZei4rTeFkF6WPC7wW3TCxYsSH6fdCacApZmTK3y1txvbLyATD+dSea1LKRtWj+7XuyjVV+U+6RNrfJGa7+18QIy/XTPK9I2rUducZzqfOV0ZQcGanj+nzg2FySs8jZNJi8gi0f/g2J+Hw4iyAQSVnmbZqYXkM00+h8R4nAQQSoIJS3QYT59UAgEmUKmf0QI5hviPmmBQCAQCDKU6fukBQLBjPGi+KQFAoFEMp90UiUtEAgEAoFg9pjWwjGfX+xmFwjShcdtFzNpgeAF4rRiy50WwictEAgEAkGGIpS0QCAQCAQZilDSGcUdWtyH6Z/raggEc03gIaePjKTxIheBIDMR+6Qt0t9kx3vdRMSKy/gaNyke3KHF/TZfJUQspLb9JuUv7iFbgheSxEM/zB32IZ2w9etfdeLqXX0pEMxT5r2S7m+y4yVeIc4ehY1hfI2KB72H8Rz8lG3NYRqKjVJuosEfpkH5KHSeQ1Xfky0UtOCFQlLQHN3K/gLlsw7aasTxlgKBkvmlpHsP47n4R1rP15m7x3y26T2M5yB4/Zfpc9tpSaqo1YT/fZPBivconL0aCgSZR+AnHqxxsUc1E85iXY2NS6EpMHHy/IMPO/TP71bdQy0QzG/ml5LWoLAxjC/dhYbOc6jqfQbzPqDVL30wFPr93Kiz4zlo1nx9h2tnoLY93gLwKV73pwgzuOC5peBV8j/8hm8COQqT9QTfXJzEXmNuFq1rGg885LSlm6wEgszGspJW+2R3420Gb3R2O8yNOjdjNeoZZbhtO7Uj76lM0uG27dSeiSyRildIav/ttuYwRd2xcmvd70d9vsnzJsE/3N9k50qOn4O8G4uX90HyGXpEObMbrz8cNwPOpfx8mHLu0OK24zFUssPcqHub4D4/DQnhu/H6PzI1u06PLPSQ8m+NXgZmvt4RF4FWOmPZRdrkZ9lFN60DhdS2fwLH3XRvNpKn2b5IfO+sWEZmB8kHe/e7yG/pasPI/SbBcx3cc5SwlQexW6XWuOJ8smr/b/7Rray8J6WLmZZjJuiI8pv03eNSaGX0ykP1zVXKm6YifuISXr7azd3vkt1ClcXma+vorOzgtOJp/tGtVJu8gGJmZtKz07dJ/eqBh5z+MHJHpLrMhNvBVFdOavVzPpyVbsHSr6e2Hz9evim1RTDrWFLS4bbteIdis8eI0iLvA0uFhtu2U9u5nVb/TSmf3sN4qg6T7f+IQnmgpDmMTx4g+9vOk90YxleS3NwtDfIuvP6b8sAvDdCeJrWiHjzjpnmfH58/Nxqnua2Mk9W5+hV31HHSX5ekdRq+ZxWyclt1GZ9RWUlIjyyM0v0DjoXxycq8v8mOt+mtpGsDYvKJfOTc4UbbMIXVuaZlF7z4D5ZFyx7mBvHylOpTW4cF14j2excunlvXyqRvGN7Zyn65EsFzHVw/96rqruDwxW46akrYf20hkQG5w/cHWQEn+n+DvhFs6x2Er/7CpEceyAM/MbHGBuNTUCAp7snQJPnrFQrav5Q919bL8R9yuuEhLykUzMTVYV5W1NWYVC+iWEjBia3EdMcEnZU/sVJRD7PMVt9OFugvWpOU8GIqrm2V6ztBwDeF07NQERa5x1kq7/Q59d3Q6n6eIpBQT6ktl45gYQGd9bYI0oMFJR0xzyoGLUcdB/fdpLbTSpEa+RT/ndo8N329H1FYDFDIsmWxFIXVyRRjfN5KBZNL+bEP6K76mBu7NsVmiBWXFQo5lzc2F9LaeYtwtd6grLc624i42aVikZlvWjO0dMpCG3v1R5QrfheW7IaLQ4TZZKDUtOSzifJqvTAd2W3+e+JMXyVPKGy8zDb3x3wdqrPgMkj1vZs9bJ7XUU5mnOsdcPUZk2TFBs+ydYoZ8UJy3Tbu+hUKGBsvL1Xk4cmB8Aj2754yCdiA4L2nrP7rUh5FFfcEj2/ZeLkcJFM0bGxRDNgFuWxc083jwOs4IxV05ya9w9nUPdJl69i/3jjKTDBrfatLpB+VHxRZFHj0whZS8I6LRw2PCZRnxfpWq59V9QRn/TryKx8zHM6xcK+2lbYI0oV5JR0aIoiLorgBz77c4iKN0BBB+vmqyk5rXJBrdBiKN1G572Nqq+y0JmxjSq2OOMooyXufsTFADnPlqGexyduRbIZshDx75gNa/eHpz8zSJgvjmX6CaTrZLF5PPkZhGrJzLk+sV7w8IZdlef2qdMZM472bZRIUW5w51+5Q+3Ft2YsVv7JYV/OYSw0d3FWaTu1/YPWaQVnJTvHb6GJeqv8Dq68OS4o78BMP1ixljx0IP2OCSR40dHA3rm728Skimi4rO7k/OeEeab0tU4Gf4lLq39MM8KAypPFUbUrWYlb6Vo/wMyZYzEqtAUAvTJbTrz9CZODQ6uf4esJCXl4zqUpnjMW2CNLGHC0cM/Zf2qtv4quWTJYeNxp7jueeBAUl49rn1zCZR/zVxmmpuIzP/9GM19UYC77kKLJVoeIyPr8sl97DeC4mSTb2PYOpVTIFhhkbSB5LSea9d7JiKlvH/mvyoJnCwqiIYgye6+B0JVE/Z67bxqPxKVj6C4+Wv0o1C2F5iMeB17GNP8XuzlUoTiOFN5Vi++JQ+WqJU5gzfU/z7PatJj8+JX03IUzx63fJYymx1BZB2rB44tgg43EfrP3dnybECo4Oq36PjygUkmMVTo18tChsDONr/wDX9Y+5YSK+bt6hW3QPqE2Z06G/yS77ccP4VH9+SjrdeJrupJA2jJe38Zg+cSy9slDR+yVfsRuvcvHZqAn1u+yPuPTKm2nZhYYIspsihcnesC8UWH7vZovATzzAQYVioJwcf5pyds76rexvcWG/9ZhAGGzrloL/F4Lf/EiW7Ht2rncwMT7BsB9Wr5NnZ/ZFZPGU32ZQw0z67nH63IT6YcHr7L+2Vfo7qmf+mKCz8h6B6dZllvtWk6WLsev1o14fh3/h0XdqM7Qpws+YwMFKhT1/Ylz9MTUZ0nY7mGqLIG2YV9KOOnZW9NN6/Hzsa7D3MFeGlHMwybc7eOYfMUXTezjuhK5NFFX001qlVkb9TZHf0kIiNa7YgR8D3zOuW8lNVO4jLu9hbhx/n8GK92ZoO9Md+q4XUntMy3ct+VBd17/UUbRGaaGw0U9t3qf09SapQtpkoUO8sg2dp1nLMqBXb1V5EXlPT3aDZ9y0RPstku4t2UJgpi+SvHdzQfygHh6hI5k/NwFpYZKaxbxkR1IM3z3lcYiYEli6GPyPecRScqMvaRYryya52/CQoCKX4Dn175QoeJ39Woub9J7PFLPdt1rYc1if0I+RPLJYV0Nc2BSBs4OEy1Ym9SuHL3bTGYhP96ps+ZB86eGLw7G8Aw+5fmsabRGkDUvm7sJGP7V1bmkLFEjmwBpUZk579U28I3Z5r68Up3XfILUjynzCeFHEQd7uAsAm3mA7Hrd6S1AhRBc1ed2f6poi7dU3aWU7te7Y2+Xa55/WSmo1myiqeBvv8fO8kbByOKIcLuuYj43SQn+Tm9aB3XhNLCpLjyx0iCxSi/iy8z6gtXk3tcnM3XJ5rTlK+Ujm9kh9U5Wda99lll204zlItK3K9yN5Xxi8d3OFPYetNT9KfkKQttUcdXDJkkk2i1zucbpSvXXKKYetLPuG66MuyfcMkg+UQe4uX6lSkM76rVTQwXWF7zf/6Eyan9PMrPetNs76rexx3ONS9HpCyY0Akrl5D8owsNeUsN/ECWz2mnW8fLWD0x/KD+JM1TbPeipCCvmVrWNPzVMuRcVpvS2C9LDgd4PbphcsWJD8PulMOwUsTVjzSceh2iesYLo+0BdSFtKivO7NJvp9HvCi3CdtapU3WvutjReQ6ad7XpG2aT1yi+NU5yunKzswUMPz/8SxuSKyyCglij+agwViVtHfcmZ8yEeq6QQvEgmrvE0z0wvIZhr9jwhxOIggFYSSFuiQ6paz6WxVEwjmO5n+ESGYb4j7pAUCgUAgyFCm75MWCAQzxovikxYIBBLJfNJJlbRAIBAIBILZY1oLx7Yc6JnRyggEAn1un9qAP5j2y1cFAsEc4XYar6AUPmmBQCAQCDIUoaQFAoFAIMhQhJKeDYJN3P68faauHRAIBKbpp8nZQrKTdQWC+YLYJ50SfXx76pDqDPHsHT28lvQMvRCjn+/kWbFO3GATt3uduP9WhTg7SPC80nvEw8HPTETc5cV/Qnkwaz9NTi/XEyK62Hf3JNXi+mPBc8i8V9JPbm0gwEm2lBWlqURJQbOjhy1O5bMN9JdeobBgLm9kEAgyn+ITPvwnFA+6WnDXfEXFRR+NpUYpC2kM+mhUPhq5Qd3GMZYLBS14TplfSjoTZprBfzFufxe3aiZcRE5pHv6JEJBcSY9/sUH/Ji/7u9Ovo0AwX+hqwV0DzUEv3U4PTUkVtZpQZzcDu3YiTpsVPK/MLyWtwZKyHraks0Dnn8n+4hAjwSqFybqPka4BbKXmZtG6pvFgE7eFM03wIjByg7qNrQysraU9WI4DKA620rbDg7vGrPm6n8+Ow7678XeVfcVB51cIM7jgecCykn5yawOB6Onxf6FgBwSis1ttn+tUoB7/xB6VSXoqUI+/a0D+lcfqvedYHr0bT+3zzd7Rwyvfx8r1n/oE8iUTd/K8icZVtmE46wp/4sNYPPu7JmfoRbx24CTfntrAbcXT7B09FJq8122mZtLpkYUBwSZuf/HPWPkHGlmimSdxMojU7QqLenfyKJzH6r1H4cud/OwykovZNiW+P8nXC2QqYdp21HLmfuT3NpqDDdGZY+8RDxdXtXKMZqqOD0oP19bS/kW5wqaj9uVWXPRRcktKd36vXRUHxUw2dOEQVUM7o37h0IVDsTJUClCq48g7reScreXMfQPlGFHObKM56IubAdup/sJHNf00OT24DZVsmLYdXoaOtdKYEK7uI31mp2+NLQHGZRoiuwW00qllQ5w/X0s+B+G9Wjo9Ru2LpFO3Kf69sN4HAitYUtJTgXoCE+/iPiAPmpPt9F/4xLKJdipQj39wE+4D56R8gk3cvtDEogONLNHw+T4JtPO/ZT1s+WNyc7ekHFZQcOCcrDCkgf32LbWinuzayX9Kr7DlgCMa5z8Bt0mfchGvHejhNUutBnCw/G89LI/+7uPbU//iFYVyM0t6ZJEkXdcKCg70yPH6GA2EWFLgMC2DZ72fseitHrbY5DjEy0X6EPF/jgUXh/b7M+Wcn4vxQheuwcc+/LIi6j3i4eCRItWCqoHjtRw/1oo/aCcysB6/8IasgGPK1y8PnL0XbrCsbBsDZ78mtFcekLv6GFrrguEwlEqKe2xokIoyhYL2ldAePCnHb8G9sYXlCkUxdPYaOYq6apJTzvlgeZJWa/ieVciKzuXFvzf1+yhnq29DpeW6Ti8zZWqnO0TVcafiw6aftgthivfaFWEn5TCpnu4j6oV3avmEaUton1Sfqh3EfYgYYb0PBNawsAWrj5EuWP2WYrCzVfGn0jyLRWrk49zFavs/+TkYeZDHIsVMbkmB+QF6pAtW71UqGAfL33oX24NLjCqvr80/qVDIDrJceUwO+g23TU0F6rl9aoPx360+UzWdHumUhUE6VT8XsbzAoROmIwPXrsQZu0ousKTsJNnhO0wku3pYRarvT+bh2NugmkkWl22DwXFCyki7vIoZsZ03PS4GfF8r4rjIWaHIY285jhXLyLs/xpj8rPdWkM3vlEA0XT/dn0XSyWbljxUDb2kl+9Z+RXeXoh6eyiSmZXmGbOkvbjtVVwtupzS7S6bYkjFrfTvdMhOImPWVM+5CquUPhcQwO9Uf15L32RXaRhTZaMlH1T4oPuGl4n43/x7BAtb6QGAN8zPpyVGesYJX4gbVhS+t0I5vmM8A4xc28CguyPZbCJxF5JRewn9hA4/yLa7a1qkjNjev2D/h2SQgh9my1K+RmXYsLDjHFuV9sHoL2YL/ikuZuGVLyfipf2o8VZuPVaRNFjr/1fT62ShMQwaLXkrMP14u4GCRfUCVzphpvD8ZSoIpc22tKjxvVbbqtyNXadsvZNexK1Rt9HBGaQLNeYPNa1vp7mqguDTM6KCT5SfeYPPZa4wBjq4+rq8toT0HGBlniEGub/RwJq5uecNhkGdQq3KTzWqTzZCNkGfP1NIe9M2YEpiVvp1mmQmMjDOEkxJN94FOmCzfkR8AOUxLPvHtg2xy1g6q0hmTWh8IzDNHC8cMFBAxZfjk1gZunyLBp5yRqPyzxJmdUzWPpwNjWWgyGcTSxHZahHhm8SK2efn+aCL7+nZ58Qflwa+rBfdZa7k49p7Ev1cyZbqdRP2Vb3pcdA6HYcXXdLqKOI8dXF/R3dXAsuEgeZ5KhTI08p2mdlNegrKSyTvWqprdSUT81cZp2eXFHzRzm/ns9u2MlvnDGANJoswc44zcTx5LibU+EFjF4oljP0gzGgVPvk+cBT77TW28+e+E4hWzLWeRRj5aLCnrYcteDTOpHnp5T/r5Oaw2gU6HqUB9olnb2ciWAz3S346/6KTs49tT9ebakpT0ykKFzYlNL91My2BylGf8hVcUExjDNimw/P5kGl19XGcbzYoBLzRs6IcwpPiED//dmBnUsVkyb/d2drNK9j0Xl21jaLiff/tg82ZZUeZks4ogo5ZMoMb0HvHIPm4fftVfK5t9tbiP9KeQ1kcz3kQTuRaz3LczWuaKZeTp9b+ebEa+pvO+2gxtipFxhthGiWLh19Cw+iNsbEjj4wiTfSCwjHklbasiN3+AR18qjrsMNjE8ofSDyr7drs94oogTW4EMUMQr+QM8utAUiwM8uRX5LS1AUrMiNriHg/xXt5JF5JQSl3eI0S8/YTJ/j7kVy1ZwNrJFa0GT3vOZIm2ySFK+Kl1EbtOTwWTXTr6NjluRdH+WZ/pm2pTk/ZlPxA/OIzc4rjV7NERaYKTGKR3+kZPNqvtjdA8RG8xXLAPfFTop4c2oubOQkl2DnNmoVn69R1I9flPyd6t83FEi/tQ+nbyN0kLxidZEX7kWs923M1lmTjk1Cf0fKbuQXceICwvT9l4rA7t2Jt1+NnC8lqZoX0XSFckWE9kHf/xaLO+ulrjT4iz2gcAylszdS8qusPrzndIWKJDMiMWo9vYuLDhHwcQGAhE/a/5J3KU/4J9Q5tNDAYo4yNtkACgii3pun1JvCVoC8qKmnVI6HRPmwoJzuKnHf2pD9Jmt9ApbnrOTwNIjC6Pye3BnKftZMptHyk1VBrbSkyzq3cDtL4i1SyHn5G0yeH/mGznlHDvWLfn7QNoec3EbVZZMsoW8ySHcTvXWqWI5rGSXl4ODtZLvGSRfJqhx5+QAAAzRSURBVK2cce1UKcHiEz6a8cj7jyUqLqbqX5bLfe8GbyasIo4oCq+Oad0oLfQeqeXM/W00J9sCNOt9O7NlFp/w0b7qEFXRaw0l9wNI5uZ2lGGSy8DMyve8Y15yznpw18gP4kzVjr0naR5SyH2Xl/ZjQaqGIjEs9oHAMgt+N7htesGCBcnvk86EU8DSTMIeYE209hsbLyDTT2eSeS0LaZvWz64X+2jVF+k+aWs+6ThUe4YVCH+oSaSFeJ0eE30tmFXcTg8Ganj+nzg2FySs8jZNJi8gi0f/g2J+Hw4iyBQiC45SorTB5AKxdKN3CUiyQz5STSd43hFKWqDDfPqgEAgyhVS3mU1ne5rgeUbcJy0QCAQCQYYyfZ+0QCCYMV4kn7RAIEjuk06qpAUCgUAgEMwe01o45vOndpqQQCCwjsdtFzNpgeAFwq3YOqeF8EkLBAKBQJChCCUtEAgEAkGGIpR0RnGHFvdh9E8tFggExvTTZObsboFgniD2SVukv8mOV+vEgXgqLuNr3KR4cIcW99sknpFUSG37Tcpf3EO2BC8YvUc8cec/65BwepjegR/SUZTJzqkWCOYj815J9zfZ8RKvEGePwsYwPuWJA72H8Rz8lG3NYRoMD6zdRIM/jOqMpNB5DlV9T7ZQ0IIXiOITPvwnFA/kIz6Tn6ylceDHyA3qNo6JCx0Ezy3zy9zdexhP3fkUb6+dBXoP4zkIXv9lOGinxaKNLfzvmwxWvIU4aVjwwtLVgrsGmoNeqPEobmQyR6izW3Frk0Dw/DG/lLQG0sw2PbPoKKHzHHLb8Vz8I63+jyhkEw1+P8su2vG4t3Mj/qZETe5w7QzU7oqv+6d43VbyEQjmISM3qHN6cJ9dRnuwgWIKaQy2SjcyOQ+ZvI+4n8+Ow77/F/+Z+xUHnVbyEQgyF8tKur/Jjscd+TtMv2p2O8yNusQZZbhtO56mO4nPovnEK6Q7tLhj5bT0yuUe/BQG3qfWbY/mlzxve0J4f5OdQ23D6nhmZugR5Vz1PTv9YXzn64jdH5NL+fkwPv97jFUlU7LD3Kh7m+C+TzR80bvx+sP4/Mn91OmRhVEb4so3kwwkC4ROOmPZRdoUKXs7N0LSv43labYvEt+7uSdM2w4PbmfkL/5OZw91F8KELhyKxdlxA7UI+2lyxvJo6oqli4+jnMmGLhzCfaRf/Tuaj1IBSnVs6orU1UA5RpTzxjFqgj78qusm7VR/4cMf3MnIxmRKNkzbDi9Dxw5q+KK30Rz04Q8m81PPTt8aY1ymIV0tuunUsvGo5KYtH+nfxu2LpFNXI/69sN4HAitY8kmH27bjHfqAVr+snELnOVT1PuR9YKnQcNt2aju30+q/KeXTexhP1WGy/R9RKC+wojmMT7Zh9bedJ7sxjK/ksDR7VSlHjbzPuPD6b8pm5GFu1LnxNKn91oNn3DTv8+Pz50bjNLeVcbI6V7/ijjpO+uuStE7D96xCKqt11WV8RmUlIT2yMEr3DzgWxiePsP1NdrxNbyW1asTkE5bzv8ONtmEKq3NNyy548R8si5Y9zA3i5SnVp7YOw3dFjfZ7Fy42m352CF24Bh/78MvKpveIh4NHilQLqgaO13L8WCv+oJ3IFYTHL7whX0EoLbbiog+/7O/tvXCDZWXbGDj7NaG9spLs6mNorQuGw1AqtXhsaJCKskK5Hoeo8pXQHjwpx2/BvbGF5cGGqKl56Ow1chR11SSnnPPB8iStTnbZhNTGMy6vqTuT9Zitvg2VJt5zbaVM7XSHqDrupDnok/u7n7YLYYr32hVhkXucpXq6j6gX3qnlE6YtoX1Sfap2QLvGXd3aWO8DgTUszKRl8+wxxaDlqOPgPqseVY18iv9Obd6n9EU/DQtZtiyWorDa/EB77QzUtisVTC7lxz7Adf1j9Qyx4rJCIefyxuZCBjtvGcym1bMsc39xs8vew3jcbsZqpmuiT6cstLFXf6Sa6ReW7IahoSTWCC35bKK8OlcnTEd2m/+eaGVQyRMKGy+zbeAmX1tyGaT63s0ejr0Nqtlgcdk2GBxXz+Z2eRV3Att50+NiwPe1Io6LnBWKPPaW41ixjLz7Y4zJz3pvBdn8TglE0/XT/VkknWxW/lgx8JZWsm/tV3QrZ02eyiQzV/WMy9xf3EyzqwW3s5aRd3zTvjd61vp2umUmIPf/3QaF772QavlDITHMTvXHteR9dkVtidCSj6p9UHzCS8X9bv5tyU1grQ8E1jA/kw4NEcRFUVzv25e7rJUYGiJIP19V2WmNC3KNDkPxJir3fUxtlZ3WhG1MqdURRxklee8zNgaRt8eVo57FJm9HshmyEfLsmQ9o9YenP/CnTRbGM31p5qv4DEk2i9eTj1GYhuycyxPrFS9PyGVZXr8qnTHTeO9mGWmmNBh7sLZWFZ63Klv125GrvOy7kF3HrlC10cMZ5ZamnDfYvLaV7q4GikvDjA46WX7iDTafvcYY4Ojq4/raEtpzgJFxhhjk+kYPZ+LqljccBnkGtSo32Zs9nesY5dkztbQHfTOmBGalb6dZZgIj4wzhpETrA0gvTJbvyA+AHKYln/j2QTY5awdV6YxJrQ8E5pmjLVi78RqYU+3VN/FVR3yuaOw5nnsSFJSMa59fw2Qu+asjRj69tFRcxuf/aMbraoyxLLSR93xXXMbnl+XSexjPxSTJxr5nMEmUmWOYsQFrKTLvvZP3Be/y4g/KEupqwX3WWi6OvSfx75VMmW4n0f3Hb3pcdA6HYcXXdLqKOI8dXF/R3dXAsuEgeZ5KhTLcRnOwQWcVdWr7LRKUlUzesVbV7E5C8ldXJ0nLLi/+oJlP6dnt2xkt84cxLL7K02CckfvWUljrA4FVLC4cG2Q8zi7T3/1pQqzg6LDq9/iIQiE5VuHUyEeLwsYwvnYNc6ceenmHbtE9oDZlTof+Jrvsxw3jU/35Kel0JyzMMpc2jJe3LSzASq8sVPR+yVfsxqtQYOFRE+p32R9x6ZU307ILDRFkN0UKrWLYFwosv3ezRVcf19lGs2LACw0HU86u+IQP/92YGdSxWTJv93Z2s0r2PReXbWNouJ9/+2DzZllR5mSziiCjM7hSuveIR/Zx+/Cr/lrZ7KuNW5hkNq2PZrzmFmPNct/OaJkrlpGn1/96shn5ms77ajO0KUbGGWIbJYr96kPD6o+wsSHt/+um+kBgGfNK2lHHzop+Wo8rVs32HubKkPKLSfbtnvlHTNH0Ho47oWsTRRX9tFaplVF/U+S3tJBIjSt24MfA94zrVnITlfuIy3uYG8ffZ7DivRk61esOfdcL1X7cKBEf6pc6itYoLRQ2+k35g9MnCx3ilW3oPM1algG9eqvKi8h7erIbPONWrMaOpIvsQTfTF0neu7kgfnAeucFxrdmjIdICIzVO6fCPnGxW3R+je4jYYL5iGfiu0EkJb0bNnYWU7BrkzMb41c+pHr8p+btVPu4oEX9qn07eRmmh+ERroq9ci9nu25ksM6ecmoT+j5RdyK5jxIWFaXuvlYFdO5OewjZwvFaxGjuSLrLvXPbBH78Wy7urJe60OIt9ILCMJXN3YaOf2jo3te73pQcVl/HVoDJz2qtv4h2x43V/Go3Tum+Q2hFlPmG8KOKAdGIXAJt4g+143JHhVDo2sxDkRU1uKZ2OKdJefZNWtlPrjqlB1z7/tFZSq9lEUcXbeI+f542ElcMR5XBZx3xslBb6m9y0DuzGa+JkhvTIQgdHHQf33ZT8twB5H9DavJvaZOZuubzWHKV8JHN7pL6pys6177K0T/0g0bYq34/kfWHw3s0VOeUcO9Yt+fsA1tbSfnEbVZZMsoW8ySHczogykI7QLJbDSnZ5OThYK/meQfJl0soZ106VEiw+4aMZDwedsYNtKy6m6l+Wy33vBm8mrCKOKAqvjmndKC30HqnlzP1tNBueXEYa+nZmyyw+4aN91SGqotcaSu4HkMzN7SjDJJeBmZXvece80t70GvlBnKnasfckzUMKue/y0n4sSNVQJIbFPhBYZsHvBrdNL1iwIPl90r3Jt0U9j1jzScchHyWawHR9oC+kLKRFed2bTfT7POBFuk/amk86Dvko0QSEP9Qk0kK8To+JvhbMKm6nBwM1PP/P7p4rIouMUqL4ozlYIGYVvQtBSHJOearpBC8akQVHKVHaYHKBWLrRuwSEJGeTp5pO8LwjlLRAh1S3nE1nq5pAMN9JdZvZdLanCZ5n5v3Z3QKBQCAQPK9M3yctEAhmjBfJJy0QCJL7pJMqaYFAIBAIBLNHygvHjBIKBAKBQCCYXYRPWiAQCASCDEUoaYFAIBAIMhShpAUCgUAgyFCEkhYIBAKBIEMRSlogEAgEggxFKGmBQCAQCDIUoaQFAoFAIMhQhJIWCAQCgSBD+f+SJ/6JZKLUaAAAAABJRU5ErkJggg=="
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ§ ë°ì´íƒ€ ê²°í•©   \n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30160, 30160)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(question_corpus), len(answer_corpus) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ì­‰', 'ì¢‹', 'ë‹¤ê°€', 'ì˜¤ëŠ˜', 'ê°‘ìê¸°', 'ë„ˆë¬´', 'ë³´', 'ê³ ', 'ì‹¶', 'ê³ ', 'ìƒê°ë‚˜', 'ë„¤', 'ã… ã… ']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_corpus[12329]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ê·¸ëŸ°', 'ë‚ ', 'ì´', 'ìˆ', 'ë”ë¼ê³ ìš”', 'ë‹¤ë¥¸', 'ìƒê°', 'ì„', 'í•´', 'ë³´', 'ì„¸ìš”']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_corpus[12329]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. ë°ì´í„° ë²¡í„°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start>',\n",
       " 'ê·¸ëŸ°',\n",
       " 'ë‚ ',\n",
       " 'ì´',\n",
       " 'ìˆ',\n",
       " 'ë”ë¼ê³ ìš”',\n",
       " 'ë‹¤ë¥¸',\n",
       " 'ìƒê°',\n",
       " 'ì„',\n",
       " 'í•´',\n",
       " 'ë³´',\n",
       " 'ì„¸ìš”',\n",
       " '<end>']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# <start><end>ì¶”ê°€\n",
    "\n",
    "answer_corpus = [['<start>'] + sentence +['<end>'] for sentence in answer_corpus]\n",
    "\n",
    "answer_corpus[12329]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=20000\n",
    "\n",
    "def gen_tokenizor(corpus): \n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token=\"<unk>\", num_words=vocab_size)\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    return  tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 7179\n"
     ]
    }
   ],
   "source": [
    "chatbot_tokenizer = gen_tokenizor(question_corpus + answer_corpus)\n",
    "\n",
    "print(\"Vocab Size:\", len(chatbot_tokenizer.index_word))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 50\n",
    "enc_tensor = chatbot_tokenizer.texts_to_sequences(question_corpus)\n",
    "dec_tensor = chatbot_tokenizer.texts_to_sequences(answer_corpus)\n",
    "\n",
    "enc_tensor = tf.keras.preprocessing.sequence.pad_sequences(enc_tensor, padding='post', maxlen=MAX_LEN)\n",
    "dec_tensor = tf.keras.preprocessing.sequence.pad_sequences(dec_tensor, padding='post', maxlen=MAX_LEN)\n",
    "\n",
    "enc_train = enc_tensor\n",
    "dec_train = dec_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6. ëª¨ë¸ ì„¤ê³„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# positional_encoding\n",
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i) / d_model)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "    return sinusoid_table\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.W_q = tf.keras.layers.Dense(d_model) # Linear Layer\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "\n",
    "        \"\"\"\n",
    "        Scaled QK ê°’ êµ¬í•˜ê¸°\n",
    "        \"\"\"\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9) \n",
    "\n",
    "        \"\"\"\n",
    "        1. Attention Weights ê°’ êµ¬í•˜ê¸° -> attentions\n",
    "        2. Attention ê°’ì„ Vì— ê³±í•˜ê¸° -> out\n",
    "        \"\"\" \n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "        \n",
    "\n",
    "    def split_heads(self, x):\n",
    "        \"\"\"\n",
    "        Embeddingì„ Headì˜ ìˆ˜ë¡œ ë¶„í• í•˜ëŠ” í•¨ìˆ˜\n",
    "\n",
    "        x: [ batch x length x emb ]\n",
    "        return: [ batch x heads x length x self.depth ]\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        split_x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"\n",
    "        ë¶„í• ëœ Embeddingì„ í•˜ë‚˜ë¡œ ê²°í•©í•˜ëŠ” í•¨ìˆ˜\n",
    "\n",
    "        x: [ batch x heads x length x self.depth ]\n",
    "        return: [ batch x length x emb ]\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (batch_size, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "    \n",
    "\n",
    "    def call(self, Q, K, V, mask):\n",
    "        \"\"\"\n",
    "        ì•„ë˜ ìˆœì„œì— ë”°ë¼ ì†ŒìŠ¤ë¥¼ ì‘ì„±í•˜ì„¸ìš”.\n",
    "\n",
    "        Step 1: Linear_in(Q, K, V) -> WQ, WK, WV\n",
    "        Step 2: Split Heads(WQ, WK, WV) -> WQ_split, WK_split, WV_split\n",
    "        Step 3: Scaled Dot Product Attention(WQ_split, WK_split, WV_split)\n",
    "                 -> out, attention_weights\n",
    "        Step 4: Combine Heads(out) -> out\n",
    "        Step 5: Linear_out(out) -> out\n",
    "\n",
    "        \"\"\"\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "\n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "            \n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "    \t\t\t\t        \n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out, attention_weights\n",
    "    \n",
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.w_1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.w_2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.w_1(x)\n",
    "        out = self.w_2(out)\n",
    "            \n",
    "        return out\n",
    "    \n",
    "#Encoder Layer êµ¬í˜„\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "        \n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn\n",
    "    \n",
    "# Decoder ë ˆì´ì–´ êµ¬í˜„í•˜ê¸°\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Masked Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out, dec_enc_attn = self.enc_dec_attn(out, enc_out, enc_out, causality_mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "        \n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn\n",
    "    \n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "    \n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns\n",
    "    \n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "                            \n",
    "                            \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "    \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns\n",
    "\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 src_vocab_size,\n",
    "                 tgt_vocab_size,\n",
    "                 pos_len,\n",
    "                 dropout=0.2,\n",
    "                 shared=True):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        \"\"\"\n",
    "        1. Embedding Layer ì •ì˜\n",
    "        2. Positional Encoding ì •ì˜\n",
    "        3. Encoder / Decoder ì •ì˜\n",
    "        4. Output Linear ì •ì˜\n",
    "        5. Shared Weights\n",
    "        6. Dropout ì •ì˜\n",
    "        \"\"\"\n",
    "\n",
    "        self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared = shared\n",
    "\n",
    "        if shared: self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "        \n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        \"\"\"\n",
    "        ì…ë ¥ëœ ì •ìˆ˜ ë°°ì—´ì„ Embedding + Pos Encoding\n",
    "        + Sharedì¼ ê²½ìš° Scaling ì‘ì—… í¬í•¨\n",
    "\n",
    "        x: [ batch x length ]\n",
    "        return: [ batch x length x emb ]\n",
    "        \"\"\"\n",
    "        seq_len = x.shape[1]\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "        \n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        \"\"\"\n",
    "        Step 1: Embedding(enc_in, dec_in) -> enc_in, dec_in\n",
    "        Step 2: Encoder(enc_in, enc_mask) -> enc_out, enc_attns\n",
    "        Step 3: Decoder(dec_in, enc_out, mask)\n",
    "                -> dec_out, dec_attns, dec_enc_attns\n",
    "        Step 4: Out Linear(dec_out) -> logits\n",
    "        \"\"\"\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "        \n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
    "        \n",
    "        logits = self.fc(dec_out)\n",
    "        \n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "\n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
    "\n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask\n",
    "\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 7. í›ˆë ¨í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_layers = 1            # encoder, decoder layerë¥¼ ê°ê° 2ê°œì¸µ ìŒ“ìŒ\n",
    "d_model = 368           # denseì¸µ ë° ëª¨ë“  ëª¨ë¸ì˜ dimention \n",
    "n_heads = 8             # multiheadattentionì„ ìœ„í•œ head ê°¯ìˆ˜\n",
    "d_ff = 1024             # FeedForward ì¸µì˜ dimention   \n",
    "dropout = 0.2           \n",
    "\n",
    "\n",
    "src_vocab_size = vocab_size\n",
    "tgt_vocab_size = vocab_size\n",
    "pos_len = 600           # position endodingì˜ ìµœëŒ€ê°’\n",
    "\n",
    "warmup_steps = 1000     #learning rate\n",
    "\n",
    "\n",
    "transformer = Transformer(\n",
    "        n_layers,\n",
    "        d_model,\n",
    "        n_heads,\n",
    "        d_ff,\n",
    "        src_vocab_size,\n",
    "        tgt_vocab_size,\n",
    "        pos_len,\n",
    "        dropout\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "\n",
    "\n",
    "learning_rate = LearningRateScheduler(d_model, warmup_steps)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                     beta_1=0.9,\n",
    "                                     beta_2=0.98, \n",
    "                                     epsilon=1e-9)\n",
    "\n",
    "\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    # Masking ë˜ì§€ ì•Šì€ ì…ë ¥ì˜ ê°œìˆ˜ë¡œ Scalingí•˜ëŠ” ê³¼ì •\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "# Train Step í•¨ìˆ˜ ì •ì˜\n",
    "\n",
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    gold = tgt[:, 1:]\n",
    "        \n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt)\n",
    "\n",
    "    # ê³„ì‚°ëœ lossì— tf.GradientTape()ë¥¼ ì ìš©í•´ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤.\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions[:, :-1])\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(question, model, src_tokenizer, tgt_tokenizer):\n",
    "    que_temp = preprocess_sentence(question)\n",
    "    que_temp = tuple(mecab.morphs(que_temp))\n",
    "    \n",
    "    que_temp = src_tokenizer.texts_to_sequences(que_temp)     \n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences(que_temp, padding='post')\n",
    "    \n",
    "    ids = []\n",
    "    output = tf.expand_dims([tgt_tokenizer.word_index['<start>']], 0)\n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(_input, output)\n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "        model(_input, \n",
    "              output,\n",
    "              enc_padding_mask,\n",
    "              combined_mask,\n",
    "              dec_padding_mask)\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "        \n",
    "        ids.append(predicted_id)\n",
    "        if tgt_tokenizer.word_index['<end>'] == predicted_id:\n",
    "            result = tgt_tokenizer.sequences_to_texts([ids])[0]\n",
    "            return question, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "        \n",
    "    result = tgt_tokenizer.sequences_to_texts([ids])[0]\n",
    "    return question, result, enc_attns, dec_attns, dec_enc_attns\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, model, src_tokenizer, tgt_tokenizer, plot_attention=False):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        evaluate(sentence, model, src_tokenizer, tgt_tokenizer)\n",
    "    \n",
    "    print('Question: %s' % (sentence))\n",
    "    print('Answer: {}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb337cea67c34d8c97229a891e43da84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/472 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: ì§€ë£¨í•˜ë‹¤, ë†€ëŸ¬ê°€ê³  ì‹¶ì–´.\n",
      "Answer: ì •ë§ ë§ì´ í˜ë“¤ ì£  <end>\n",
      "Question: ì˜¤ëŠ˜ ì¼ì° ì¼ì–´ë‚¬ë”ë‹ˆ í”¼ê³¤í•˜ë‹¤.\n",
      "Answer: ì¢‹ ì€ ê²ƒ ê°™ ì€ë°ìš” <end>\n",
      "Question: ê°„ë§Œì— ì—¬ìì¹œêµ¬ë‘ ë°ì´íŠ¸ í•˜ê¸°ë¡œ í–ˆì–´.\n",
      "Answer: ì¢‹ ì€ ì„ íƒ ì´ ì—ìš” <end>\n",
      "Question: ì§‘ì— ìˆëŠ”ë‹¤ëŠ” ì†Œë¦¬ì•¼.\n",
      "Answer: ì¢‹ ì€ ì‚¬ëŒ ì´ ìˆ ê¸¸ ë°”ë„ ê²Œìš” <end>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5879d5a9f8bd4954be1f5b98229a32f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/472 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: ì§€ë£¨í•˜ë‹¤, ë†€ëŸ¬ê°€ê³  ì‹¶ì–´.\n",
      "Answer: ë‹¤ë¥¸ ìƒê° ì„ í•´ ë³´ ì„¸ìš” <end>\n",
      "Question: ì˜¤ëŠ˜ ì¼ì° ì¼ì–´ë‚¬ë”ë‹ˆ í”¼ê³¤í•˜ë‹¤.\n",
      "Answer: ë‚´ì¼ ì€ ì˜¤ëŠ˜ ë‚´ì¼ ì´ í‘¹ í‘¹ í‘¹ í‘¹ í‘¹ í‘¹ í‘¹ í‘¹ í‘¹ í‘¹ í‘¹ í‘¹ í‘¹ í‘¹ í‘¹ í‘¹ í‘¹ í‘¹ í‘¹ í‘¹ í‘¹ í‘¹ í‘¹ í‘¹ í‘¹ í‘¹ í‘¹ í‘¹ í‘¹ í‘¹ í‘¹ í‘¹ í‘¹ í‘¹ í‘¹ í‘¹ í‘¹ í‘¹ í‘¹ í‘¹ í‘¹ í‘¹ í‘¹ í‘¹ í‘¹\n",
      "Question: ê°„ë§Œì— ì—¬ìì¹œêµ¬ë‘ ë°ì´íŠ¸ í•˜ê¸°ë¡œ í–ˆì–´.\n",
      "Answer: ë“í…œ í–ˆ ë„¤ìš” <end>\n",
      "Question: ì§‘ì— ìˆëŠ”ë‹¤ëŠ” ì†Œë¦¬ì•¼.\n",
      "Answer: í¸í•˜ ê³  ì‹¶ ì€ í¸í•˜ ê³  ì‹¶ ì€ í¸í•˜ ê³  ë‚˜ê°€ ìš” <end>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7e06592062540b0978aaa5c5cc3e71d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/472 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: ì§€ë£¨í•˜ë‹¤, ë†€ëŸ¬ê°€ê³  ì‹¶ì–´.\n",
      "Answer: ê·¸ ì„ ì½ ì„ í–ˆ ëŠ”ì§€ ìƒê° í•´ ë³´ ì„¸ìš” <end>\n",
      "Question: ì˜¤ëŠ˜ ì¼ì° ì¼ì–´ë‚¬ë”ë‹ˆ í”¼ê³¤í•˜ë‹¤.\n",
      "Answer: í˜ë“  ì‹œê°„ ì´ ì—ˆ ë‚˜ìš” <end>\n",
      "Question: ê°„ë§Œì— ì—¬ìì¹œêµ¬ë‘ ë°ì´íŠ¸ í•˜ê¸°ë¡œ í–ˆì–´.\n",
      "Answer: ë“í…œ í–ˆ ë‚˜ ë´ìš” <end>\n",
      "Question: ì§‘ì— ìˆëŠ”ë‹¤ëŠ” ì†Œë¦¬ì•¼.\n",
      "Answer: ê°™ì´ ë°ë ¤ë‹¤ ì£¼ ë ¤ë‚˜ ë´ìš” <end>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a567c657ed2c4b1bb32cf1ac6ca0b4dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/472 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: ì§€ë£¨í•˜ë‹¤, ë†€ëŸ¬ê°€ê³  ì‹¶ì–´.\n",
      "Answer: ë‹¤ë¥¸ ìƒê° ì„ í•´ ë³´ ì„¸ìš” <end>\n",
      "Question: ì˜¤ëŠ˜ ì¼ì° ì¼ì–´ë‚¬ë”ë‹ˆ í”¼ê³¤í•˜ë‹¤.\n",
      "Answer: í—›í—› í•˜ êµ°ìš” <end>\n",
      "Question: ê°„ë§Œì— ì—¬ìì¹œêµ¬ë‘ ë°ì´íŠ¸ í•˜ê¸°ë¡œ í–ˆì–´.\n",
      "Answer: íœ´ì‹ ë„ í•„ìš” í•´ìš” <end>\n",
      "Question: ì§‘ì— ìˆëŠ”ë‹¤ëŠ” ì†Œë¦¬ì•¼.\n",
      "Answer: ì§‘ ë¬´ì‘ì • ì°¾ì•„ê°€ ì§‘ ë§ˆë ¨ ì¶•í•˜ ë“œë ¤ìš” <end>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e22240c42b194a3aa3685c99e3d93a50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/472 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: ì§€ë£¨í•˜ë‹¤, ë†€ëŸ¬ê°€ê³  ì‹¶ì–´.\n",
      "Answer: ë‹¤ë¥¸ ìƒê° ì„ í•´ ì‚´í´ë³´ ì„¸ìš” <end>\n",
      "Question: ì˜¤ëŠ˜ ì¼ì° ì¼ì–´ë‚¬ë”ë‹ˆ í”¼ê³¤í•˜ë‹¤.\n",
      "Answer: í˜ë“  í•˜ë£¨ ì˜€ ë‚˜ìš” <end>\n",
      "Question: ê°„ë§Œì— ì—¬ìì¹œêµ¬ë‘ ë°ì´íŠ¸ í•˜ê¸°ë¡œ í–ˆì–´.\n",
      "Answer: ë–¨ë¦¬ ì£  <end>\n",
      "Question: ì§‘ì— ìˆëŠ”ë‹¤ëŠ” ì†Œë¦¬ì•¼.\n",
      "Answer: ì§‘ ì´ í¸í•˜ ê³  ì‹¶ ì£  <end>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51814da343c740679590954208095e5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/472 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: ì§€ë£¨í•˜ë‹¤, ë†€ëŸ¬ê°€ê³  ì‹¶ì–´.\n",
      "Answer: ë‹¤ë¥¸ ìƒê° ì„ í•´ ë³´ ì„¸ìš” <end>\n",
      "Question: ì˜¤ëŠ˜ ì¼ì° ì¼ì–´ë‚¬ë”ë‹ˆ í”¼ê³¤í•˜ë‹¤.\n",
      "Answer: ì € ë„ ì•„ì¹¨ ì´ êµ°ìš” <end>\n",
      "Question: ê°„ë§Œì— ì—¬ìì¹œêµ¬ë‘ ë°ì´íŠ¸ í•˜ê¸°ë¡œ í–ˆì–´.\n",
      "Answer: ë“í…œ í–ˆ ìš” <end>\n",
      "Question: ì§‘ì— ìˆëŠ”ë‹¤ëŠ” ì†Œë¦¬ì•¼.\n",
      "Answer: ì§‘ ì— ê°™ì´ ë“¤ì–´ê°€ ìš” <end>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a5dc68e8c6f4930815e3cca40ce7c69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/472 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: ì§€ë£¨í•˜ë‹¤, ë†€ëŸ¬ê°€ê³  ì‹¶ì–´.\n",
      "Answer: ì¢€ ë” ì–´ë–»ê²Œ í•´ ë³´ ì„¸ìš” <end>\n",
      "Question: ì˜¤ëŠ˜ ì¼ì° ì¼ì–´ë‚¬ë”ë‹ˆ í”¼ê³¤í•˜ë‹¤.\n",
      "Answer: í˜ë“  ì¼ ì´ êµ°ìš” <end>\n",
      "Question: ê°„ë§Œì— ì—¬ìì¹œêµ¬ë‘ ë°ì´íŠ¸ í•˜ê¸°ë¡œ í–ˆì–´.\n",
      "Answer: ë–¨ë¦¬ ëŠ” ë–¨ë¦¬ ì£  <end>\n",
      "Question: ì§‘ì— ìˆëŠ”ë‹¤ëŠ” ì†Œë¦¬ì•¼.\n",
      "Answer: ë°ë ¤ë‹¤ ì£¼ ë ¤ë‚˜ ë´ìš” <end>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "815c84762bb540039309d89c49a4e271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/472 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: ì§€ë£¨í•˜ë‹¤, ë†€ëŸ¬ê°€ê³  ì‹¶ì–´.\n",
      "Answer: ê·¸ ìœ¼ë©´ í•´ ë³´ ì„¸ìš” <end>\n",
      "Question: ì˜¤ëŠ˜ ì¼ì° ì¼ì–´ë‚¬ë”ë‹ˆ í”¼ê³¤í•˜ë‹¤.\n",
      "Answer: í˜ë“  ì¼ ì´ ì•½ ì´ ë˜ ì—ˆ ì–´ìš” <end>\n",
      "Question: ê°„ë§Œì— ì—¬ìì¹œêµ¬ë‘ ë°ì´íŠ¸ í•˜ê¸°ë¡œ í–ˆì–´.\n",
      "Answer: ë“í…œ í•˜ ê³  ìˆ ì£  <end>\n",
      "Question: ì§‘ì— ìˆëŠ”ë‹¤ëŠ” ì†Œë¦¬ì•¼.\n",
      "Answer: ì§‘ ì´ ì‚¬ ë‚˜ ë´ìš” <end>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69bd17e5925243faac188fc7c35ea449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/472 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: ì§€ë£¨í•˜ë‹¤, ë†€ëŸ¬ê°€ê³  ì‹¶ì–´.\n",
      "Answer: ì–´ë–»ê²Œ í•´ ë³´ ì„¸ìš” <end>\n",
      "Question: ì˜¤ëŠ˜ ì¼ì° ì¼ì–´ë‚¬ë”ë‹ˆ í”¼ê³¤í•˜ë‹¤.\n",
      "Answer: ì˜¤ëŠ˜ ì€ í˜ë‚´ ë ¤ í•˜ ì§€ ë§ ì•„ìš” ì € ì—ê²Œ ê¸°ëŒ€ ì„¸ìš” <end>\n",
      "Question: ê°„ë§Œì— ì—¬ìì¹œêµ¬ë‘ ë°ì´íŠ¸ í•˜ê¸°ë¡œ í–ˆì–´.\n",
      "Answer: ë“í…œ í•˜ ê³  ìš” <end>\n",
      "Question: ì§‘ì— ìˆëŠ”ë‹¤ëŠ” ì†Œë¦¬ì•¼.\n",
      "Answer: ì§‘ ì´ ì œì¼ ë°ë ¤ë‹¤ ì¤„ ë•Œ ì´ ì£  <end>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3234370ee94741af9fecda4afb6ea5da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/472 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: ì§€ë£¨í•˜ë‹¤, ë†€ëŸ¬ê°€ê³  ì‹¶ì–´.\n",
      "Answer: ê·¸ ìì²´ ë¡œ í˜ë“œ ë‹ˆê¹Œìš” <end>\n",
      "Question: ì˜¤ëŠ˜ ì¼ì° ì¼ì–´ë‚¬ë”ë‹ˆ í”¼ê³¤í•˜ë‹¤.\n",
      "Answer: ì˜¤ëŠ˜ ì€ í˜ë‚´ ë ¤ ì‹œí‚¤ ì§€ ë§ ì•„ìš” ì € ì—ê²Œ ê¸°ëŒ€ ì„¸ìš” <end>\n",
      "Question: ê°„ë§Œì— ì—¬ìì¹œêµ¬ë‘ ë°ì´íŠ¸ í•˜ê¸°ë¡œ í–ˆì–´.\n",
      "Answer: ë“í…œ í•˜ ê²Œ ìš” <end>\n",
      "Question: ì§‘ì— ìˆëŠ”ë‹¤ëŠ” ì†Œë¦¬ì•¼.\n",
      "Answer: ì§‘ ì— ì°¾ì•„ê°€ ê³  ë‚˜ ì„œ ì‰¬ ê³  ì§‘ ì— ìš” <end>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b92ed04e11074034a371c596fafa098d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/472 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: ì§€ë£¨í•˜ë‹¤, ë†€ëŸ¬ê°€ê³  ì‹¶ì–´.\n",
      "Answer: ê·¸ ì‚¬ëŒ ì„ í†µí•´ ì—ë„ˆì§€ ë¥¼ ì“° ë‹ˆê¹Œìš” <end>\n",
      "Question: ì˜¤ëŠ˜ ì¼ì° ì¼ì–´ë‚¬ë”ë‹ˆ í”¼ê³¤í•˜ë‹¤.\n",
      "Answer: ì˜¤ëŠ˜ ë„ ê³ ìƒ ì´ ìˆ˜ë§ ì•˜ ì–´ìš” <end>\n",
      "Question: ê°„ë§Œì— ì—¬ìì¹œêµ¬ë‘ ë°ì´íŠ¸ í•˜ê¸°ë¡œ í–ˆì–´.\n",
      "Answer: ë“í…œ í•˜ ê²Œ ìš” <end>\n",
      "Question: ì§‘ì— ìˆëŠ”ë‹¤ëŠ” ì†Œë¦¬ì•¼.\n",
      "Answer: ì§‘ ì— ìš” <end>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9684e3f580a14fe5b1531b02e3b16b93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/472 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: ì§€ë£¨í•˜ë‹¤, ë†€ëŸ¬ê°€ê³  ì‹¶ì–´.\n",
      "Answer: ê·¸ ì‚¬ëŒ ì—ê²Œ ì¢‹ ì„ ê²ƒ ê°™ ì•„ìš” <end>\n",
      "Question: ì˜¤ëŠ˜ ì¼ì° ì¼ì–´ë‚¬ë”ë‹ˆ í”¼ê³¤í•˜ë‹¤.\n",
      "Answer: ì˜¤ëŠ˜ ì€ í˜ë‚´ ìš” <end>\n",
      "Question: ê°„ë§Œì— ì—¬ìì¹œêµ¬ë‘ ë°ì´íŠ¸ í•˜ê¸°ë¡œ í–ˆì–´.\n",
      "Answer: íœ´ì‹ ë„ ì¢‹ ì€ ìš” <end>\n",
      "Question: ì§‘ì— ìˆëŠ”ë‹¤ëŠ” ì†Œë¦¬ì•¼.\n",
      "Answer: ë°ë ¤ë‹¤ ì£¼ ë ¤ë‚˜ ë´ìš” <end>\n"
     ]
    }
   ],
   "source": [
    "# í•™ìŠµ\n",
    "\n",
    "#from tqdm import tqdm_notebook \n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 12\n",
    "\n",
    "examples = [\n",
    "            \"ì§€ë£¨í•˜ë‹¤, ë†€ëŸ¬ê°€ê³  ì‹¶ì–´.\",\n",
    "            \"ì˜¤ëŠ˜ ì¼ì° ì¼ì–´ë‚¬ë”ë‹ˆ í”¼ê³¤í•˜ë‹¤.\",\n",
    "            \"ê°„ë§Œì— ì—¬ìì¹œêµ¬ë‘ ë°ì´íŠ¸ í•˜ê¸°ë¡œ í–ˆì–´.\",\n",
    "            \"ì§‘ì— ìˆëŠ”ë‹¤ëŠ” ì†Œë¦¬ì•¼.\"\n",
    "]\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 30\n",
    "    \n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                    dec_train[idx:idx+BATCH_SIZE],\n",
    "                    transformer,\n",
    "                    optimizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
    "        \n",
    "    #if (epoch % 2 == 1):\n",
    "    for example in examples:\n",
    "        translate(example, transformer, chatbot_tokenizer, chatbot_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: ë˜ í•˜ë£¨ ë©€ì–´ì ¸ê°„ë‹¤.\n",
      "Answer: ì˜¤ëŠ˜ ë„ í—¤ì–´ì§€ ë©´ ì¢‹ ì„ ê±° ì˜ˆìš” <end>\n"
     ]
    }
   ],
   "source": [
    "translate('ë˜ í•˜ë£¨ ë©€ì–´ì ¸ê°„ë‹¤.', transformer, chatbot_tokenizer, chatbot_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: ë‚˜ëŠ” ë¬¸ì œ ì—†ì–´\n",
      "Answer: ë‚˜ ë¥¼ ë¬´ì‹œ í•˜ ì§€ ì•Š ê³  ì¡´ì¤‘ í•˜ ëŠ”ì§€ ì‚´í´ë³´ ì„¸ìš” <end>\n"
     ]
    }
   ],
   "source": [
    "translate('ë‚˜ëŠ” ë¬¸ì œ ì—†ì–´', transformer, chatbot_tokenizer, chatbot_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: ì–´ì œ ìˆ ì„ ë§ì´ ë¨¹ì—ˆì–´.\n",
      "Answer: í˜ê» ìš¸ ê³  ì²œì²œíˆ í„¸ ì–´ ë‚´ ê¸¸ ë°”ë„ ê²Œìš” <end>\n"
     ]
    }
   ],
   "source": [
    "translate('ì–´ì œ ìˆ ì„ ë§ì´ ë¨¹ì—ˆì–´.', transformer, chatbot_tokenizer, chatbot_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. í‰ê°€í•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "def eval_bleu_single(model, src_sentence, tgt_sentence, src_tokenizer, tgt_tokenizer, verbose=True):\n",
    "    \n",
    "\n",
    "    reference=tgt_sentence\n",
    "    #candidate = translate(' '.join(src_sentence), model, src_tokenizer, tgt_tokenizer)\n",
    "    \n",
    "    pieces, candidate, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        evaluate(' '.join(src_sentence), model, src_tokenizer, tgt_tokenizer)\n",
    "    \n",
    "    score = sentence_bleu(' '.join(reference), ' '.join(candidate), weights=(1,0,0,0))\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Source Sentence: \", src_sentence)\n",
    "        print(\"Model Prediction: \", candidate)\n",
    "        print(\"Real: \", reference)\n",
    "        print(\"Score: %lf\\n\" % score)\n",
    "        \n",
    "    return score\n",
    "        \n",
    "print('ìŠ=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "def eval_bleu(model, src_sentences, tgt_sentence, src_tokenizer, tgt_tokenizer, verbose=True):\n",
    "    total_score = 0.0\n",
    "    sample_size = len(src_sentences)\n",
    "    \n",
    "    for idx in tqdm(range(sample_size)):\n",
    "        score = eval_bleu_single(model, src_sentences[idx], tgt_sentence[idx], src_tokenizer, tgt_tokenizer, verbose)\n",
    "        if not score: continue\n",
    "        \n",
    "        total_score += score\n",
    "    \n",
    "    avg_score = total_score / sample_size\n",
    "    print(\"Num of Sample:\", sample_size)\n",
    "    print(f\"Total Score:{avg_score:.3f}\", )\n",
    "    \n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.105\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "# ref = 'ì¹œêµ¬ë“¤ì´ ì‚¬ë‘í•˜ë‹ˆê¹Œ ì˜ˆë»ì¡ŒëŒ€.'\n",
    "# cand = 'í–‰ë³µ ë°”ì´ëŸ¬ìŠ¤ì— ê°ì—¼ë˜ì…¨êµ°ìš”.'\n",
    "\n",
    "ref=['ì¹œêµ¬', 'ë“¤', 'ì´', 'ì‚¬ë‘', 'í•˜', 'ë‹ˆê¹Œ', 'ì˜ˆë»ì¡Œ', 'ëŒ€']\n",
    "cand = ['í–‰ë³µ', 'ë°”ì´ëŸ¬ìŠ¤', 'ì—', 'ê°ì—¼', 'ë˜', 'ì…¨', 'êµ°ìš”']\n",
    "score = sentence_bleu(' '.join(ref), ' '.join(cand), weights=(1,0,0,0))\n",
    "\n",
    "print(f'{score:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Sentence:  ['ì¹œêµ¬', 'ë“¤', 'ì´', 'ì‚¬ë‘', 'í•˜', 'ë‹ˆê¹Œ', 'ì˜ˆë»ì¡Œ', 'ëŒ€']\n",
      "Model Prediction:  ì¹œêµ¬ ë“¤ ì´ ì¹œêµ¬ ë¥¼ ì‚¬ê·ˆ ìˆ˜ ìˆ ê²  ë„¤ìš” <end>\n",
      "Real:  ['í–‰ë³µ', 'ë°”ì´ëŸ¬ìŠ¤', 'ì—', 'ê°ì—¼', 'ë˜', 'ì…¨', 'êµ°ìš”']\n",
      "Score: 0.052632\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05263157894736841"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_idx = 5\n",
    "\n",
    "eval_bleu_single(transformer, \n",
    "                 test_que_corpus[test_idx], \n",
    "                 test_ans_corpus[test_idx], \n",
    "                 chatbot_tokenizer, \n",
    "                 chatbot_tokenizer,\n",
    "                 verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0351e0cabb1346a790893b47c16d7e9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of Sample: 100\n",
      "Total Score:0.063\n"
     ]
    }
   ],
   "source": [
    "eval_bleu(transformer, test_que_corpus, test_ans_corpus, chatbot_tokenizer, chatbot_tokenizer, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ§ í…ŒìŠ¤íŠ¸ ë°ì´íƒ€ 100ì— ëŒ€í•´ì„œ BLEU Score 0.066ìœ¼ë¡œ ê¸°ë¡ ë˜ì—ˆë‹¤.    \n",
    "weights=(1,0,0,0) ë¡œ 1-gram ê¸°ì¤€ìœ¼ë¡œ ì‚°ì¶œí•˜ì˜€ë‹¤. ë‹¤ë¥¸ ê¸°ì¤€ë“¤ì€ ìˆ˜ì¹˜ê°€ ë„ˆë¬´ ì ê²Œ ë‚˜ì™€ ê·¸ë‚˜ë§ˆ ë†’ê²Œ ë‚˜ì˜¤ëŠ” ìˆ˜ì¹˜ë¡œ ì¸¡ì •í–ˆë‹¤. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Beam Search Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc_prob() êµ¬í˜„\n",
    "def calc_prob(src_ids, tgt_ids, model):\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "    generate_masks(src_ids, tgt_ids)\n",
    "\n",
    "    predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "    model(src_ids, \n",
    "            tgt_ids,\n",
    "            enc_padding_mask,\n",
    "            combined_mask,\n",
    "            dec_padding_mask)\n",
    "    \n",
    "    return tf.math.softmax(predictions, axis=-1)\n",
    "\n",
    "\n",
    "# beam_search_decoder() êµ¬í˜„\n",
    "def beam_search_decoder(sentence, \n",
    "                        src_len,\n",
    "                        tgt_len,\n",
    "                        model,\n",
    "                        src_tokenizer,\n",
    "                        tgt_tokenizer,\n",
    "                        beam_size):\n",
    "    \n",
    "    tokens = src_tokenizer.texts_to_sequences(sentence)\n",
    "\n",
    "    \n",
    "    src_in = tf.keras.preprocessing.sequence.pad_sequences(tokens,\n",
    "                                                            #maxlen=src_len,\n",
    "                                                            padding='post')\n",
    "\n",
    "    pred_cache = np.zeros((beam_size * beam_size, tgt_len), dtype=np.int64)\n",
    "    pred_tmp = np.zeros((beam_size, tgt_len), dtype=np.int64)\n",
    "\n",
    "    eos_flag = np.zeros((beam_size, ), dtype=np.int64)\n",
    "    scores = np.ones((beam_size, ))\n",
    "\n",
    "    pred_tmp[:, 0] = tgt_tokenizer.word_index['<start>']\n",
    "\n",
    "    dec_in = tf.expand_dims(pred_tmp[0, :1], 0)\n",
    "    prob = calc_prob(src_in, dec_in, model)[0, -1].numpy()\n",
    "    \n",
    "\n",
    "    for seq_pos in range(1, tgt_len):\n",
    "        score_cache = np.ones((beam_size * beam_size, ))\n",
    "\n",
    "        # init\n",
    "        for branch_idx in range(beam_size):\n",
    "            cache_pos = branch_idx*beam_size\n",
    "\n",
    "            score_cache[cache_pos:cache_pos+beam_size] = scores[branch_idx]\n",
    "            pred_cache[cache_pos:cache_pos+beam_size, :seq_pos] = \\\n",
    "            pred_tmp[branch_idx, :seq_pos]\n",
    "\n",
    "        for branch_idx in range(beam_size):\n",
    "            cache_pos = branch_idx*beam_size\n",
    "\n",
    "            if seq_pos != 1:   # ëª¨ë“  Branchë¥¼ ë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš°ë¥¼ ë°©ì§€\n",
    "                dec_in = pred_cache[branch_idx, :seq_pos]\n",
    "                dec_in = tf.expand_dims(dec_in, 0)\n",
    "\n",
    "                prob = calc_prob(src_in, dec_in, model)[0, -1].numpy()\n",
    "\n",
    "            for beam_idx in range(beam_size):\n",
    "                max_idx = np.argmax(prob)\n",
    "\n",
    "                score_cache[cache_pos+beam_idx] *= prob[max_idx]\n",
    "                pred_cache[cache_pos+beam_idx, seq_pos] = max_idx\n",
    "\n",
    "                prob[max_idx] = -1\n",
    "\n",
    "        for beam_idx in range(beam_size):\n",
    "            if eos_flag[beam_idx] == -1: continue\n",
    "\n",
    "            max_idx = np.argmax(score_cache)\n",
    "            prediction = pred_cache[max_idx, :seq_pos+1]\n",
    "\n",
    "            pred_tmp[beam_idx, :seq_pos+1] = prediction\n",
    "            scores[beam_idx] = score_cache[max_idx]\n",
    "            score_cache[max_idx] = -1\n",
    "\n",
    "            if prediction[-1] == tgt_tokenizer.word_index['<end>']:\n",
    "                eos_flag[beam_idx] = -1\n",
    "\n",
    "    pred = []\n",
    "    for long_pred in pred_tmp:\n",
    "        zero_idx = long_pred.tolist().index(tgt_tokenizer.word_index['<end>'])\n",
    "        short_pred = long_pred[:zero_idx+1]\n",
    "        pred.append(short_pred)\n",
    "    return pred\n",
    "\n",
    "def calculate_bleu(reference, candidate, weights=[1,0,0,0]):\n",
    "    return sentence_bleu(' '.join(reference),\n",
    "                            candidate,\n",
    "                            weights=weights,\n",
    "                            ) # smoothing_function=SmoothingFunction(epsilon=1e-12).method1\n",
    "\n",
    "\n",
    "# Q. beam_bleu() í•¨ìˆ˜ë¥¼ êµ¬í˜„í•´ë´…ì‹œë‹¤.\n",
    "def beam_bleu(reference, ids, tokenizer):\n",
    "\n",
    "    total_score = 0.0\n",
    "    for _id in ids:\n",
    "\n",
    "        candidate = tokenizer.sequences_to_texts([_id])[0]\n",
    "        score = calculate_bleu(reference, candidate)\n",
    "\n",
    "        print(\"Reference:\", ' '.join(reference))\n",
    "        print(\"Candidate:\", candidate)\n",
    "        print(\"BLEU:\", calculate_bleu(reference, candidate))\n",
    "        print(\"=\"*50)\n",
    "        print(\"\\n\")\n",
    "        total_score += score\n",
    "        \n",
    "    return total_score / len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: ë­ ë“  í•¨ê»˜ í•˜ ë ¤ëŠ” ê²ƒ ë„ ì¢‹ ê²  ì§€ë§Œ ì˜¤ë˜ ë¶™ ì–´ ì‡ ëŠ”ë‹¤ê³  í•´ì„œ ì‚¬ë‘ ì´ ë” ì»¤ ì§€ ê±°ë‚˜ ê¹Š ì–´ ì§€ ì§€ ì•Š ì•„ìš”\n",
      "Candidate: <start> ì•„ì¹¨ ë„ ì¢‹ ì•„ í•˜ ëŠ” ê²Œ ë­”ì§€ ì•Œì•„ë³´ ì„¸ìš” <end>\n",
      "BLEU: 0.21052631578947367\n",
      "==================================================\n",
      "\n",
      "\n",
      "Reference: ë­ ë“  í•¨ê»˜ í•˜ ë ¤ëŠ” ê²ƒ ë„ ì¢‹ ê²  ì§€ë§Œ ì˜¤ë˜ ë¶™ ì–´ ì‡ ëŠ”ë‹¤ê³  í•´ì„œ ì‚¬ë‘ ì´ ë” ì»¤ ì§€ ê±°ë‚˜ ê¹Š ì–´ ì§€ ì§€ ì•Š ì•„ìš”\n",
      "Candidate: <start> ì•„ì¹¨ ë„ ì¢‹ ì•„ í•˜ ëŠ” ê²Œ í•˜ë£¨ ì•Œì•„ë³´ ì„¸ìš” <end>\n",
      "BLEU: 0.1842105263157895\n",
      "==================================================\n",
      "\n",
      "\n",
      "Reference: ë­ ë“  í•¨ê»˜ í•˜ ë ¤ëŠ” ê²ƒ ë„ ì¢‹ ê²  ì§€ë§Œ ì˜¤ë˜ ë¶™ ì–´ ì‡ ëŠ”ë‹¤ê³  í•´ì„œ ì‚¬ë‘ ì´ ë” ì»¤ ì§€ ê±°ë‚˜ ê¹Š ì–´ ì§€ ì§€ ì•Š ì•„ìš”\n",
      "Candidate: <start> ì•„ì¹¨ ë„ ì¢‹ ì•„ í•˜ ëŠ” ê²Œ ì•„ì¹¨ ì•Œì•„ë³´ ì„¸ìš” <end>\n",
      "BLEU: 0.1842105263157895\n",
      "==================================================\n",
      "\n",
      "\n",
      "Reference: ë­ ë“  í•¨ê»˜ í•˜ ë ¤ëŠ” ê²ƒ ë„ ì¢‹ ê²  ì§€ë§Œ ì˜¤ë˜ ë¶™ ì–´ ì‡ ëŠ”ë‹¤ê³  í•´ì„œ ì‚¬ë‘ ì´ ë” ì»¤ ì§€ ê±°ë‚˜ ê¹Š ì–´ ì§€ ì§€ ì•Š ì•„ìš”\n",
      "Candidate: <start> ì•„ì¹¨ ë„ ì¢‹ ì•„ í•˜ ëŠ” ë§ˆìŒ ë­”ì§€ ì•Œì•„ë³´ ì„¸ìš” <end>\n",
      "BLEU: 0.20512820512820512\n",
      "==================================================\n",
      "\n",
      "\n",
      "Reference: ë­ ë“  í•¨ê»˜ í•˜ ë ¤ëŠ” ê²ƒ ë„ ì¢‹ ê²  ì§€ë§Œ ì˜¤ë˜ ë¶™ ì–´ ì‡ ëŠ”ë‹¤ê³  í•´ì„œ ì‚¬ë‘ ì´ ë” ì»¤ ì§€ ê±°ë‚˜ ê¹Š ì–´ ì§€ ì§€ ì•Š ì•„ìš”\n",
      "Candidate: <start> ì•„ì¹¨ ë„ ì¢‹ ì£  í•˜ ëŠ” ê²Œ ë­”ì§€ ì•Œì•„ë³´ ì„¸ìš” <end>\n",
      "BLEU: 0.21052631578947367\n",
      "==================================================\n",
      "\n",
      "\n",
      "0.19892037786774627\n"
     ]
    }
   ],
   "source": [
    "# Q. ì¸ë±ìŠ¤ë¥¼ ë°”ê¿”ê°€ë©° í™•ì¸í•´ ë³´ì„¸ìš”\n",
    "test_idx = 50\n",
    "\n",
    "ids = \\\n",
    "beam_search_decoder(test_que_corpus[test_idx],\n",
    "                    MAX_LEN,\n",
    "                    MAX_LEN,\n",
    "                    transformer,\n",
    "                    chatbot_tokenizer,\n",
    "                    chatbot_tokenizer,\n",
    "                    beam_size=5)\n",
    "bleu = beam_bleu(test_ans_corpus[test_idx], ids, chatbot_tokenizer)\n",
    "\n",
    "print(bleu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ§ Beam Search ì— ëŒ€í•œ BLEU scoreë„ ì‚°ì¶œí–ˆë‹¤. ì—ëŸ¬ ë‚˜ì„œ í¬ê¸°í• ê¹Œ í–ˆëŠ”ë°, ì˜¤ê¸° ë°œë™~!\n",
    "ì†ŒìŠ¤ëŠ” ë¹„ë¡ ê¹”ë”í•˜ì§„ ì•Šì§€ë§Œ ì™„ë£Œí•œ ê²ƒìœ¼ë¡œ ë§Œì¡±í•œë‹¤. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# íšŒê³ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CV data ì¦ê°•ì„ ìœ„í•œ ë°©ë²•ë“¤ì€ ì´ë¯¸ ì•Œê³  ìˆì—ˆëŠ”ë°, NLPì—ì„œ ë°ì´íƒ€ ì¦ê°•ì„ ì²˜ìŒìœ¼ë¡œ í•´ë³´ì•˜ë‹¤. \n",
    "- ìœ ì‚¬ì–´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë°ì´íƒ€ ì¦ê°•ì„ ì§„í–‰í•˜ë©´ì„œ, ì ì€ ë°ì´íƒ€ë¡œë„ ë°ì´íƒ€ë¥¼ ë§Œë“¤ì–´ ë‚¼ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ ì¬ë¯¸ìˆì—ˆë‹¤. \n",
    "- ì¦ê°•ëœ ë°ì´íƒ€ì—ì„œë„ Question-Answerì˜ ìŒì„ ë§ì¶”ë ¤ í•˜ë‹¤ë³´ë‹ˆ ì¤‘ë³µ ë°ì´íƒ€ê°€ ìƒê¸¸ ìˆ˜ ë°–ì— ì—†ëŠ” êµ¬ì¡°ê°€ ë˜ì—ˆë‹¤.\n",
    "- ì¶”ê°€ì ìœ¼ë¡œ ì¤‘ë³µì„ ì œê±°í• ê¹Œ í•˜ë‹¤ê°€ ë°ì´íƒ€ ìˆ˜ê°€ ì ì–´ ê·¸ëƒ¥ ì§„í–‰í–ˆë‹¤. \n",
    "- ì¦ê°•ëœ Question- ì¦ê°•ëœ Answer ì„¸íŠ¸ëŠ” ê³¼ì—° ì–‘ì§ˆì˜ ë°ì´íƒ€ ì˜€ì„ì§€ ê²€ì¦ì´ í•„ìš”í•´ ë³´ì¸ë‹¤. \n",
    "- Tokenizerë¥¼ ì‚¬ìš©í•  ë•Œ keras Tokenizer ëŒ€ì‹  SentencePieceë¥¼ ì‚¬ìš©í•˜ë ¤ í–ˆëŠ”ë° ì‹¤íŒ¨í–ˆë‹¤. Vocab_sizeê°€ 2000ë‚´ì™¸ë¡œ ë°–ì— ìƒì„±ì´ ì•ˆë˜ë©° ì—ëŸ¬ê°€ ë‚¬ê³ , ì–µì§€ë¡œ ìƒì„±ì„ í•´ì„œ ì§„í–‰ì„ í–ˆì§€ë§Œ, Answer í’ˆì§ˆì´ ìƒê°ë³´ë‹¤ ì¢‹ì§€ ì•Šì•˜ë‹¤. \n",
    "- Beam Search DecoderëŠ” ì‹œë„í•´ì„œ í•´ëƒˆë‹¤. \n",
    "- ê·¸ë˜ë„ ì´ë²ˆ í”„ë¡œì íŠ¸ë¥¼ í•˜ë©´ì„œ ì¬ë¯¸ìˆì—ˆë˜ ê²ƒì€ ì—‰ëš±í•˜ê³  ì¬ë¯¸ìˆëŠ” ë‹µë³€ë“¤ì´ ë‚˜ì™€ì„œ ë¹µ í„°ì¡Œë‹¤. ğŸ¤£ Q: ë‚˜ëŠ” ë¬¸ì œ ì—†ì–´ - A:ìƒê¸¸êº¼ì—ìš”. \n",
    "- ë°ì´í„° ì¦ê°•ì„ í†µí•´ì„œë„ ì¬ë¯¸ìˆëŠ” ë‚´ìš©ë“¤ì´ ë§ì•˜ë‹¤. Q: ê°„ì¥ ì¹˜í‚¨ ì‹œì¼œì¤˜ - QA(ì¦ê°•): ê°„ì¥ ì•„ì´ìŠ¤í¬ë¦¼ ì‹œì¼œì¤˜ ğŸ—ğŸ¦ \n",
    "- ì˜ì§€ê°€ ë˜ëŠ” ì±—ë´‡ íƒ„ìƒ ğŸ˜ Q: ì˜¤ëŠ˜ ì¼ì° ì¼ì–´ë‚¬ë”ë‹ˆ í”¼ê³¤í•˜ë‹¤.- A:ì˜¤ëŠ˜ ì€ í˜ë‚´ ë ¤ í•˜ ì§€ ë§ ì•„ìš” ì € ì—ê²Œ ê¸°ëŒ€ ì„¸ìš”\n",
    "- ìœ„ë¡œê°€ ë˜ëŠ” ì±—ë´‡ ğŸ˜ Q: ì–´ì œ ìˆ ì„ ë§ì´ ë¨¹ì—ˆì–´.- A:í˜ê» ìš¸ ê³  ì²œì²œíˆ í„¸ ì–´ ë‚´ ê¸¸ ë°”ë„ ê²Œìš” "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [GN-6]Fantastic Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 프로젝트는 Transformer를 이용하여 Chatbot을 만드는 것이다.    \n",
    "- 데이타 증강(Agumented)를 통해서 부족한 데이타를 추가\n",
    "- BLEU Score를 계산해 보기\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.21.4\n",
      "1.3.3\n",
      "2.6.0\n",
      "3.6.5\n",
      "3.8.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "import gensim\n",
    "import re\n",
    "from konlpy.tag import Mecab\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "print(np.__version__)\n",
    "print(pd.__version__)\n",
    "print(tf.__version__)\n",
    "print(nltk.__version__)\n",
    "print(gensim.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. 데이터 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>questions</th>\n",
       "      <th>answers</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         questions      answers  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'transformer/ChatbotData.csv'\n",
    "\n",
    "df = pd.read_csv(file_path, names=['questions','answers', 'label'], skiprows=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. 데이터 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    \n",
    "    sentence = sentence.lower().strip()\n",
    "    #sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    #sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z0-9]+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🧐 기본적으로 Tokenizer에서 기능으로 해주기에 특수문자와 공백처리는 제외함. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. 데이터 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11823"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df) #11823"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab=Mecab()\n",
    "\n",
    "max_len = 50\n",
    "\n",
    "que_corpus = []\n",
    "ans_corpus = []\n",
    "\n",
    "def build_corpus(df, max_len=50):\n",
    "    que = []\n",
    "    ans = []\n",
    "\n",
    "    unique_question = set()\n",
    "    unique_answer = set()\n",
    " \n",
    "    for row in df.iterrows():\n",
    "        question = row[1]['questions']\n",
    "        answer = row[1]['answers']\n",
    "\n",
    "        que_temp = preprocess_sentence(question)\n",
    "        que_temp = tuple(mecab.morphs(que_temp))\n",
    "        ans_temp = preprocess_sentence(answer)\n",
    "        ans_temp = tuple(mecab.morphs(ans_temp))\n",
    "\n",
    "        if que_temp not in unique_question:\n",
    "            if ans_temp not in unique_answer:\n",
    "                if len(que_temp) <= max_len and len(ans_temp) <= max_len:\n",
    "                    unique_question.add(que_temp)\n",
    "                    unique_answer.add(ans_temp)\n",
    "                    que.append(list(que_temp))\n",
    "                    ans.append(list(ans_temp))\n",
    "\n",
    "    return que, ans \n",
    "\n",
    "que_corpus, ans_corpus = build_corpus(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🧐 preprocess_sentence를 통해 전처리를 진행하고, Mecab을 통해서 형태소 분석 했다. \n",
    "Question과 Answer에서 중복값을 걸러내고, 문장길이도 max_len = 50으로 설정해서 분류해 냈다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['가난', '한', '자', '의', '설움'],\n",
       " ['가만', '있', '어도', '땀', '난다'],\n",
       " ['가상', '화폐', '쫄딱', '망함'],\n",
       " ['가스', '불', '켜', '고', '나갔', '어'],\n",
       " ['가스', '비', '너무', '많이', '나왔', '다'],\n",
       " ['가스', '비', '비싼데', '감기', '걸리', '겠', '어'],\n",
       " ['가장', '확실', '한', '건', '뭘까'],\n",
       " ['가족', '여행', '가', '기', '로', '했', '어'],\n",
       " ['가족', '있', '어'],\n",
       " ['가족', '끼리', '여행', '간다']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "que_corpus[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7640, 7640)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(que_corpus), len(ans_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n"
     ]
    }
   ],
   "source": [
    "total_sentence_count = len(que_corpus)\n",
    "test_sentence_count = int(total_sentence_count*0.01)\n",
    "print(test_sentence_count)# 100 개반 테스트 데이타로 남겨두자\n",
    "test_sentence_count = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test , trans 분리\n",
    "test_que_corpus = que_corpus[-test_sentence_count:]\n",
    "que_corpus = que_corpus[:-test_sentence_count]\n",
    "\n",
    "test_ans_corpus = ans_corpus[-test_sentence_count:]\n",
    "ans_corpus = ans_corpus[:-test_sentence_count]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100, 7540, 7540)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_que_corpus),len(test_ans_corpus),len(que_corpus),len(ans_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🧐 Test 데이타로 약 5%정도의 데이타인 100개를 분류해 두고, BLEU Score 계산에 이용할 예정이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://yeon22.tistory.com/158   \n",
    "pip install --upgrade gensim==3.8.3\n",
    "\n",
    "Gensim  버전이 4.1.2버전이었는데, ko.bin에 만들어진 버전과 맞지 않아 downgrade gensim==3.8.3로 진행했다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "wv=Word2Vec.load('transformer/ko.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('항해', 0.6469576358795166),\n",
       " ('탐사', 0.6048390865325928),\n",
       " ('여행', 0.5845088362693787),\n",
       " ('정복', 0.5619137287139893),\n",
       " ('개척', 0.5535061955451965),\n",
       " ('남극', 0.5296272039413452),\n",
       " ('탐험가', 0.5190513134002686),\n",
       " ('탐험대', 0.511405348777771),\n",
       " ('횡단', 0.4878459572792053),\n",
       " ('대서양', 0.4818343222141266)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(\"탐험\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: 은\n",
      "To: 은데 \n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "sample_sentence = \"은\"\n",
    "sample_tokens = sample_sentence.split()\n",
    "\n",
    "selected_tok = random.choice(sample_tokens)\n",
    "\n",
    "result = \"\"\n",
    "for tok in sample_tokens:\n",
    "    if tok is selected_tok:\n",
    "        if wv.most_similar(tok)[0][0] is not None:\n",
    "            result += wv.most_similar(tok)[0][0] + \" \"\n",
    "    else:\n",
    "        result += tok + \" \"\n",
    "\n",
    "print(\"From:\", sample_sentence)\n",
    "print(\"To:\", result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_sub(src_sentence, wv):\n",
    "    sample_tokens = src_sentence\n",
    "\n",
    "    selected_tok = random.choice(sample_tokens)\n",
    "\n",
    "    result = \"\"\n",
    "    \n",
    "    try:\n",
    "        sim_word = wv.most_similar(selected_tok)[0][0]\n",
    "    except:\n",
    "        sim_word = selected_tok\n",
    "\n",
    "    for tok in sample_tokens:\n",
    "        if tok is selected_tok: \n",
    "            result += sim_word + \" \"\n",
    "        else:\n",
    "            result += tok + \" \"\n",
    "\n",
    "    return result.strip().split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🧐 데이타 증강을 위한 기본 함수를 정의했다. 문장 토큰 중에서 임의로 하나를 선정해 most_similar를 찾고 있으면 그 단어를 사용하가 없으면 기본 토큰을 사용하도록 했다. 만일 공백문자나 '-'와 같은 문자를 반환하면, 의미없는 데이타가 생성되고, 아예 빼면 Question과 Answer과 쌍이 맞지 않기에 유사한 단어를 찾지 못하더라도 에러나 빈문자를 반환하기 보다, 기존 문장을 반환하도록 했다. \n",
    "이렇게 하면 전체적으로 중복 데이타가 생길 수 있지만, 따로 이후에 중복데이타 제거는 진행하지 않았다. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['탐험', '를', '떠날', '시간', '이다.']\n"
     ]
    }
   ],
   "source": [
    "temp = lexical_sub(['항해','를', '떠날', '시간','이다.'],wv)\n",
    "\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7540\n"
     ]
    }
   ],
   "source": [
    "# questin Argumented\n",
    "que_a_corpus=[]\n",
    "\n",
    "for idx, sen in enumerate(que_corpus):\n",
    "    new_src = lexical_sub(sen, wv)\n",
    "    que_a_corpus.append(new_src)  \n",
    "    \n",
    "print(len(que_a_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7540\n"
     ]
    }
   ],
   "source": [
    "# answer argumanted \n",
    "ans_a_corpus=[]\n",
    "\n",
    "for idx, sen in enumerate(ans_corpus):\n",
    "    new_src = lexical_sub(sen, wv)\n",
    "    ans_a_corpus.append(new_src)  \n",
    "    \n",
    "print(len(ans_a_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['간장', '치킨', '시켜야', '지'],\n",
       " ['간접흡연', '싫', '어'],\n",
       " ['갈까', '말', '까', '고민', '돼'],\n",
       " ['감기', '같애'],\n",
       " ['감기', '걸린', '것', '같', '아'],\n",
       " ['감기', '가', '오', '려나'],\n",
       " ['감미', '로운', '목소리', '좋', '아'],\n",
       " ['감정', '이', '쓰레기통', '처럼', '엉망진창', '이', '야'],\n",
       " ['감정', '컨트롤', '을', '못', '하', '겠', '어'],\n",
       " ['감히', '나', '를', '무시', '하', '는', '애', '가', '있', '어']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "que_corpus[30:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['고추장', '치킨', '시켜야', '지'],\n",
       " ['간접흡연', '힘들', '어'],\n",
       " ['갈까', '말로', '까', '고민', '돼'],\n",
       " ['달라붙', '같애'],\n",
       " ['감기', '걸린', '것', '똑같', '아'],\n",
       " ['감기', '가', '오', '여러해'],\n",
       " ['감미', '로운', '목소리', '괜찮', '아'],\n",
       " ['감정', '이', '쓰레기통', '처럼', '엉망진창', '그러', '야'],\n",
       " ['욕망', '컨트롤', '을', '못', '하', '겠', '어'],\n",
       " ['감히', '나의', '를', '무시', '하', '는', '애', '가', '있', '어']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "que_a_corpus[30:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['맛있', '게', '드세요'],\n",
       " ['저', '도', '싫', '어요'],\n",
       " ['가세', '요'],\n",
       " ['병원', '가', '세요'],\n",
       " ['이럴', '때', '잘', '쉬', '는', '게', '중요', '해요'],\n",
       " ['따뜻', '하', '게', '관리', '하', '세요'],\n",
       " ['저', '도', '듣', '고', '싶', '네요'],\n",
       " ['자신', '을', '더', '사랑', '해', '주', '세요'],\n",
       " ['그건', '습관', '이', '에요'],\n",
       " ['콕', '집', '어서', '물', '어', '보', '세요']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_corpus[30:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['맛있', '도록', '드세요'],\n",
       " ['저', '도', '힘들', '어요'],\n",
       " ['격분', '요'],\n",
       " ['병원', '놀드', '세요'],\n",
       " ['이럴', '때', '잘', '쉬', '는', '게', '핵심적', '해요'],\n",
       " ['따뜻', '하', '게', '관리', '하', 'ㅂ시오'],\n",
       " ['제', '도', '듣', '고', '싶', '네요'],\n",
       " ['자신', '을', '더', '사랑', '해', '주기도', '세요'],\n",
       " ['그건', '습관', '그러', '에요'],\n",
       " ['콕', '집', '어서', '물', '어', '살펴보', '세요']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_a_corpus[30:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이타 결합\n",
    "\n",
    "question_corpus=[]\n",
    "answer_corpus = []\n",
    "\n",
    "question_corpus = que_corpus + que_a_corpus + que_corpus + que_a_corpus\n",
    "answer_corpus = ans_corpus + ans_corpus + ans_a_corpus + ans_a_corpus\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🧐 데이타 결합   \n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30160, 30160)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(question_corpus), len(answer_corpus) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['쭉', '좋', '다가', '오늘', '갑자기', '너무', '보', '고', '싶', '고', '생각나', '네', 'ㅠㅠ']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_corpus[12329]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['그런', '날', '이', '있', '더라고요', '다른', '생각', '을', '해', '보', '세요']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_corpus[12329]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. 데이터 벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start>',\n",
       " '그런',\n",
       " '날',\n",
       " '이',\n",
       " '있',\n",
       " '더라고요',\n",
       " '다른',\n",
       " '생각',\n",
       " '을',\n",
       " '해',\n",
       " '보',\n",
       " '세요',\n",
       " '<end>']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# <start><end>추가\n",
    "\n",
    "answer_corpus = [['<start>'] + sentence +['<end>'] for sentence in answer_corpus]\n",
    "\n",
    "answer_corpus[12329]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=20000\n",
    "\n",
    "def gen_tokenizor(corpus): \n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token=\"<unk>\", num_words=vocab_size)\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    return  tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 7179\n"
     ]
    }
   ],
   "source": [
    "chatbot_tokenizer = gen_tokenizor(question_corpus + answer_corpus)\n",
    "\n",
    "print(\"Vocab Size:\", len(chatbot_tokenizer.index_word))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 50\n",
    "enc_tensor = chatbot_tokenizer.texts_to_sequences(question_corpus)\n",
    "dec_tensor = chatbot_tokenizer.texts_to_sequences(answer_corpus)\n",
    "\n",
    "enc_tensor = tf.keras.preprocessing.sequence.pad_sequences(enc_tensor, padding='post', maxlen=MAX_LEN)\n",
    "dec_tensor = tf.keras.preprocessing.sequence.pad_sequences(dec_tensor, padding='post', maxlen=MAX_LEN)\n",
    "\n",
    "enc_train = enc_tensor\n",
    "dec_train = dec_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6. 모델 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# positional_encoding\n",
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i) / d_model)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "    return sinusoid_table\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.W_q = tf.keras.layers.Dense(d_model) # Linear Layer\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "\n",
    "        \"\"\"\n",
    "        Scaled QK 값 구하기\n",
    "        \"\"\"\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9) \n",
    "\n",
    "        \"\"\"\n",
    "        1. Attention Weights 값 구하기 -> attentions\n",
    "        2. Attention 값을 V에 곱하기 -> out\n",
    "        \"\"\" \n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "        \n",
    "\n",
    "    def split_heads(self, x):\n",
    "        \"\"\"\n",
    "        Embedding을 Head의 수로 분할하는 함수\n",
    "\n",
    "        x: [ batch x length x emb ]\n",
    "        return: [ batch x heads x length x self.depth ]\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        split_x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"\n",
    "        분할된 Embedding을 하나로 결합하는 함수\n",
    "\n",
    "        x: [ batch x heads x length x self.depth ]\n",
    "        return: [ batch x length x emb ]\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (batch_size, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "    \n",
    "\n",
    "    def call(self, Q, K, V, mask):\n",
    "        \"\"\"\n",
    "        아래 순서에 따라 소스를 작성하세요.\n",
    "\n",
    "        Step 1: Linear_in(Q, K, V) -> WQ, WK, WV\n",
    "        Step 2: Split Heads(WQ, WK, WV) -> WQ_split, WK_split, WV_split\n",
    "        Step 3: Scaled Dot Product Attention(WQ_split, WK_split, WV_split)\n",
    "                 -> out, attention_weights\n",
    "        Step 4: Combine Heads(out) -> out\n",
    "        Step 5: Linear_out(out) -> out\n",
    "\n",
    "        \"\"\"\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "\n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "            \n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "    \t\t\t\t        \n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out, attention_weights\n",
    "    \n",
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.w_1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.w_2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.w_1(x)\n",
    "        out = self.w_2(out)\n",
    "            \n",
    "        return out\n",
    "    \n",
    "#Encoder Layer 구현\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "        \n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn\n",
    "    \n",
    "# Decoder 레이어 구현하기\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Masked Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out, dec_enc_attn = self.enc_dec_attn(out, enc_out, enc_out, causality_mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "        \n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn\n",
    "    \n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "    \n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns\n",
    "    \n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "                            \n",
    "                            \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "    \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns\n",
    "\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 src_vocab_size,\n",
    "                 tgt_vocab_size,\n",
    "                 pos_len,\n",
    "                 dropout=0.2,\n",
    "                 shared=True):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        \"\"\"\n",
    "        1. Embedding Layer 정의\n",
    "        2. Positional Encoding 정의\n",
    "        3. Encoder / Decoder 정의\n",
    "        4. Output Linear 정의\n",
    "        5. Shared Weights\n",
    "        6. Dropout 정의\n",
    "        \"\"\"\n",
    "\n",
    "        self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared = shared\n",
    "\n",
    "        if shared: self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "        \n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        \"\"\"\n",
    "        입력된 정수 배열을 Embedding + Pos Encoding\n",
    "        + Shared일 경우 Scaling 작업 포함\n",
    "\n",
    "        x: [ batch x length ]\n",
    "        return: [ batch x length x emb ]\n",
    "        \"\"\"\n",
    "        seq_len = x.shape[1]\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "        \n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        \"\"\"\n",
    "        Step 1: Embedding(enc_in, dec_in) -> enc_in, dec_in\n",
    "        Step 2: Encoder(enc_in, enc_mask) -> enc_out, enc_attns\n",
    "        Step 3: Decoder(dec_in, enc_out, mask)\n",
    "                -> dec_out, dec_attns, dec_enc_attns\n",
    "        Step 4: Out Linear(dec_out) -> logits\n",
    "        \"\"\"\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "        \n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
    "        \n",
    "        logits = self.fc(dec_out)\n",
    "        \n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "\n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
    "\n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 7. 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_layers = 1            # encoder, decoder layer를 각각 2개층 쌓음\n",
    "d_model = 368           # dense층 및 모든 모델의 dimention \n",
    "n_heads = 8             # multiheadattention을 위한 head 갯수\n",
    "d_ff = 1024             # FeedForward 층의 dimention   \n",
    "dropout = 0.2           \n",
    "\n",
    "\n",
    "src_vocab_size = vocab_size\n",
    "tgt_vocab_size = vocab_size\n",
    "pos_len = 600           # position endoding의 최대값\n",
    "\n",
    "warmup_steps = 1000     #learning rate\n",
    "\n",
    "\n",
    "transformer = Transformer(\n",
    "        n_layers,\n",
    "        d_model,\n",
    "        n_heads,\n",
    "        d_ff,\n",
    "        src_vocab_size,\n",
    "        tgt_vocab_size,\n",
    "        pos_len,\n",
    "        dropout\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "\n",
    "\n",
    "learning_rate = LearningRateScheduler(d_model, warmup_steps)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                     beta_1=0.9,\n",
    "                                     beta_2=0.98, \n",
    "                                     epsilon=1e-9)\n",
    "\n",
    "\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    # Masking 되지 않은 입력의 개수로 Scaling하는 과정\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "# Train Step 함수 정의\n",
    "\n",
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    gold = tgt[:, 1:]\n",
    "        \n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt)\n",
    "\n",
    "    # 계산된 loss에 tf.GradientTape()를 적용해 학습을 진행합니다.\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions[:, :-1])\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(question, model, src_tokenizer, tgt_tokenizer):\n",
    "    que_temp = preprocess_sentence(question)\n",
    "    que_temp = tuple(mecab.morphs(que_temp))\n",
    "    \n",
    "    que_temp = src_tokenizer.texts_to_sequences(que_temp)     \n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences(que_temp, padding='post')\n",
    "    \n",
    "    ids = []\n",
    "    output = tf.expand_dims([tgt_tokenizer.word_index['<start>']], 0)\n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(_input, output)\n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "        model(_input, \n",
    "              output,\n",
    "              enc_padding_mask,\n",
    "              combined_mask,\n",
    "              dec_padding_mask)\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "        \n",
    "        ids.append(predicted_id)\n",
    "        if tgt_tokenizer.word_index['<end>'] == predicted_id:\n",
    "            result = tgt_tokenizer.sequences_to_texts([ids])[0]\n",
    "            return question, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "        \n",
    "    result = tgt_tokenizer.sequences_to_texts([ids])[0]\n",
    "    return question, result, enc_attns, dec_attns, dec_enc_attns\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, model, src_tokenizer, tgt_tokenizer, plot_attention=False):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        evaluate(sentence, model, src_tokenizer, tgt_tokenizer)\n",
    "    \n",
    "    print('Question: %s' % (sentence))\n",
    "    print('Answer: {}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb337cea67c34d8c97229a891e43da84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/472 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: 지루하다, 놀러가고 싶어.\n",
      "Answer: 정말 많이 힘들 죠 <end>\n",
      "Question: 오늘 일찍 일어났더니 피곤하다.\n",
      "Answer: 좋 은 것 같 은데요 <end>\n",
      "Question: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "Answer: 좋 은 선택 이 에요 <end>\n",
      "Question: 집에 있는다는 소리야.\n",
      "Answer: 좋 은 사람 이 있 길 바랄 게요 <end>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5879d5a9f8bd4954be1f5b98229a32f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/472 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: 지루하다, 놀러가고 싶어.\n",
      "Answer: 다른 생각 을 해 보 세요 <end>\n",
      "Question: 오늘 일찍 일어났더니 피곤하다.\n",
      "Answer: 내일 은 오늘 내일 이 푹 푹 푹 푹 푹 푹 푹 푹 푹 푹 푹 푹 푹 푹 푹 푹 푹 푹 푹 푹 푹 푹 푹 푹 푹 푹 푹 푹 푹 푹 푹 푹 푹 푹 푹 푹 푹 푹 푹 푹 푹 푹 푹 푹 푹\n",
      "Question: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "Answer: 득템 했 네요 <end>\n",
      "Question: 집에 있는다는 소리야.\n",
      "Answer: 편하 고 싶 은 편하 고 싶 은 편하 고 나가 요 <end>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7e06592062540b0978aaa5c5cc3e71d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/472 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: 지루하다, 놀러가고 싶어.\n",
      "Answer: 그 을 읽 을 했 는지 생각 해 보 세요 <end>\n",
      "Question: 오늘 일찍 일어났더니 피곤하다.\n",
      "Answer: 힘든 시간 이 었 나요 <end>\n",
      "Question: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "Answer: 득템 했 나 봐요 <end>\n",
      "Question: 집에 있는다는 소리야.\n",
      "Answer: 같이 데려다 주 려나 봐요 <end>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a567c657ed2c4b1bb32cf1ac6ca0b4dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/472 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: 지루하다, 놀러가고 싶어.\n",
      "Answer: 다른 생각 을 해 보 세요 <end>\n",
      "Question: 오늘 일찍 일어났더니 피곤하다.\n",
      "Answer: 헛헛 하 군요 <end>\n",
      "Question: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "Answer: 휴식 도 필요 해요 <end>\n",
      "Question: 집에 있는다는 소리야.\n",
      "Answer: 집 무작정 찾아가 집 마련 축하 드려요 <end>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e22240c42b194a3aa3685c99e3d93a50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/472 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: 지루하다, 놀러가고 싶어.\n",
      "Answer: 다른 생각 을 해 살펴보 세요 <end>\n",
      "Question: 오늘 일찍 일어났더니 피곤하다.\n",
      "Answer: 힘든 하루 였 나요 <end>\n",
      "Question: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "Answer: 떨리 죠 <end>\n",
      "Question: 집에 있는다는 소리야.\n",
      "Answer: 집 이 편하 고 싶 죠 <end>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51814da343c740679590954208095e5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/472 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: 지루하다, 놀러가고 싶어.\n",
      "Answer: 다른 생각 을 해 보 세요 <end>\n",
      "Question: 오늘 일찍 일어났더니 피곤하다.\n",
      "Answer: 저 도 아침 이 군요 <end>\n",
      "Question: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "Answer: 득템 했 요 <end>\n",
      "Question: 집에 있는다는 소리야.\n",
      "Answer: 집 에 같이 들어가 요 <end>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a5dc68e8c6f4930815e3cca40ce7c69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/472 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: 지루하다, 놀러가고 싶어.\n",
      "Answer: 좀 더 어떻게 해 보 세요 <end>\n",
      "Question: 오늘 일찍 일어났더니 피곤하다.\n",
      "Answer: 힘든 일 이 군요 <end>\n",
      "Question: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "Answer: 떨리 는 떨리 죠 <end>\n",
      "Question: 집에 있는다는 소리야.\n",
      "Answer: 데려다 주 려나 봐요 <end>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "815c84762bb540039309d89c49a4e271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/472 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: 지루하다, 놀러가고 싶어.\n",
      "Answer: 그 으면 해 보 세요 <end>\n",
      "Question: 오늘 일찍 일어났더니 피곤하다.\n",
      "Answer: 힘든 일 이 약 이 되 었 어요 <end>\n",
      "Question: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "Answer: 득템 하 고 있 죠 <end>\n",
      "Question: 집에 있는다는 소리야.\n",
      "Answer: 집 이 사 나 봐요 <end>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69bd17e5925243faac188fc7c35ea449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/472 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: 지루하다, 놀러가고 싶어.\n",
      "Answer: 어떻게 해 보 세요 <end>\n",
      "Question: 오늘 일찍 일어났더니 피곤하다.\n",
      "Answer: 오늘 은 힘내 려 하 지 말 아요 저 에게 기대 세요 <end>\n",
      "Question: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "Answer: 득템 하 고 요 <end>\n",
      "Question: 집에 있는다는 소리야.\n",
      "Answer: 집 이 제일 데려다 줄 때 이 죠 <end>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3234370ee94741af9fecda4afb6ea5da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/472 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: 지루하다, 놀러가고 싶어.\n",
      "Answer: 그 자체 로 힘드 니까요 <end>\n",
      "Question: 오늘 일찍 일어났더니 피곤하다.\n",
      "Answer: 오늘 은 힘내 려 시키 지 말 아요 저 에게 기대 세요 <end>\n",
      "Question: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "Answer: 득템 하 게 요 <end>\n",
      "Question: 집에 있는다는 소리야.\n",
      "Answer: 집 에 찾아가 고 나 서 쉬 고 집 에 요 <end>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b92ed04e11074034a371c596fafa098d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/472 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: 지루하다, 놀러가고 싶어.\n",
      "Answer: 그 사람 을 통해 에너지 를 쓰 니까요 <end>\n",
      "Question: 오늘 일찍 일어났더니 피곤하다.\n",
      "Answer: 오늘 도 고생 이 수많 았 어요 <end>\n",
      "Question: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "Answer: 득템 하 게 요 <end>\n",
      "Question: 집에 있는다는 소리야.\n",
      "Answer: 집 에 요 <end>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9684e3f580a14fe5b1531b02e3b16b93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/472 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: 지루하다, 놀러가고 싶어.\n",
      "Answer: 그 사람 에게 좋 을 것 같 아요 <end>\n",
      "Question: 오늘 일찍 일어났더니 피곤하다.\n",
      "Answer: 오늘 은 힘내 요 <end>\n",
      "Question: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "Answer: 휴식 도 좋 은 요 <end>\n",
      "Question: 집에 있는다는 소리야.\n",
      "Answer: 데려다 주 려나 봐요 <end>\n"
     ]
    }
   ],
   "source": [
    "# 학습\n",
    "\n",
    "#from tqdm import tqdm_notebook \n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 12\n",
    "\n",
    "examples = [\n",
    "            \"지루하다, 놀러가고 싶어.\",\n",
    "            \"오늘 일찍 일어났더니 피곤하다.\",\n",
    "            \"간만에 여자친구랑 데이트 하기로 했어.\",\n",
    "            \"집에 있는다는 소리야.\"\n",
    "]\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 30\n",
    "    \n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                    dec_train[idx:idx+BATCH_SIZE],\n",
    "                    transformer,\n",
    "                    optimizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
    "        \n",
    "    #if (epoch % 2 == 1):\n",
    "    for example in examples:\n",
    "        translate(example, transformer, chatbot_tokenizer, chatbot_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: 또 하루 멀어져간다.\n",
      "Answer: 오늘 도 헤어지 면 좋 을 거 예요 <end>\n"
     ]
    }
   ],
   "source": [
    "translate('또 하루 멀어져간다.', transformer, chatbot_tokenizer, chatbot_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: 나는 문제 없어\n",
      "Answer: 나 를 무시 하 지 않 고 존중 하 는지 살펴보 세요 <end>\n"
     ]
    }
   ],
   "source": [
    "translate('나는 문제 없어', transformer, chatbot_tokenizer, chatbot_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: 어제 술을 많이 먹었어.\n",
      "Answer: 힘껏 울 고 천천히 털 어 내 길 바랄 게요 <end>\n"
     ]
    }
   ],
   "source": [
    "translate('어제 술을 많이 먹었어.', transformer, chatbot_tokenizer, chatbot_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 평가하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def eval_bleu_single(model, src_sentence, tgt_sentence, src_tokenizer, tgt_tokenizer, verbose=True):\n",
    "    \n",
    "\n",
    "    reference=tgt_sentence\n",
    "    #candidate = translate(' '.join(src_sentence), model, src_tokenizer, tgt_tokenizer)\n",
    "    \n",
    "    pieces, candidate, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        evaluate(' '.join(src_sentence), model, src_tokenizer, tgt_tokenizer)\n",
    "    \n",
    "    score = sentence_bleu(' '.join(reference), ' '.join(candidate), weights=(1,0,0,0))\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Source Sentence: \", src_sentence)\n",
    "        print(\"Model Prediction: \", candidate)\n",
    "        print(\"Real: \", reference)\n",
    "        print(\"Score: %lf\\n\" % score)\n",
    "        \n",
    "    return score\n",
    "        \n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def eval_bleu(model, src_sentences, tgt_sentence, src_tokenizer, tgt_tokenizer, verbose=True):\n",
    "    total_score = 0.0\n",
    "    sample_size = len(src_sentences)\n",
    "    \n",
    "    for idx in tqdm(range(sample_size)):\n",
    "        score = eval_bleu_single(model, src_sentences[idx], tgt_sentence[idx], src_tokenizer, tgt_tokenizer, verbose)\n",
    "        if not score: continue\n",
    "        \n",
    "        total_score += score\n",
    "    \n",
    "    avg_score = total_score / sample_size\n",
    "    print(\"Num of Sample:\", sample_size)\n",
    "    print(f\"Total Score:{avg_score:.3f}\", )\n",
    "    \n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.105\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "# ref = '친구들이 사랑하니까 예뻐졌대.'\n",
    "# cand = '행복 바이러스에 감염되셨군요.'\n",
    "\n",
    "ref=['친구', '들', '이', '사랑', '하', '니까', '예뻐졌', '대']\n",
    "cand = ['행복', '바이러스', '에', '감염', '되', '셨', '군요']\n",
    "score = sentence_bleu(' '.join(ref), ' '.join(cand), weights=(1,0,0,0))\n",
    "\n",
    "print(f'{score:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Sentence:  ['친구', '들', '이', '사랑', '하', '니까', '예뻐졌', '대']\n",
      "Model Prediction:  친구 들 이 친구 를 사귈 수 있 겠 네요 <end>\n",
      "Real:  ['행복', '바이러스', '에', '감염', '되', '셨', '군요']\n",
      "Score: 0.052632\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05263157894736841"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_idx = 5\n",
    "\n",
    "eval_bleu_single(transformer, \n",
    "                 test_que_corpus[test_idx], \n",
    "                 test_ans_corpus[test_idx], \n",
    "                 chatbot_tokenizer, \n",
    "                 chatbot_tokenizer,\n",
    "                 verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0351e0cabb1346a790893b47c16d7e9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of Sample: 100\n",
      "Total Score:0.063\n"
     ]
    }
   ],
   "source": [
    "eval_bleu(transformer, test_que_corpus, test_ans_corpus, chatbot_tokenizer, chatbot_tokenizer, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🧐 테스트 데이타 100에 대해서 BLEU Score 0.066으로 기록 되었다.    \n",
    "weights=(1,0,0,0) 로 1-gram 기준으로 산출하였다. 다른 기준들은 수치가 너무 적게 나와 그나마 높게 나오는 수치로 측정했다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Beam Search Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc_prob() 구현\n",
    "def calc_prob(src_ids, tgt_ids, model):\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "    generate_masks(src_ids, tgt_ids)\n",
    "\n",
    "    predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "    model(src_ids, \n",
    "            tgt_ids,\n",
    "            enc_padding_mask,\n",
    "            combined_mask,\n",
    "            dec_padding_mask)\n",
    "    \n",
    "    return tf.math.softmax(predictions, axis=-1)\n",
    "\n",
    "\n",
    "# beam_search_decoder() 구현\n",
    "def beam_search_decoder(sentence, \n",
    "                        src_len,\n",
    "                        tgt_len,\n",
    "                        model,\n",
    "                        src_tokenizer,\n",
    "                        tgt_tokenizer,\n",
    "                        beam_size):\n",
    "    \n",
    "    tokens = src_tokenizer.texts_to_sequences(sentence)\n",
    "\n",
    "    \n",
    "    src_in = tf.keras.preprocessing.sequence.pad_sequences(tokens,\n",
    "                                                            #maxlen=src_len,\n",
    "                                                            padding='post')\n",
    "\n",
    "    pred_cache = np.zeros((beam_size * beam_size, tgt_len), dtype=np.int64)\n",
    "    pred_tmp = np.zeros((beam_size, tgt_len), dtype=np.int64)\n",
    "\n",
    "    eos_flag = np.zeros((beam_size, ), dtype=np.int64)\n",
    "    scores = np.ones((beam_size, ))\n",
    "\n",
    "    pred_tmp[:, 0] = tgt_tokenizer.word_index['<start>']\n",
    "\n",
    "    dec_in = tf.expand_dims(pred_tmp[0, :1], 0)\n",
    "    prob = calc_prob(src_in, dec_in, model)[0, -1].numpy()\n",
    "    \n",
    "\n",
    "    for seq_pos in range(1, tgt_len):\n",
    "        score_cache = np.ones((beam_size * beam_size, ))\n",
    "\n",
    "        # init\n",
    "        for branch_idx in range(beam_size):\n",
    "            cache_pos = branch_idx*beam_size\n",
    "\n",
    "            score_cache[cache_pos:cache_pos+beam_size] = scores[branch_idx]\n",
    "            pred_cache[cache_pos:cache_pos+beam_size, :seq_pos] = \\\n",
    "            pred_tmp[branch_idx, :seq_pos]\n",
    "\n",
    "        for branch_idx in range(beam_size):\n",
    "            cache_pos = branch_idx*beam_size\n",
    "\n",
    "            if seq_pos != 1:   # 모든 Branch를 로 시작하는 경우를 방지\n",
    "                dec_in = pred_cache[branch_idx, :seq_pos]\n",
    "                dec_in = tf.expand_dims(dec_in, 0)\n",
    "\n",
    "                prob = calc_prob(src_in, dec_in, model)[0, -1].numpy()\n",
    "\n",
    "            for beam_idx in range(beam_size):\n",
    "                max_idx = np.argmax(prob)\n",
    "\n",
    "                score_cache[cache_pos+beam_idx] *= prob[max_idx]\n",
    "                pred_cache[cache_pos+beam_idx, seq_pos] = max_idx\n",
    "\n",
    "                prob[max_idx] = -1\n",
    "\n",
    "        for beam_idx in range(beam_size):\n",
    "            if eos_flag[beam_idx] == -1: continue\n",
    "\n",
    "            max_idx = np.argmax(score_cache)\n",
    "            prediction = pred_cache[max_idx, :seq_pos+1]\n",
    "\n",
    "            pred_tmp[beam_idx, :seq_pos+1] = prediction\n",
    "            scores[beam_idx] = score_cache[max_idx]\n",
    "            score_cache[max_idx] = -1\n",
    "\n",
    "            if prediction[-1] == tgt_tokenizer.word_index['<end>']:\n",
    "                eos_flag[beam_idx] = -1\n",
    "\n",
    "    pred = []\n",
    "    for long_pred in pred_tmp:\n",
    "        zero_idx = long_pred.tolist().index(tgt_tokenizer.word_index['<end>'])\n",
    "        short_pred = long_pred[:zero_idx+1]\n",
    "        pred.append(short_pred)\n",
    "    return pred\n",
    "\n",
    "def calculate_bleu(reference, candidate, weights=[1,0,0,0]):\n",
    "    return sentence_bleu(' '.join(reference),\n",
    "                            candidate,\n",
    "                            weights=weights,\n",
    "                            ) # smoothing_function=SmoothingFunction(epsilon=1e-12).method1\n",
    "\n",
    "\n",
    "# Q. beam_bleu() 함수를 구현해봅시다.\n",
    "def beam_bleu(reference, ids, tokenizer):\n",
    "\n",
    "    total_score = 0.0\n",
    "    for _id in ids:\n",
    "\n",
    "        candidate = tokenizer.sequences_to_texts([_id])[0]\n",
    "        score = calculate_bleu(reference, candidate)\n",
    "\n",
    "        print(\"Reference:\", ' '.join(reference))\n",
    "        print(\"Candidate:\", candidate)\n",
    "        print(\"BLEU:\", calculate_bleu(reference, candidate))\n",
    "        print(\"=\"*50)\n",
    "        print(\"\\n\")\n",
    "        total_score += score\n",
    "        \n",
    "    return total_score / len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: 뭐 든 함께 하 려는 것 도 좋 겠 지만 오래 붙 어 잇 는다고 해서 사랑 이 더 커 지 거나 깊 어 지 지 않 아요\n",
      "Candidate: <start> 아침 도 좋 아 하 는 게 뭔지 알아보 세요 <end>\n",
      "BLEU: 0.21052631578947367\n",
      "==================================================\n",
      "\n",
      "\n",
      "Reference: 뭐 든 함께 하 려는 것 도 좋 겠 지만 오래 붙 어 잇 는다고 해서 사랑 이 더 커 지 거나 깊 어 지 지 않 아요\n",
      "Candidate: <start> 아침 도 좋 아 하 는 게 하루 알아보 세요 <end>\n",
      "BLEU: 0.1842105263157895\n",
      "==================================================\n",
      "\n",
      "\n",
      "Reference: 뭐 든 함께 하 려는 것 도 좋 겠 지만 오래 붙 어 잇 는다고 해서 사랑 이 더 커 지 거나 깊 어 지 지 않 아요\n",
      "Candidate: <start> 아침 도 좋 아 하 는 게 아침 알아보 세요 <end>\n",
      "BLEU: 0.1842105263157895\n",
      "==================================================\n",
      "\n",
      "\n",
      "Reference: 뭐 든 함께 하 려는 것 도 좋 겠 지만 오래 붙 어 잇 는다고 해서 사랑 이 더 커 지 거나 깊 어 지 지 않 아요\n",
      "Candidate: <start> 아침 도 좋 아 하 는 마음 뭔지 알아보 세요 <end>\n",
      "BLEU: 0.20512820512820512\n",
      "==================================================\n",
      "\n",
      "\n",
      "Reference: 뭐 든 함께 하 려는 것 도 좋 겠 지만 오래 붙 어 잇 는다고 해서 사랑 이 더 커 지 거나 깊 어 지 지 않 아요\n",
      "Candidate: <start> 아침 도 좋 죠 하 는 게 뭔지 알아보 세요 <end>\n",
      "BLEU: 0.21052631578947367\n",
      "==================================================\n",
      "\n",
      "\n",
      "0.19892037786774627\n"
     ]
    }
   ],
   "source": [
    "# Q. 인덱스를 바꿔가며 확인해 보세요\n",
    "test_idx = 50\n",
    "\n",
    "ids = \\\n",
    "beam_search_decoder(test_que_corpus[test_idx],\n",
    "                    MAX_LEN,\n",
    "                    MAX_LEN,\n",
    "                    transformer,\n",
    "                    chatbot_tokenizer,\n",
    "                    chatbot_tokenizer,\n",
    "                    beam_size=5)\n",
    "bleu = beam_bleu(test_ans_corpus[test_idx], ids, chatbot_tokenizer)\n",
    "\n",
    "print(bleu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🧐 Beam Search 에 대한 BLEU score도 산출했다. 에러 나서 포기할까 했는데, 오기 발동~!\n",
    "소스는 비록 깔끔하진 않지만 완료한 것으로 만족한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CV data 증강을 위한 방법들은 이미 알고 있었는데, NLP에서 데이타 증강을 처음으로 해보았다. \n",
    "- 유사어를 기반으로 데이타 증강을 진행하면서, 적은 데이타로도 데이타를 만들어 낼 수 있다는 것이 재미있었다. \n",
    "- 증강된 데이타에서도 Question-Answer의 쌍을 맞추려 하다보니 중복 데이타가 생길 수 밖에 없는 구조가 되었다.\n",
    "- 추가적으로 중복을 제거할까 하다가 데이타 수가 적어 그냥 진행했다. \n",
    "- 증강된 Question- 증강된 Answer 세트는 과연 양질의 데이타 였을지 검증이 필요해 보인다. \n",
    "- Tokenizer를 사용할 때 keras Tokenizer 대신 SentencePiece를 사용하려 했는데 실패했다. Vocab_size가 2000내외로 밖에 생성이 안되며 에러가 났고, 억지로 생성을 해서 진행을 했지만, Answer 품질이 생각보다 좋지 않았다. \n",
    "- Beam Search Decoder는 시도해서 해냈다. \n",
    "- 그래도 이번 프로젝트를 하면서 재미있었던 것은 엉뚱하고 재미있는 답변들이 나와서 빵 터졌다. 🤣 Q: 나는 문제 없어 - A:생길꺼에요. \n",
    "- 데이터 증강을 통해서도 재미있는 내용들이 많았다. Q: 간장 치킨 시켜줘 - QA(증강): 간장 아이스크림 시켜줘 🍗🍦 \n",
    "- 의지가 되는 챗봇 탄생 😁 Q: 오늘 일찍 일어났더니 피곤하다.- A:오늘 은 힘내 려 하 지 말 아요 저 에게 기대 세요\n",
    "- 위로가 되는 챗봇 😏 Q: 어제 술을 많이 먹었어.- A:힘껏 울 고 천천히 털 어 내 길 바랄 게요 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
